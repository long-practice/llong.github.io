---
title: 프로그래머스 인공지능 데브코스 40일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 9주차(22.2.7.(월) ~ 22.1.12.(토)) 
  신경망, 딥러닝 기초
  다층 퍼셉트론
  Weekly Mission
tags:
- 신경망
- 뉴런
- 인공신경망
- 퍼셉트론
- 퍼셉트론 구조
- 퍼셉트론 동작
- 퍼셉트론 학습
- 목적함수
- 경사하강법
- pytorch

use_math: True
---

# 40일차(신경망 기초 - 신경망 기초, pytorch)

9주차(22.2.7.(월) ~ 22.2.12.(토)): 신경망 기초
* 신경망 기초
* 다층 퍼셉트론
* 딥러닝 기초
* monthly project

### 신경망

#### 사람의 뉴런
- 사람의 뉴런은 두뇌의 가장 작은 정보처리 단위
- 구조<br>
  - 세포체는 간단한 연산
  - 수상돌기는 신호 수신
  - 축삭은 세포체에서 임계치보다 높은 처리된 신호 결과를 전송
- 각 뉴런은 약 1000개의 다른 뉴런과 연결되어 $10^{14}$개의 연결을 가짐

#### 사람 신경망 vs 인공 신경망
- 세포체 vs 노드
- 수상돌기 vs 입력
- 축삭 vs 출력
- 시냅스 vs 가중치

#### 인공신경망
- 기계학습 역사에서 가장 오래된 기계 학습 모델<br>
  - 1950년대 퍼셉트론(인공두뇌학)
  - 1980년대 다층 퍼셉트론(결합설)
  - 2000년대 깊은 인공신경망(심층학습)
- 현재 다양한 형태의 인공신명망을 가지며, 주목할 만한 결과를 제공

### 다양한 신경망 모델
- 전방 신경망과 순환 신경망<br>
  - 한 쪽으로 신호가 흘러감 vs 중간에 피드백(신호가 되돌아가는 과정)이 있는 신경망
- 얕은 신경망과 깊은 신경망
- 결정론 신경망과 확률론적 신경망<br>
  - 모델의 매개변수와 조건에 의해 출력이 완전히 결정 vs 고유의 임의성을 가지고 매개변수와 조건이 같더라도 다른 출력(확률적인 결과값)
  
---

### 퍼셉트론
- 1950년대 컴퓨터 과학(컴퓨터 계산(연산) 능력의 획기적 발전)과 뇌 과학(뇌의 정보처리 방식 규명 연구) 발전
- 컴퓨터가 사람 뇌의 정보처리를 모방하여 지능적 행위를 할 수 있는 인공지능 도전
- 뉴런의 동작 이해를 모방한 초기 인공신경망 연구 - 퍼셉트론 고안<br>
<br>
- 절(node), 가중치(weight), 층(layer)과 같은 새로운 개념의 구조 도입
- 제시된 퍼셉트론 구조의 학습알고리즘 제안
- 원시적 신경망이지만, 깊은 인공신경망을 포함한 현대 인공신경망의 토대, 현대 인공신경망의 중요하 구성 요소
- **깊은 인공신경망은 퍼셉트론의 병렬 배치를 순차적으로 구조로 결합**<br>

#### 구조
- 입력<br>
  - $i$번째 노드는 특징 벡터 $\mathbf{x} = (x_1, x_2, \cdots, x_d)^T$의 요소 $x_i$를 담당
  - 항상 1이 입력되는 편향(bias)노드 포함
- 입출력 사이에 연산하는 구조<br>
  - $i$번째 입력 노드와 출력 노드를 연결하는 변(edge)는 가중치$w_i$를 가짐
  - 퍼셉트론은 단일 층 구조로 간주
- 출력<br>
  - 한 개의 노드에 의해 수치(1 또는 -1) 출력
<br>

#### 동작
- 선형 연산을 비선형 연산으로 변환<br>
  - 입력(특징)값과 가중치를 곱하고 모두 더해 $s$를 구함 - 선형 연산
  - 활성함수 $\tau$(예를 들면 계단 함수(출력이 1또는 -1))를 적용 - 비선형 연산
- 수식으로 표현하면 다음과 같음<br>
  $ y = \tau(s)$<br>
  $\displaystyle s = w_0 + \sum_{i=1}^{d}w_ix_i, \quad \tau(s) = \begin{cases} 1 & s \ge 0 \\ 0 & s < 0 \end{cases}$<br>
- 행렬로 표기하면 다음과 같음<br>
  - 편향 항을 벡터에 추가하면 $s = \mathbf{w}^T\mathbf{x}$로 표현이 가능하며, 이 때 $\mathbf{x} = (1, x_1, x_2, x_3, \cdots, x_d)^T$, $\mathbf{w} = (w_0, w_1, w_2, w_3, \cdots, w_d)^T$
  - 퍼셉트론의 동작을 식으로 표현하면 $y = \tau(\mathbf{w}^T\mathbf{x})$
<br>

#### 기하학적 의미
- 결정직선 $d(\mathbf{x}) = d(x_1, x_2) = w_1x_1 + w_2x_2 + w_0 = 0$에서 $w_1, w_2$는 직선의 기울기, $w_0$는 절편(편향)을 결정
- 결정 직선은 특징 공간을 1과 -1의 두 부분공간으로 이분할하는 분류기 역할<br>
<br>
- $d$차원 공간으로 일반화 $d(\mathbf{x}) = w_1x_1 + w_2x_2 + \cdots + w_dx_d + w_0 = 0 $<br>
  - 2차원: 결정 직선, 3차원: 결정 평면, 4차원 이상: 결정 초평면
  
<br>

#### 학습
- 어떻게 하면 입력데이터를 올바르게 100% 분류시킬 수 있는 $\mathbf{w}$를 구할 수 있을까?<br>
- 일반적인 분류기의 학습과정은 다음과 같음<br>
  1. 과업 정의와 분류 과정의 수학적 정의(가설 설정)
  2. 해당 분류기의 목적합수 $J(\Theta)$ 정의
  3. $J(\Theta)$를 최소화하는 $\Theta$를 찾기 위한 최적화 방법 수행
<br>
<br>
- 목적함수 정의<br>
  - 퍼셉트론(가설 혹은 모델 설정)의 매개변수를 $\mathbf{w} = (w_0, w_1, w_2, \cdots, w_d)^T$라 표기하면 매개변수 집합은 $\Theta = \{\mathbf{w}\}$라고 표기
  - 목적함수를 $J(\Theta)$ 또는 $J(\mathbf{w})$로 표기
  - 목적함수는 다음과 같이 정의가 가능<br>
    $\displaystyle J(\mathbf{w}) = \sum_{x_k \in Y} \{-y_k(\mathbf{w}^Tx_k)\}$<br>
    이 때, $Y$는 $\mathbf{w}$가 틀리는 샘플의 집합, 즉, 잘못 분류된 샘플들의 집합<br>
    <br>
  - 퍼셉트론 목적함수의 상세 조건 3가지<br>
    1. $J(\mathbf{w}) \ge 0$: 잘못 분류된 데이터 샘플에서 $y_k$와 $\mathbf{w}^Tx_k$의 부호는 다르므로 $- y_k(\mathbf{w}^Tx_k)$는 항상 양수이므로 성립
    2. $\mathbf{w}$가 최적이면, 즉 모든 샘플을 맞히면 $J(\mathbf{w}) = 0$: 잘못 분류된 데이터 샘플이 없으면 목적함수에서 계산되는 값이 없으므로 성립
    3. 틀리는 샘플이 많을수록 $J(\mathbf{w})$는 큰 값을 가짐: 많은 데이터에 대해 목적함수가 계산되므로 성립<br>
    <br>

- 경사 하강법<br>
  - 최소 $J(\Theta)$ 기울기를 이용하여 반복 탐색하여 극값을 찾음
  - 일반화된 가중치 갱신 규칙($\Theta = \Theta - \rho g$) 적용<br>
    $\displaystyle \frac{\partial J(\mathbf{w})}{\partial w_i} = \sum_{x_k \in Y} \frac{\partial(-y_k(w_0x_{k0} + w_1x_{k1} + \cdots + w_ix_{ki} + \cdots + w_dx_{kd}))}{\partial w_i} = \sum_{x_k \in Y}(-y_kx_{ki})$<br>
    $x_{ki}$는 $x_k = (x_{k0}, x_{k1}, \cdots, x_{kd})^T$의 $i$번째 요소<br>
  - 편미분 결과를 가중치 갱신 규칙에 적용하면 다음과 같고 이를 델타 규칙이라고 함<br>
    $\displaystyle w_i = w_i + \rho\sum_{x_k \in Y}y_kx_{ki} \rightarrow$ 델타 규칙은 퍼셉트론의 학습 방법

---

### pytorch

- Python 기반 과학 연산 패키지<br>
  - numpy를 대체하면서 GPU를 이용한 연산이 필요한 경우
  - 최대한의 유연성과 속도를 제공하는 딥러닝 연구 플랫폼이 필요한 경우
- Tensor는 numpy의 ndarray와 유사, GPU를 사용한 연산 가속이 가능


```python
%matplotlib inline
from __future__ import print_function
import torch

torch.__version__
```




    '1.10.2'




```python
# 초기화되지 않는 5x3 행렬 생성
x = torch.empty(5, 3)
print(x)
```

    tensor([[ 0.0000e+00, -4.6566e-10,  0.0000e+00],
            [-4.6566e-10,  1.2612e-44, -4.6566e-10],
            [ 0.0000e+00, -4.6566e-10,  1.1210e-44],
            [ 0.0000e+00,  1.4013e-45,  0.0000e+00],
            [ 1.4013e-45,  0.0000e+00,  0.0000e+00]])


---


```python
# 무작위로 초기화된 행렬 생성
# randn은 표준 정규분포를 따르도록 값을 샘플링
x = torch.randn(5, 3)
print(x)
# 0 ~ 1사이의 값을 샘플링
x = torch.rand(5, 3)
print(x)
```

    tensor([[ 0.7219, -0.5530, -1.3998],
            [ 0.5170,  0.2936,  0.6752],
            [ 0.2156, -1.1345, -0.9747],
            [-1.2731, -0.5829, -0.8094],
            [-0.9608, -0.7833, -1.4087]])
    tensor([[0.7607, 0.5598, 0.1787],
            [0.3975, 0.2569, 0.1561],
            [0.7032, 0.6359, 0.7025],
            [0.5459, 0.3884, 0.4644],
            [0.1845, 0.3645, 0.7663]])


---


```python
# 영행렬 생성
x = torch.zeros(5, 3, dtype = torch.long)
print(x)
```

    tensor([[0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0]])



```python
# 모든 값을 1로 행렬 생성
x = torch.ones(5, 3, dtype = torch.long)
print(x)
```

    tensor([[1, 1, 1],
            [1, 1, 1],
            [1, 1, 1],
            [1, 1, 1],
            [1, 1, 1]])


---


```python
# 직접 값을 넣어 행렬 생성
x = torch.tensor([5.5, 3])
print(x)
```

    tensor([5.5000, 3.0000])


---


```python
# 존재하는 tensor를 바탕으로 tensor를 만듦
x = x.new_ones(5, 3, dtype = torch.double)
print(x)

# 사용자로부터 제공된 새로운 값이 없는한 입력 tensor의 속성(사이즈 등..)들을 이어서 사용
x = torch.randn_like(x, dtype = torch.float)
print(x)
```

    tensor([[1., 1., 1.],
            [1., 1., 1.],
            [1., 1., 1.],
            [1., 1., 1.],
            [1., 1., 1.]], dtype=torch.float64)
    tensor([[-0.0672, -1.1319, -0.6783],
            [ 1.0764, -0.4748, -0.8270],
            [-0.6925,  1.4828,  0.6273],
            [ 0.5359, -1.9229, -0.9072],
            [ 1.5642,  0.0552,  0.3140]])


---


```python
# 행렬의 크기
print(x.size())
print(x.shape)
```

    torch.Size([5, 3])
    torch.Size([5, 3])


---


```python
# 행렬의 덧셈
y = torch.rand(5, 3)
print(x)
print(y)
print(x + y)
```

    tensor([[-0.0672, -1.1319, -0.6783],
            [ 1.0764, -0.4748, -0.8270],
            [-0.6925,  1.4828,  0.6273],
            [ 0.5359, -1.9229, -0.9072],
            [ 1.5642,  0.0552,  0.3140]])
    tensor([[0.9435, 0.9044, 0.2280],
            [0.1416, 0.0011, 0.9547],
            [0.9615, 0.6646, 0.8267],
            [0.2254, 0.6871, 0.3123],
            [0.1584, 0.8186, 0.3307]])
    tensor([[ 0.8763, -0.2275, -0.4503],
            [ 1.2180, -0.4737,  0.1277],
            [ 0.2689,  2.1474,  1.4541],
            [ 0.7613, -1.2357, -0.5948],
            [ 1.7226,  0.8739,  0.6447]])


---


```python
# 다른 문법
# 값은 동일한 것을 확인 가능
print(torch.add(x, y))
```

    tensor([[ 0.8763, -0.2275, -0.4503],
            [ 1.2180, -0.4737,  0.1277],
            [ 0.2689,  2.1474,  1.4541],
            [ 0.7613, -1.2357, -0.5948],
            [ 1.7226,  0.8739,  0.6447]])


---


```python
# 결과 tensor를 인자로 제공
result = torch.empty(5, 3)
print(result)

torch.add(x, y, out = result)
print(result)
```

    tensor([[ 0.0000e+00, -4.6566e-10,  0.0000e+00],
            [-4.6566e-10, -4.7266e-01,  1.2770e-01],
            [ 2.6893e-01,  2.1474e+00,  1.4541e+00],
            [ 7.6128e-01, -1.2357e+00, -5.9481e-01],
            [ 1.7226e+00,  8.7388e-01,  6.4470e-01]])
    tensor([[ 0.8763, -0.2275, -0.4503],
            [ 1.2180, -0.4737,  0.1277],
            [ 0.2689,  2.1474,  1.4541],
            [ 0.7613, -1.2357, -0.5948],
            [ 1.7226,  0.8739,  0.6447]])


---


```python
# 바꿔치기(inplace) 방식
# 바꿔치기 방식으로 tensor의 값을 변경하는 연산은 _를 접미사로 갖음

# 예를 들어 x.copy_(y), x.t_()는 x를 변경하는 연산

# adds x to y
print(y)
y.add_(x)
print(y)
```

    tensor([[0.9435, 0.9044, 0.2280],
            [0.1416, 0.0011, 0.9547],
            [0.9615, 0.6646, 0.8267],
            [0.2254, 0.6871, 0.3123],
            [0.1584, 0.8186, 0.3307]])
    tensor([[ 0.8763, -0.2275, -0.4503],
            [ 1.2180, -0.4737,  0.1277],
            [ 0.2689,  2.1474,  1.4541],
            [ 0.7613, -1.2357, -0.5948],
            [ 1.7226,  0.8739,  0.6447]])


---


```python
# 인덱싱
print(x)
print(x[:, 1])
```

    tensor([[-0.0672, -1.1319, -0.6783],
            [ 1.0764, -0.4748, -0.8270],
            [-0.6925,  1.4828,  0.6273],
            [ 0.5359, -1.9229, -0.9072],
            [ 1.5642,  0.0552,  0.3140]])
    tensor([-1.1319, -0.4748,  1.4828, -1.9229,  0.0552])


---


```python
# 크기 변경(view)
# 4x4 행렬을 1x16, 8x2 행렬로 변환
# numpy reshape과 동일
x = torch.randn(4, 4)
print(x)

y = x.view(16)
z = y.view(-1, 2)
print(y)
print(z)
print(y.size(), z.size())
```

    tensor([[-0.2308, -0.5417, -0.3878, -0.3290],
            [-0.0103, -1.7750,  1.1609, -0.8385],
            [ 0.5443, -0.6074,  0.0725, -0.0153],
            [-1.2598,  2.6045,  0.4724, -1.6326]])
    tensor([-0.2308, -0.5417, -0.3878, -0.3290, -0.0103, -1.7750,  1.1609, -0.8385,
             0.5443, -0.6074,  0.0725, -0.0153, -1.2598,  2.6045,  0.4724, -1.6326])
    tensor([[-0.2308, -0.5417],
            [-0.3878, -0.3290],
            [-0.0103, -1.7750],
            [ 1.1609, -0.8385],
            [ 0.5443, -0.6074],
            [ 0.0725, -0.0153],
            [-1.2598,  2.6045],
            [ 0.4724, -1.6326]])
    torch.Size([16]) torch.Size([8, 2])


---


```python
# tensor에 하나의 값만 존재하면 item()을 사용하면 숫자 값을 얻을 수 있음
x = torch.randn(1)
print(x)
print(type(x), type(x.item()))
print(x.item())
```

    tensor([-0.2832])
    <class 'torch.Tensor'> <class 'float'>
    -0.28324905037879944


여러가지 자세한 Tensor연산은 pytorch 공식문서에 수록되어 있음

---


```python
# Numpy변환(Bridge)
# Torch Tensor를 Numpy 배열(array)로 변환
# CPU 상에서 Torch Tensor와 Numpy 배열은 저장공간을 공유하기 때문에 하나를 변경하면 다른 하나도 변경

a = torch.ones(5)
print('a:', a)
```

    a: tensor([1., 1., 1., 1., 1.])



```python
b = a.numpy()
print('b:', b)
```

    b: [1. 1. 1. 1. 1.]


---


```python
# tensor와 numpy배열은 같은 저장공간을 공유하기 때문에 값이 서로 같이 변하는 것을 확인 가능
a.add_(1)
print('a:', a)
print('b:', b)
```

    a: tensor([2., 2., 2., 2., 2.])
    b: [2. 2. 2. 2. 2.]


---


```python
# 다음과 같이 temp배열을 따로 선언하여 서로 다른 메모리 공간에 값이 각각 바뀌도록 함
temp = a.clone()
temp_numpy = temp.numpy()

a.add_(1)
print(a)
print(temp_numpy)
```

    tensor([3., 3., 3., 3., 3.])
    [2. 2. 2. 2. 2.]


---


```python
# numpy 배열을 Torch Tensor로 변환(torch.from_numpy())
import numpy as np
a = np.ones(5)
print('a:', a)
print()

b = torch.from_numpy(a)
np.add(a, 1, out=a)
print('a:', a)
print('b:', b)
```

    a: [1. 1. 1. 1. 1.]
    
    a: [2. 2. 2. 2. 2.]
    b: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)


---

### CUDA Tensors

.to 메소드를 사용하여 Tensor를 어떤 장치로도 옮길 수 있음 <br>
아래 코드는 CUDA가 사용 가능한 환경에서 실행 가능


```python
x = torch.rand(4, 4)
if torch.cuda.is_available():
    device = "cuda:0"
    y = torch.ones_like(x, device = device)
    print(y)
    
    
    x = x.to(device)
    z = x + y
    print(z)
    print(z.to("cpu", torch.double))
```


```python
x = x.cuda()
```

---

### 느낀점
딥러닝의 기초 신경망을 배우게 되었는데 이전에 결정이론, 선형분류에서 나왔던 개념의 반복인 느낌이 들었다.<br>
개념을 하루 빨리 더 확고하게 다지고 이제는 실습을 하는데에 집중해야할 것 같다. <br>
특히 pytorch의 기본문법에 대해 살펴봤는데 numpy문법과 굉장히 유사해서 사용하는데 거부감이 들지 않았다.<br>
tensorflow도 문법이 어느정도 비슷해서 쉽게 사용할 수 있을 것 같았지만<br>
아쉽게도 tensorflow는 python버전이 맞지않아서 사용하지 못했고 실습을 진행하지 못했는데<br>
나중에 사용할 기회가 생긴다면 따로 추가적으로 가상환경을 만들어서 python 버전을 3.7버전으로 낮춰주고 사용해주면될 듯 싶다<br>
또한 pytorch는 아직 M1칩에서 GPU를 이용한 연산을 아직 지원하지 않아서 가급적 많은 연산이 필요로할 경우 colab을 이용하면될 것 같다
