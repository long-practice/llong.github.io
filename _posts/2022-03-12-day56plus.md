---
title: 프로그래머스 인공지능 데브코스 56일차(보충)
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 12주차(22.2.28.(월) ~ 22.3.4.(금)) 
  데이터 팀에 관한 커리어 이야기
  빅데이터와 분산처리 시스템
  Hadoop과 Spark에 대한 이해
  Spark를 이용한 데이터 전처리 및 머신러닝 모델 설계
tags:
- Spark 실습

use_math: True
---
### Spark 실습

**실습환경: 구글 Colab**

pyspark 패키지: python으로 Spark객체를 만들고 클러스터 내부를 작동시키는 함수들을 포함<br>
Py4j 패키지: 파이썬 프로그램이 자바 가상머신 상의 오브젝트들을 접근할 수 있게 해줌


```python
!pip install pyspark==3.0.1 py4j==0.10.9
```

    Collecting pyspark==3.0.1
      Downloading pyspark-3.0.1.tar.gz (204.2 MB)
    [K     |████████████████████████████████| 204.2 MB 34 kB/s 
    [?25hCollecting py4j==0.10.9
      Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)
    [K     |████████████████████████████████| 198 kB 56.6 MB/s 
    [?25hBuilding wheels for collected packages: pyspark
      Building wheel for pyspark (setup.py) ... [?25l[?25hdone
      Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=9b451b6cc2f85b3addccf4afa557f3f80277fe2086f5896a7eb21ca7b7a362b8
      Stored in directory: /root/.cache/pip/wheels/5e/34/fa/b37b5cef503fc5148b478b2495043ba61b079120b7ff379f9b
    Successfully built pyspark
    Installing collected packages: py4j, pyspark
    Successfully installed py4j-0.10.9 pyspark-3.0.1



```python
!ls -tl
```

    total 4
    drwxr-xr-x 1 root root 4096 Feb 18 14:33 sample_data



```python
!ls -tl sample_data
```

    total 55504
    -rw-r--r-- 1 root root 18289443 Feb 18 14:33 mnist_test.csv
    -rw-r--r-- 1 root root   301141 Feb 18 14:33 california_housing_test.csv
    -rw-r--r-- 1 root root  1706430 Feb 18 14:33 california_housing_train.csv
    -rw-r--r-- 1 root root 36523880 Feb 18 14:33 mnist_train_small.csv
    -rwxr-xr-x 1 root root     1697 Jan  1  2000 anscombe.json
    -rwxr-xr-x 1 root root      930 Jan  1  2000 README.md


#### Spark 세션 만들기


```python
from pyspark.sql import SparkSession

spark = SparkSession.builder\
        .master("local[*]")\
        .appName("PySpark_Tutorial")\
        .getOrCreate()
```


```python
spark
```





    <div>
        <p><b>SparkSession - in-memory</b></p>

<div>
    <p><b>SparkContext</b></p>

    <p><a href="http://d061b9830ae4:4040">Spark UI</a></p>

    <dl>
      <dt>Version</dt>
        <dd><code>v3.0.1</code></dd>
      <dt>Master</dt>
        <dd><code>local[*]</code></dd>
      <dt>AppName</dt>
        <dd><code>PySpark_Tutorial</code></dd>
    </dl>
</div>

    </div>




#### python 객체를 RDD로 변환


```python
num_list_json = ['{"number": "1"}', 
                  '{"number": "2"}', 
                  '{"number": "3"}', 
                  '{"number": "4"}',
                  '{"number": "5"}']
```


```python
# 데이터 확인
for n in num_list_json:
    print(n)
```

    {"number": "1"}
    {"number": "2"}
    {"number": "3"}
    {"number": "4"}
    {"number": "5"}



```python
import json

for n in num_list_json:
    jn = json.loads(n)
    print(jn['number'])
```

    1
    2
    3
    4
    5


#### 파이썬 리스트를 RDD로 변환
- RDD로 변환되는 순간 Spark 클러스터들의 서버들에 데이터가 나눠 저장(파티션)
- Lazy Execution: RDD가 어떤 의미있는 작업을 수행할 때 데이터가 RDD로 변환
- 만약 천만 개의 데이터를 뽑는다고 생각하면<br>
  위의 방식으로 데이터를 뽑을 경우 굉장히 많은 시간이 걸리지만<br>
  분산 처리 방식으로는 굉장히 빨리 끝낼 수 있음


```python
rdd = spark.sparkContext.parallelize(num_list_json)
rdd
```




    ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262




```python
rdd.count()
```




    5




```python
parsed_rdd = rdd.map(lambda el:json.loads(el))
parsed_rdd
```




    PythonRDD[2] at RDD at PythonRDD.scala:53




```python
parsed_rdd.collect()
```




    [{'number': '1'},
     {'number': '2'},
     {'number': '3'},
     {'number': '4'},
     {'number': '5'}]




```python
parsed_num_rdd = rdd.map(lambda el: json.loads(el)['number'])
parsed_num_rdd.collect()
```




    ['1', '2', '3', '4', '5']



#### 파이썬 리스트를 데이터프레임으로 변환


```python
from pyspark.sql.types import StringType

df = spark.createDataFrame(num_list_json, StringType())
```


```python
df.count()
```




    5




```python
df.printSchema()
```

    root
     |-- value: string (nullable = true)
    



```python
df.select('*').collect()
```




    [Row(value='{"number": "1"}'),
     Row(value='{"number": "2"}'),
     Row(value='{"number": "3"}'),
     Row(value='{"number": "4"}'),
     Row(value='{"number": "5"}')]




```python
df.select('value').collect()
```




    [Row(value='{"number": "1"}'),
     Row(value='{"number": "2"}'),
     Row(value='{"number": "3"}'),
     Row(value='{"number": "4"}'),
     Row(value='{"number": "5"}')]




```python
from pyspark.sql import Row

row = Row("number")
df_num = parsed_num_rdd.map(row).toDF()
```


```python
df_num.printSchema()
```

    root
     |-- number: string (nullable = true)
    



```python
df_num.select("number").collect()
```




    [Row(number='1'),
     Row(number='2'),
     Row(number='3'),
     Row(number='4'),
     Row(number='5')]



#### califonia housing 데이터를 이용한 데이터 처리 


```python
# Spark Session 만들기
from pyspark.sql import SparkSession

spark = SparkSession \
        .builder \
        .appName("Python Spark Dataframe basic example") \
        .getOrCreate()
```


```python
# 데이터 불러오기
import pandas as pd

housing_pandas_df = pd.read_csv("./sample_data/california_housing_test.csv")
housing_spark_df = spark.createDataFrame(housing_pandas_df)
```


```python
# 데이터 프레임의 칼럼들을 모두 출력
housing_spark_df.columns
```




    ['longitude',
     'latitude',
     'housing_median_age',
     'total_rooms',
     'total_bedrooms',
     'population',
     'households',
     'median_income',
     'median_house_value']




```python
# 스키마 출력
housing_spark_df.printSchema()
```

    root
     |-- longitude: double (nullable = true)
     |-- latitude: double (nullable = true)
     |-- housing_median_age: double (nullable = true)
     |-- total_rooms: double (nullable = true)
     |-- total_bedrooms: double (nullable = true)
     |-- population: double (nullable = true)
     |-- households: double (nullable = true)
     |-- median_income: double (nullable = true)
     |-- median_house_value: double (nullable = true)
    



```python
# 처음 5개 레코드 출력
housing_spark_df.show(n=5)
```

    +---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+
    |longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|
    +---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+
    |  -122.05|   37.37|              27.0|     3885.0|         661.0|    1537.0|     606.0|       6.6085|          344700.0|
    |   -118.3|   34.26|              43.0|     1510.0|         310.0|     809.0|     277.0|        3.599|          176500.0|
    |  -117.81|   33.78|              27.0|     3589.0|         507.0|    1484.0|     495.0|       5.7934|          270500.0|
    |  -118.36|   33.82|              28.0|       67.0|          15.0|      49.0|      11.0|       6.1359|          330000.0|
    |  -119.67|   36.33|              19.0|     1241.0|         244.0|     850.0|     237.0|       2.9375|           81700.0|
    +---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+
    only showing top 5 rows
    



```python
# 칼럼별 각종 통계치 추출
housing_spark_df.describe().show()
```

    +-------+-------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+
    |summary|          longitude|         latitude|housing_median_age|       total_rooms|   total_bedrooms|        population|       households|     median_income|median_house_value|
    +-------+-------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+
    |  count|               3000|             3000|              3000|              3000|             3000|              3000|             3000|              3000|              3000|
    |   mean|-119.58920000000013|35.63539000000004|28.845333333333333| 2599.578666666667|529.9506666666666|1402.7986666666666|          489.912|3.8072717999999988|        205846.275|
    | stddev| 1.9949362939550175|2.129669523343834|12.555395554955757|2155.5933316255814| 415.654368136323|1030.5430124122422|365.4227098055264|1.8545117296914786|113119.68746964629|
    |    min|            -124.18|            32.56|               1.0|               6.0|              2.0|               5.0|              2.0|            0.4999|           22500.0|
    |    max|            -114.49|            41.92|              52.0|           30450.0|           5419.0|           11935.0|           4930.0|           15.0001|          500001.0|
    +-------+-------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+
    



```python
# "total_rooms" 칼럼의 평균을 구하고 위의 결과와 비교
from pyspark.sql.functions import mean

housing_spark_df.select(mean("total_rooms")).show()
```

    +-----------------+
    | avg(total_rooms)|
    +-----------------+
    |2599.578666666667|
    +-----------------+
    



```python
# "population"의 최댓값, 최솟값 구하기
from pyspark.sql.functions import max, min

housing_spark_df.select(max("population"), min("population")).show()
```

    +---------------+---------------+
    |max(population)|min(population)|
    +---------------+---------------+
    |        11935.0|            5.0|
    +---------------+---------------+
    



```python
# "bedroom_ratio"라는 새로운 칼럼 만들기
# bedroom_ratio = total_bedrooms / total_rooms

new_housing_spark_df = housing_spark_df.withColumn("bedroom_ratio", 
                                                  housing_spark_df.total_bedrooms/housing_spark_df.total_rooms)
new_housing_spark_df.select("total_rooms", "total_bedrooms", "bedroom_ratio").show(5)
```

    +-----------+--------------+-------------------+
    |total_rooms|total_bedrooms|      bedroom_ratio|
    +-----------+--------------+-------------------+
    |     3885.0|         661.0|0.17014157014157014|
    |     1510.0|         310.0| 0.2052980132450331|
    |     3589.0|         507.0| 0.1412649763165227|
    |       67.0|          15.0|0.22388059701492538|
    |     1241.0|         244.0|0.19661563255439163|
    +-----------+--------------+-------------------+
    only showing top 5 rows
    

