---
title: 프로그래머스 인공지능 데브코스 38일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 8주차(22.1.24.(월) ~ 22.1.28.(금)) 
  선형회귀
  선형분류
  Weekly Mission
tags:
- 선형분류
- MNIST
- 이진 분류
- 로지스틱 회귀 모델
- 교차 검증
- 오차 행렬
- 정밀도
- 재현율
- precision/recall trade-off
- 데이터 증진(data augmentation)

use_math: True
---
# 38일차(ML_basics - 선형분류)

8주차(22.1.24.(월) ~ 22.1.28.(금)): ML_basic2
* 선형회귀
* 선형분류
* Weekly Mission
<br>
<br>

출처: 프로그래머스 인공지능 데브코스 3기 강의

### 라이브러리 불러오기


```python
# 파이썬 버전 3.5 이상
import sys
assert sys.version_info >= (3, 5)

# 사이킷런 버전 0.2 이상
import sklearn
assert sklearn.__version__ >= "0.20"

# Common imports
import numpy as np
import pandas as pd

# 랜덤 시드값 설정
np.random.seed(42)

# 시각화 도구
%matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize = 14)
mpl.rc('xtick', labelsize = 12)
mpl.rc('ytick', labelsize = 12)

# 경고문구 제거
import warnings
warnings.filterwarnings(action = 'ignore')
```

---

### MNIST 데이터 불러오기


```python
# 데이터 불러오기
from sklearn.datasets import fetch_openml
mnist = fetch_openml('mnist_784', version = 1, cache = True)
mnist.keys()
```




    dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])

<br>


```python
# 70000개의 데이터가 784차원으로 구성
# 28 X 28 = 784 픽셀
X, y = mnist['data'], mnist['target']
X.shape
```




    (70000, 784)




```python
# 입력 데이터
X
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pixel1</th>
      <th>pixel2</th>
      <th>pixel3</th>
      <th>pixel4</th>
      <th>pixel5</th>
      <th>pixel6</th>
      <th>pixel7</th>
      <th>pixel8</th>
      <th>pixel9</th>
      <th>pixel10</th>
      <th>...</th>
      <th>pixel775</th>
      <th>pixel776</th>
      <th>pixel777</th>
      <th>pixel778</th>
      <th>pixel779</th>
      <th>pixel780</th>
      <th>pixel781</th>
      <th>pixel782</th>
      <th>pixel783</th>
      <th>pixel784</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>69995</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>69996</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>69997</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>69998</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>69999</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>70000 rows × 784 columns</p>
</div>




```python
# 타깃 데이터
y
```




    0        5
    1        0
    2        4
    3        1
    4        9
            ..
    69995    2
    69996    3
    69997    4
    69998    5
    69999    6
    Name: class, Length: 70000, dtype: category
    Categories (10, object): ['0', '1', '2', '3', ..., '6', '7', '8', '9']


<br>

```python
# 세번째 데이터 확인
some_digit = X.to_numpy()[2]
some_digit_image = some_digit.reshape(28, 28)
plt.imshow(some_digit_image, cmap=mpl.cm.binary)
plt.axis("off")

plt.show()
```


<img width = "1200" src = "https://user-images.githubusercontent.com/83870423/155083387-7519804d-734b-467d-a506-8405aeda0883.png">
    



```python
# 위 그림에 대한 데이터
some_digit
```




    array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,  67., 232.,  39.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,  62.,  81.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0., 120., 180.,  39.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0., 126., 163.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   2., 153., 210.,  40.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 220., 163.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,  27., 254., 162.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0., 222., 163.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0., 183., 254., 125.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  46., 245., 163.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0., 198., 254.,  56.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0., 120., 254., 163.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,  23., 231., 254.,  29.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 159., 254.,
           120.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0., 163., 254., 216.,  16.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0., 159., 254.,  67.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,  14.,  86., 178., 248., 254.,  91.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 159.,
           254.,  85.,   0.,   0.,   0.,  47.,  49., 116., 144., 150., 241.,
           243., 234., 179., 241., 252.,  40.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0., 150., 253., 237., 207., 207., 207.,
           253., 254., 250., 240., 198., 143.,  91.,  28.,   5., 233., 250.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0., 119., 177., 177., 177., 177., 177.,  98.,  56.,   0.,   0.,
             0.,   0.,   0., 102., 254., 220.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 169., 254.,
           137.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0., 169., 254.,  57.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 169.,
           254.,  57.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0., 169., 255.,  94.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
           169., 254.,  96.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0., 169., 254., 153.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0., 169., 255., 153.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,  96., 254., 153.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.])


<br>

```python
# 타깃값의 데이터 형 변환
y = y.astype(np.uint8)
```


```python
def plot_digit(data):
    image = data.reshape(28, 28)
    plt.imshow(image, cmap = mpl.cm.binary,
               interpolation = "nearest")
    plt.axis("off")
```


```python
def plot_digits(instances, images_per_row=10, **options):
    size = 28
    images_per_row = min(len(instances), images_per_row)
    images = [instance.reshape(size, size) for instance in instances]
    n_rows = (len(instances) - 1) // images_per_row + 1
    row_images = []
    n_empty = n_rows * images_per_row - len(instances)
    images.append(np.zeros((size, size * n_empty)))
    for row in range(n_rows):
        rimages = images[row * images_per_row : (row + 1) * images_per_row]
        row_images.append(np.concatenate(rimages, axis = 1))
    image = np.concatenate(row_images, axis = 0)
    plt.imshow(image, cmap = mpl.cm.binary, **options)
    plt.axis("off")
```


```python
plt.figure(figsize = (9, 9))
example_images = X.to_numpy()[:100]
plot_digits(example_images, images_per_row = 10)
plt.show()
```


<img width = "1200" src = "https://user-images.githubusercontent.com/83870423/155083515-91a0dd98-8bdc-452c-ab63-e72709bdb126.png">
    



```python
# 첫 번째, 20번 째, 100번 째 타깃 값
y[0], y[19], y[99]
```




    (5, 9, 1)

<br>

```python
# 훈련 데이터, 테스트 데이터 분리
X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]
```

---

<br>

### 이진분류기(Binary Classifier)
숫자 5만 식별해보기


```python
y_train_5 = (y_train == 5)
y_test_5 = (y_test == 5)
```


```python
y_train_5
```




    0         True
    1        False
    2        False
    3        False
    4        False
             ...  
    59995    False
    59996    False
    59997     True
    59998    False
    59999    False
    Name: class, Length: 60000, dtype: bool

---

<br>

#### 로지스틱 회귀 모델 사용


```python
from sklearn.linear_model import LogisticRegression
log_clf = LogisticRegression(random_state = 0).fit(X_train, y_train_5)
log_clf.predict([X.to_numpy()[0], X.to_numpy()[1], X.to_numpy()[2]])
```




    array([ True, False, False])

---

<br>

#### 교차 검증을 사용하여 평가


```python
from sklearn.model_selection import cross_val_score
cross_val_score(log_clf, X_train, y_train_5, cv=3, scoring = "accuracy")
```




    array([0.97525, 0.97325, 0.9732 ])



모든 교차 검증 폴드에 대해 정확도가 97% 이상인데 좋다고 볼 수 있을까?<br>
단순히 5가 아니라고 예측하는 함수에 대해서도 90% 이상의 정확도를 보임<br>
<br>
숫자 5는 대략 10%정도 분포를 차지, 즉 무조건 5가 아닌 것으로 예측하면 정확도는 90%<br>
목표 값이 불균형한 경우 다수의 클래스에서 5가 아니라고 예측하는 함수의 성능은 비슷<br>
<br>
따라서 97% 정도의 정확도를 보였다고 좋은 성능을 가졌다고 할 수는 없음<br>
좋은 성능인 것처럼 보여지는 것은 평가지표에 문제가 있는 것


```python
from sklearn.base import BaseEstimator

# 무조건 5가 아니라고 예측하는 함수
class Never5Classifier(BaseEstimator):
    def fit(self, X, y = None):
        pass
    def predict(self, X):
        return np.zeros(len(X), dtype = bool)
```


```python
never_5_clf = Never5Classifier()
cross_val_score(never_5_clf, X_train, y_train_5, cv = 3, scoring="accuracy")
```




    array([0.91125, 0.90855, 0.90915])


<br>

```python
never_5_clf.predict(X)
```




    array([False, False, False, ..., False, False, False])

---

<br>

### 오차행렬(Confusion matrix)


```python
from sklearn.model_selection import cross_val_predict

y_train_pred = cross_val_predict(log_clf, X_train, y_train_5, cv = 3)
y_train_pred.shape
```




    (60000,)


<br>

```python
from sklearn.metrics import confusion_matrix

confusion_matrix(y_train_5, y_train_pred)
```




    array([[54039,   540],
           [ 1026,  4395]])



2 x 2 행렬로 나타남<br>
행이 의미하는 것은 타깃 값, 열이 의미하는 것은 모델의 예측 값<br>
<br>
(0, 0): 54,039개의 데이터는 5가 아닌 데이터에 대해서 5가 아니라고 예측<br>
(0, 1): 540개의 데이터는 5가 아닌 데이터에 대해서 5라고 예측<br>
(1, 0): 1,026개의 데이터는 5인 데이터에 대해 5가 아니라고 예측<br>
(1, 1): 4,395개의 데이터는 5인 데이터에 대해 5라고 예측

---

**정밀도(precision) = $\displaystyle \frac{\mbox{TP}}{\mbox{TP} + \mbox{FP}}$**<br>
<br>
**재현율(recall) = $\displaystyle \frac{\mbox{TP}}{\mbox{TP} + \mbox{FN}}$**


```python
from sklearn.metrics import precision_score, recall_score

print('모듈 이용하여 정밀도 계산:', precision_score(y_train_5, y_train_pred))
print('오차 행렬에서 정밀도 계산:', 4395 / (4395 + 541))
```

    모듈 이용하여 정밀도 계산: 0.8905775075987842
    오차 행렬에서 정밀도 계산: 0.8903970826580226

<br>

아까 정확도를 계산했을 때는 97% 였으나 실제 정밀도를 계산했을 때는 89%<br>
따라서 목표값(클래스)들이 불균형한 경우 정확도(accuracy)는 좋은 지표가 아님


```python
print('모듈 이용하여 재현율 계산:', recall_score(y_train_5, y_train_pred))
print('오차 행렬에서 재현율 계산:', 4395 / (4395 + 1026))
```

    모듈 이용하여 재현율 계산: 0.8107360265633647
    오차 행렬에서 재현율 계산: 0.8107360265633647

<br>

```python
confusion_matrix(y_train_5, never_5_clf.predict(X)[:60000])
```




    array([[54579,     0],
           [ 5421,     0]])


<br>

```python
precision_score(y_train_5, never_5_clf.predict(X)[:60000])
```




    0.0

<br>


```python
recall_score(y_train_5, never_5_clf.predict(X)[:60000])
```




    0.0

---

<br>

### Error cases 조사


```python
# 에측을 한 결과가 5가 아니라고 했을 때
errors = (y_train_pred != y_train_5)
errors
```




    0        False
    1        False
    2        False
    3        False
    4        False
             ...  
    59995    False
    59996    False
    59997    False
    59998    False
    59999    False
    Name: class, Length: 60000, dtype: bool


<br>

```python
# 에러인 경우 100개 만을 보여주기
plt.figure(figsize = (9, 9))
plot_digits(X_train.to_numpy()[errors][:100], images_per_row = 10)
```

<img width = "1200" src = "https://user-images.githubusercontent.com/83870423/155084162-2c4485c2-36f8-4025-b235-036f891e53ba.png">
    
---

<br>

### Precision/Recall Trade-off


```python
for i in range(len(errors)):
    if errors[i]:
        print(i)
        break
```

    48
    
<br>


```python
y_train_pred[48], y_train_5[48]
```




    (True, False)

<br>


```python
some_digit = X_train.to_numpy()[48]
y_scores = log_clf.decision_function([some_digit])
y_scores
```




    array([0.22419047])

<br>


```python
some_digit_image = some_digit.reshape(28, 28)
plt.imshow(some_digit_image, cmap = mpl.cm.binary)
plt.axis("off")

plt.show()
```


<img width = "1200" src = "https://user-images.githubusercontent.com/83870423/155084292-f745316a-296e-4b08-b6c2-62a6f4fef77d.png">

    


y_score 값이 각기 다르기 때문에 threshold를 움직임에 따라 예측값이 바뀔 수 있음


```python
threshold = 0
y_some_digit_pred = (y_scores > threshold)
y_some_digit_pred
```




    array([ True])


<br>

```python
threshold = 0.5
y_some_digit_pred = (y_scores > threshold)
y_some_digit_pred
```




    array([False])


<br>

```python
y_scores = cross_val_predict(log_clf, X_train, y_train_5, cv=3,
                             method="decision_function")
y_scores
```




    array([  2.42505615,  -9.04032728, -14.60157935, ...,   4.36550676,
            -5.21413161,  -5.4774804 ])


<br>


```python
from sklearn.metrics import precision_recall_curve

precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)
precisions.shape
```




    (59897,)


<br>

```python
thresholds.shape
```




    (59896,)


<br>

```python
def plot_precision_vs_recall(precisions, recalls):
    plt.plot(recalls, precisions, 'b-', linewidth=2)
    plt.xlabel("recall", fontsize = 16)
    plt.ylabel("Precision", fontsize = 16)
    plt.axis([0, 1, 0, 1])
    plt.grid(True)
    
plt.figure(figsize = (8, 6))
plot_precision_vs_recall(precisions, recalls)
plt.show()
```

<img width = "1200" src = "https://user-images.githubusercontent.com/83870423/155084534-1ccfa737-9a74-4a32-a91e-c491e350f99b.png">    


각각의 점들이 threshold를 의미<br>
recall이 증가함에 따라 precision은 감소하는 양상을 보임

---

### 다중 분류(Multiclass Classification)


```python
from sklearn.linear_model import LogisticRegression
softmax_reg = LogisticRegression(multi_class = "multinomial", solver = "lbfgs", C = 10)
softmax_reg.fit(X_train, y_train)
```




    LogisticRegression(C=10, multi_class='multinomial')

<br>


```python
softmax_reg.predict(X_train)[:10]
```




    array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)

<br>


```python
from sklearn.metrics import accuracy_score
y_pred = softmax_reg.predict(X_test)
accuracy_score(y_test, y_pred)
```




    0.9243

---

### Data Augmentation


```python
from scipy.ndimage.interpolation import shift
```


```python
def shift_image(image, dx, dy):
    image = image.reshape((28, 28))
    shifted_image = shift(image, [dy, dx], cval = 0, mode = "constant")
    return shifted_image.reshape([-1])
```

<br>

```python
# 데이터 평행이동

image = X_train.to_numpy()[1000]
# 이미지를 아래로 이동
shifted_image_down = shift_image(image, 0, 5)
# 이미지를 왼쪽으로 이동
shifted_image_left = shift_image(image, -5, 0)

plt.figure(figsize = (12, 3))
plt.subplot(131)
plt.title("Original", fontsize = 14)
plt.imshow(image.reshape(28, 28), interpolation = "nearest", cmap = mpl.cm.binary)
plt.subplot(132)
plt.title("Shifted down", fontsize = 14)
plt.imshow(shifted_image_down.reshape(28, 28), interpolation = "nearest", cmap = mpl.cm.binary)
plt.subplot(133)
plt.title("Shifted left", fontsize = 14)
plt.imshow(shifted_image_left.reshape(28, 28), interpolation = "nearest", cmap = mpl.cm.binary)

plt.show()
```


<img width = "1200" src = "https://user-images.githubusercontent.com/83870423/155084721-aa8502d5-d10e-4fe8-b7cb-1ea792ba6921.png">    



```python
# 60000개의 데이터에 대해 상하좌우 평행이동시킨 데이터 생성
# 총 30만 개의 데이터 존재
X_train_augmented = [image for image in X_train.to_numpy()]
y_train_augmented = [label for label in y_train.to_numpy()]

for dx, dy in ((1, 0), (-1, 0), (0, 1), (0, -1)):
    for image, label in zip(X_train.to_numpy(), y_train.to_numpy()):
        X_train_augmented.append(shift_image(image, dx, dy))
        y_train_augmented.append(label)

X_train_augmented = np.array(X_train_augmented)
y_train_augmented = np.array(y_train_augmented)
```


```python
X_train_augmented.shape
```




    (300000, 784)


<br>

```python
# 랜덤하게 데이터를 섞어주기
shuffle_idx = np.random.permutation(len(X_train_augmented))
X_train_augmented = X_train_augmented[shuffle_idx]
y_train_augmented = y_train_augmented[shuffle_idx]
```


```python
X_train_augmented.shape, X_train.shape
```




    ((300000, 784), (60000, 784))


<br>


```python
softmax_reg_augmented = LogisticRegression(multi_class = "multinomial", solver = "lbfgs", C = 10)
softmax_reg_augmented.fit(X_train_augmented, y_train_augmented)
```




    LogisticRegression(C=10, multi_class='multinomial')


<br>

```python
y_pred = softmax_reg_augmented.predict(X_test)
accuracy_score(y_test, y_pred)
```




    0.9279



데이터를 확대했을 때 성능이 조금 향상된 것을 확인할 수 있음<br>
평행이동한 데이터를 추가하는 것 뿐만아니라 회전시킨 데이터를 추가하는 것도 성능 향상에 도움을 줄 수 있음



### 느낀점
MNIST 데이터와 타이타닉 데이터를 이용하여 선형 분류를 실습했는데 꽤 직접 눈으로 결과를 봐가면서 해보니 확 와닿는 것이 많았다.<br>
여러가지 코딩 스킬들도 알 수 있었고, 다양한 성능 평가지표를 사용하는 것도 알게 되었다.<br>
아직은 미숙하고 어색한 것이 많은 단계라서 주로 사람들이 많이 사용하는 지표들을 위주로 사용하게 될 것 같지만<br>
위에서 배운 것들을 잊지않고 나중에 적절한 상황에서 적용할 줄 알아야할 것 같다.
