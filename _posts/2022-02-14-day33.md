---
title: 프로그래머스 인공지능 데브코스 33일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 7주차(22.1.17.(월) ~ 22.1.21.(금)) 
  확률이론
  결정이론과 선형회귀
  확률분포
  캐글 경진대회
  weekly mission을 통한 실습
tags:
- 가우시안분포
- 가우시안 분포의 기하학적 형태
- 가우시안 분포 정규화
- 기댓값
- 분산
- 조건부 가우시안 분포
- 주변 확률분포
- 가우시안 분포를 위한 베이즈 정리
- 가우시안 분포의 최대 우도
- 가우시안 분포를 위한 베이즈 추론

use_math: True
---
# 33일차(ML_basics - 확률분포)

7주차(22.1.17.(월) ~ 22.1.21.(토)): ML_basic1
* 확률이론
* 결정이론
* 선형회귀
* 확률분포
* kaggle 경진대회
* Weekly Mission

### 가우시안 분포

- 가우시안 분포가 일어나는 여러가지 상황<br>
  - 정보이론에서 엔트로피를 최대화시키는 확률분포
  - 중심극한정리<br>
<br>
- 단일변수 $x$에 대한 가우시안 분포<br>
  $\mathcal{N}(x \mid \mu, \sigma^{2})=\displaystyle\frac{1}{(2 \pi \sigma^{2})^{1 / 2}} \exp \Big\\{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\Big\\}$<br>
<br>
- $\boldsymbol{D}$차원 벡터 $\boldsymbol{x}$에 대한 가우시안 분포<br>
$\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \Sigma)=\displaystyle\frac{1}{(2 \pi)^{D / 2}} \frac{1}{\vert\Sigma\vert^{1 / 2}} \exp \Big\\{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T} \Sigma^{-1}(\mathbf{x}-\boldsymbol{\mu})\Big\\}$<br>
<br>
- $\boldsymbol{\mu}$는 $\boldsymbol{D}$차원의 평균 벡터이고, $\boldsymbol{\Sigma}$는 $\boldsymbol{D} \times \boldsymbol{D}$ 크기를 가지는 공분산 행렬
- 중요한 것은 평균과 공분산으로 주어진 것이 아닌 **이것들이 파라미터로 주어진 확률밀도함수의 평균과 공분산이 각각 $\boldsymbol{\mu}, \boldsymbol{\Sigma}$**<br>
<br><br>

#### 기하학적 형태
- $\boldsymbol{x}$에 대한 함수적 종속성은 지수부에 등장하는 이차형식(quadratic form)<br>
  $\boldsymbol{\Delta} ^ 2 = (\boldsymbol{x} - \boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x} - \boldsymbol{\mu})$<br>
<br>
- $\boldsymbol{\Sigma}$가 공분산으로 주어진 것이 아니기 때문에 이 행렬을 대칭이라고 생각할 필요는 없으나<br>
  이차형식에서 나타나는 행렬은 오직 대칭부분만이 그 값에 기여하기 때문에 대칭행렬로 간주<br>
  이 때 $\boldsymbol{U}$는 $\boldsymbol{\Sigma}$의 고유 벡터들을 행으로 정렬한 벡터이고, $\boldsymbol{\Lambda}$는 고유벡터에 대응되는 고유값들을 주대각원소로 배치한 행렬<br>
  $\boldsymbol{\Sigma} = \boldsymbol{U}^T\boldsymbol{\Lambda}\boldsymbol{U} = \displaystyle\sum_{i=1}^{D}\lambda_i\boldsymbol{u}\_i\boldsymbol{u}\_i^T \Leftrightarrow \boldsymbol{\Sigma}^{-1} = \sum_{i=1}^{D}\frac{1}{\lambda_i}\boldsymbol{u}\_i\boldsymbol{u}\_i^T$<br>
- 공분산 행렬의 고유벡터들은 모두 단위 직교 <br>
<br>
- 이차형식은 다음과 같이 표현 가능(벡터식 확장) ($y_i = \boldsymbol{u}\_i^T(\boldsymbol{x} - \boldsymbol{\mu})$일 때)<br>
  $\boldsymbol{\Delta} ^ 2 = \displaystyle\sum_{i=1}^{D}\frac{y_i^2}{\lambda_i} \longrightarrow \boldsymbol{y} = \boldsymbol{U}(\boldsymbol{x} - \boldsymbol{\mu})$<br>
<br>
- 따라서 $\boldsymbol{y}$를 행렬$\boldsymbol{U}$에 의해 정의된 새로운 좌표체계 내의 점으로 해석 가능(**기저 변환**)<br>
  $\boldsymbol{y} = \boldsymbol{U}(\boldsymbol{x} - \boldsymbol{\mu}) \rightarrow \boldsymbol{x} - \boldsymbol{\mu} = \boldsymbol{U}^{-1}\boldsymbol{y} = \boldsymbol{U}^T\boldsymbol{y}$

<br><br>

#### 정규화 증명
- 목표는 가우시안 분포를 $y$좌표계로 변환하는 것으로 야코비언을 이용(공간 변환 시 부피 변화율을 적용)
- 야코비언 $\boldsymbol{J}$<br>
  $\boldsymbol{J}\_{ij} = \displaystyle\frac{\partial x_i}{\partial y_j} = \boldsymbol{U}\_{ji} = (\boldsymbol{U}^T)\_{ij}$<br>
  $\left\vert \boldsymbol{J}^2 \right\vert = \left\vert \boldsymbol{U}^T \right\vert^2 = \left\vert \boldsymbol{U}^T \right\vert \left\vert \boldsymbol{U} \right\vert = \left\vert \boldsymbol{U}T\boldsymbol{U} \right\vert = \left\vert \boldsymbol{I} \right\vert = 1$<br>
<br>
- 행렬식 $\left\vert \boldsymbol{\Sigma} \right\vert$<br>
  $\left\vert \boldsymbol{\Sigma} \right\vert^{1/2} = \displaystyle\prod_{j=1}^{D}{\lambda}\_j^{1/2}$<br>
<br>
- $\boldsymbol{y}$의 확률밀도함수<br>
  $\displaystyle p(\mathbf{y})=p(\mathbf{x}) \left\vert \mathbf{J} \right\vert = \prod_{j=1}^{D} \frac{1}{(2 \pi \lambda_{j})^{1 / 2}} \exp \Big\\{-\frac{y_{j}^{2}}{2 \lambda_{j}} \Big\\}$<br>
- 위의 식은 서로 다른 $D$개의 정규분포가 독립적으로 곱해진 것이고, 변환된 식 또한 서로 다른 정규분포가 독립적으로 곱해진 것<br>
<br>
- $\boldsymbol{y}$의 정규화<br>
  $\displaystyle \int p(\mathbf{y}) \mathrm{d} \mathbf{y}=\prod_{j=1}^{D} \int_{-\infty}^{\infty} \frac{1}{(2 \pi \lambda_{j})^{1 / 2}} \exp \Big\\{-\frac{y_{j}^{2}}{2 \lambda_{j}}\Big\\} \mathrm{d} y_{j}=1$<br>
- 위의 식의 값이 1이 나오는 것은 당연한 결과(확률식이기 때문)

<br><br>

#### 기댓값
- 적률: 함수의 모양을 수학적으로 표현하기 위한 하나의 척도
- 통계학에서 1차 적률은 평균, 2차 적률은 분산, 3차 적률은 왜도, 4차 적률은 첨도를 구할 때 적용
- n차 적률은 다음과 같이 정의<br>
  $\displaystyle \mu_n = \int_{-\infty}^{\infty}(x-c)^nf(x)dx$<br>
- 통계학에서 적률은 $c = 0$일 때의 적률을 의미<br>
  $\displaystyle \mu_n^{\prime} = \int_{-\infty}^{\infty}(x-c)^nf(x)dx$<br>
<br>

- 다변량(multivariate) 확률변수의 기댓값<br>
  $\displaystyle\mathbf{x}=\left(x_{1}, \ldots, x_{n}\right)^{T}$<br>
  $\displaystyle\mathbb{E}[\mathbf{x}]=\left(\mathbb{E}\left[x_{1}\right], \ldots, \mathbb{E}\left[x_{n}\right]\right)^{T}$<br>
  $\displaystyle\mathbb{E}\left[x_{1}\right]=\int x_{1} p\left(x_{1}\right) \mathrm{d} x_{1}=\int x_{1}\left(\int p\left(x_{1}, \ldots, x_{n}\right) \mathrm{d} x_{2}, \ldots, \mathrm{d} x_{n}\right) \mathrm{d} x_{1}=\int x_{1} p\left(x_{1}, \ldots, x_{n}\right) \mathrm{d} x_{1}, \ldots, \mathrm{d} x_{n}$<br>
  <br>
- 1차 적률 구하기<br>
  $\displaystyle \mathbb{E} [ \mathbf{x} ]= \frac{1}{(2 \pi)^{D / 2} \left\vert \Sigma\right\vert^{1 / 2}} \int \exp \Big\\{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\Big\\} \mathbf{x} d \mathbf{x}$
  $\displaystyle = \frac{1}{(2 \pi)^{D / 2}\left\vert\Sigma\right\vert^{1 / 2}} \int \exp \Big\\{-\frac{1}{2}(\mathbf{z})^{T} \boldsymbol{\Sigma}^{-1}(\mathbf{z})\Big\\}(\mathbf{z}+\boldsymbol{\mu}) d \mathbf{z}$<br>
  위의 식은 $\mathbf{z}$에 대해 기함수가 만들어졌고, 기함수의 적분값은 항상 0<br>
  또한 정규분포의 전체 범위 적분값은 항상 1이므로 위의 적분식의 값은 $\mu$가 됨을 확인할 수 있음

<br><br>

#### 공분산
- 2차 적률 구하기<br>
  $\displaystyle E[\mathbf{x x}^{T}]=\frac{1}{(2 \pi)^{D / 2}\vert\Sigma\vert^{1 / 2}} \int \exp \Big\\{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\Big\\} \mathbf{x x}^{T} d \mathbf{x}$<br>
$\displaystyle \quad\quad\quad=\frac{1}{(2 \pi)^{D / 2}\vert\Sigma\vert^{1 / 2}} \int \exp \Big\\{-\frac{1}{2}(\mathbf{z})^{T} \boldsymbol{\Sigma}^{-1}(\mathbf{z})\Big\\}(\mathbf{z}+\boldsymbol{\mu})(\mathbf{z}+\boldsymbol{\mu})^{T} d \mathbf{z}$<br>
  위의 식에서 $(\mathbf{z}+\boldsymbol{\mu})(\mathbf{z}+\boldsymbol{\mu})^{T}$를 전개했을 때<br>
  $\boldsymbol{\mu}\boldsymbol{\mu}^T$는 상수(적분 식 밖으로 빠져나옴), $\mathbf{z}\boldsymbol{\mu}^T$와 $\mathbf{z}^T\boldsymbol{\mu}$는 $\mathbf{z}$에 대한 기함수이므로 적분값에 영향을 미치는 요소는 $\mathbf{z}\mathbf{z}^T$<br>
<br>
- $\displaystyle \mathbf{z} = \sum_{j = 1}^Dy_j\boldsymbol{u}\_j$ 그리고 $y_i = \boldsymbol{u}\_j^T\boldsymbol{z}$을 적용하면 위의 전개식은 다음과 같이 표현이 가능<br>
$\displaystyle \frac{1}{(2 \pi)^{D / 2}\vert\Sigma\vert^{1 / 2}} \int \exp \Big\\{-\frac{1}{2}(\mathbf{z})^{T} \boldsymbol{\Sigma}^{-1}(\mathbf{z})\Big\\} \mathbf{z} \mathbf{z}^{T} d \mathbf{z}$
$\displaystyle = \frac{1}{(2 \pi)^{D / 2}\vert\Sigma\vert^{1 / 2}} \sum_{i=1}^{D} \sum_{j=1}^{D} \mathbf{u}\_{i} \mathbf{u}\_{j}^{T} \int \exp \Big\\{-\sum_{k=1}^{D} \frac{y_{k}^{2}}{2 \lambda_{k}}\Big\\} y_{i} y_{j} d \mathbf{y}$
$\displaystyle = \sum_{i=1}^{D} \mathbf{u}\_{i} \mathbf{u}\_{i}^{T} \lambda_{i}=\Sigma$<br>
<br>
- 2차 적률은 평균과 분산의 합으로 나타내어지고, 위의 식의 값과 일치하는 것을 확인할 수 있음<br>
  $E\left[\mathbf{x x}^{T}\right]=\boldsymbol{\mu} \boldsymbol{\mu}^{T}+\Sigma$<br>
<br>
- 따라서 공분산을 구하면 다음과 같음(이 때 $E[\mathbf{x}] = \boldsymbol{\mu}$)<br>
  $\operatorname{cov}[\mathbf{x}]=E\left[(\mathbf{x}-E[\mathbf{x}])(\mathbf{x}-E[\mathbf{x}])^{T}\right] \longrightarrow \operatorname{cov}[\mathbf{x}]=\Sigma$

---

### 조건부 가우시안 분포
- 확률변수 $\mathbf{x}$를 두 그룹으로 나누었을 때 한 그룹이 주어졌을 때 나머지 그룹의 조건부 확률도 가우시안 분포를 따름
- 또한 각 그룹의 주변확률 또한 가우시안 분포를 따름
- 확률변수, 평균, 공분산 행렬, 정확도 행렬$\mathbf{x}, \boldsymbol{\mu}, \Sigma, \Lambda$은 다음과 같음
- 정확도 행렬을 사용하는 이유는 계산을 용이하게 하기위함<br>

$
\mathbf{x}=\left[\begin{array}{l}
\mathbf{x}\_{a} \\\\
\mathbf{x}\_{b}
\end{array}\right]
\quad
\begin{gathered}
\boldsymbol{\mu}=\left[\begin{array}{c}
\boldsymbol{\mu}\_{a} \\\\
\boldsymbol{\mu}\_{b}
\end{array}\right] \quad
\Sigma=\left[\begin{array}{cc}
\Sigma_{a a} & \Sigma_{a b} \\\\
\Sigma_{b a} & \Sigma_{b b}
\end{array}\right]
\end{gathered}
\quad
\begin{gathered}
\Lambda=\Sigma^{-1}  = \left[\begin{array}{ll}
\Lambda_{a a} & \Lambda_{a b} \\\\
\Lambda_{b a} & \Lambda_{b b}
\end{array}\right]
\end{gathered}
$
<br>
<br>
- 한편 가우시안 분포는 지수부를 전개하면 다음과 같이 표현 가능<br>
  $\begin{aligned}-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T} \Sigma^{-1}(\mathbf{x}-\boldsymbol{\mu}) &=-\frac{1}{2}\left(\mathbf{x}^{T}-\boldsymbol{\mu}^{T}\right) \Sigma^{-1}(\mathbf{x}-\boldsymbol{\mu}) \\ &=-\frac{1}{2}\left(\mathbf{x}^{T} \Sigma^{-1}-\boldsymbol{\mu}^{T} \Sigma^{-1}\right)(\mathbf{x}-\boldsymbol{\mu}) \\ &=-\frac{1}{2}\left(\mathbf{x}^{T} \Sigma^{-1} \mathbf{x}-\boldsymbol{\mu}^{T} \Sigma^{-1} \mathbf{x}-\mathbf{x}^{T} \Sigma^{-1} \boldsymbol{\mu}+\boldsymbol{\mu}^{T} \Sigma^{-1} \boldsymbol{\mu}\right) \\ &=-\frac{1}{2} \mathbf{x}^{T} \Sigma^{-1} \mathbf{x}+\mathbf{x}^{T} \Sigma^{-1} \boldsymbol{\mu}+\text {const} \end{aligned}$<br>
- 이 때 $\text{const}$는 $\mathbf{x}$와는 독립된 항
- 위의 식을 $\mathbf{x}$에 대해 두 번 미분하면 공분산의 역행렬($\Sigma^{-1}$), 한 번 미분하면 평균에 대한 정보($\Sigma^{-1}\boldsymbol{\mu}$)를 얻을 수 있음
- 또한 어떤 확률분포가 $\frac{1}{2} \mathbf{x}^{T} \Sigma^{-1} \mathbf{x}+\mathbf{x}^{T} \Sigma^{-1} \boldsymbol{\mu}+\text {const}$과 같은 꼴로 표현이 가능하다면 그 확률분포는 가우시안 분포를 따른다고 볼 수 있음<br>
<br>
- 현재 우리가 구하고자 하는 것은 $p(\mathbf{x}_a \mid \mathbf{x}_b)$의 확률분포
- 결합확률분포를 우선적으로 살펴보면 $p(\mathbf{x}_a, \mathbf{x}_b)$로 $\mathbf{x}_b$는 관찰 값으로 특정한 값으로 고정
  $\Delta^{2}=(\mathbf{x}-\boldsymbol{\mu})^{T} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})$에서 이차형식 부분을 다음과 같이 표현<br>
$ \begin{aligned} -\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T} \Sigma^{-1}(\mathbf{x}-\boldsymbol{\mu})= -\frac{1}{2}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)^{T} \Lambda_{a a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)-\frac{1}{2}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)^{T} \Lambda_{a b}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right)
\\ -\frac{1}{2}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right)^{T} \Lambda_{b a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)-\frac{1}{2}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right)^{T} \Lambda_{b b}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right) \end{aligned}$<br>
<br>
- 조건부 분포의 평균과 분산은 $\mathbf{x}_a$를 미분하여 얻을 수 있으며, $\mathbf{x}_a$와 연관있는 항은 $-\frac{1}{2} \mathbf{x}_{a}^{T} \Lambda_{a a} \mathbf{x}_{a}$<br>
- 따라서 위의 식을 두 번 미분하게되면 공분산을 얻을 수 있음 $\Sigma_{a\mid b} = \Lambda_{a a}^{-1} $
- 한 번 미분하게되면 평균 벡터를 얻을 수 있음<br>
  $\Sigma_{a\mid b}^{-1}\boldsymbol{\mu}_{a\mid b} =\mathbf{x}_{a}^{T}\left\{\Lambda_{a a} \boldsymbol{\mu}_{a}-\Lambda\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right)\right\}$에서<br>
$\Sigma_{a \mid b}^{-1} \boldsymbol{\mu}_{a \mid b}=\left\{\Lambda_{a a} \boldsymbol{\mu}_{a}-\Lambda\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right)\right\}$<br>
$\boldsymbol{\mu}_{a \mid b}=\Sigma_{a \mid b}\left\{\Lambda_{a a} \boldsymbol{\mu}_{a}-\Lambda_{a b}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right)\right\}$
$=\boldsymbol{\mu}_{a}-\Lambda_{a a}^{-1} \Lambda_{a b}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right) \rightarrow$ **$\mathbf{x}_{b}$에 대해 선형함수임을 주목**

---

### 주변확률분포
- 한 번 더 주변확률분포에 대한 내용을 살펴보면<br>
  - 결합확률분포에 대해서 하나의 변수에 대해서 확률을 구하는 것
  - 즉, $p(x, y)$에서 $x$에 관한 주변확률분포는 $p(x)$, $y$에 관한 주변확률분포는 $p(y)$
  - 따라서 연속확률변수에 대해서 다음과 같이 표현이 가능<br>
    $p\left(\mathbf{x}_{a}\right)=\int p\left(\mathbf{x}_{a}, \mathbf{x}_{b}\right) d \mathbf{x}_{b}$<br>
<br>
- 주변확률분포를 계산하는 전략은 다음과 같음<br>
  $\begin{aligned} p\left(\mathbf{x}_{a}\right) &=\int p\left(\mathbf{x}_{a}, \mathbf{x}_{b}\right) \mathrm{d} \mathbf{x}_{b} \\ &=\int \alpha \exp \left\{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T} \Sigma^{-1}(\mathbf{x}-\boldsymbol{\mu})\right\} \mathrm{d} \mathbf{x}_{b} \\ &=\int \alpha \exp \left\{f\left(\mathbf{x}_{b}, \mathbf{x}_{a}\right)+g\left(\mathbf{x}_{a}\right)+\text { const }\right\} \mathrm{d} \mathbf{x}_{b} \\ &=\int \alpha \exp \left\{f\left(\mathbf{x}_{b}, \mathbf{x}_{a}\right)-\tau+\tau+g\left(\mathbf{x}_{a}\right)+\text { const }\right\} \mathrm{d} \mathbf{x}_{b} \\ &=\int \alpha \exp \left\{f\left(\mathbf{x}_{b}, \mathbf{x}_{a}\right)-\tau\right\} \exp \left\{\tau+g\left(\mathbf{x}_{a}\right)+\text { const }\right\} \mathrm{d} \mathbf{x}_{b} \\ &=\alpha \exp \left\{\tau+g\left(\mathbf{x}_{a}\right)+\text { const }\right\} \int \exp \left\{f\left(\mathbf{x}_{b}, \mathbf{x}_{a}\right)-\tau\right\} \mathrm{d} \mathbf{x}_{b} \\ &=\alpha \beta \exp \left\{\tau+g\left(\mathbf{x}_{a}\right)+\text { const }\right\} \end{aligned}$<br>
<br>
- 주변확률분포를 계산할 때에는 완전제곱식을 이용하여 계산
- 우선 $\mathbf{x}_{b}$에 관하여 적분해야하므로 위의 식에서 $\mathbf{x}_{b}$에 관련된 항만 추출하여 계산
- 의도적으로 이차형식을 만들기 위해 특정항을 더해주고 빼주기<br>
  $\displaystyle -\frac{1}{2} \mathbf{x}_{b}^{T} \Lambda_{b b} \mathbf{x}_{b}+\mathbf{x}_{b}^{T} \mathbf{m}=-\frac{1}{2}\left(\mathbf{x}_{b}-\Lambda_{b b}^{-1} \mathbf{m}\right)^{T} \Lambda_{b b}\left(\mathbf{x}_{b}-\Lambda_{b b}^{-1} \mathbf{m}\right)+\frac{1}{2} \mathbf{m}^{T} \Lambda_{b b}^{-1} \mathbf{m}$ <br>
  (where $\displaystyle \mathbf{m}=\Lambda_{b b} \boldsymbol{\mu}_{b}-\Lambda_{b a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right), \tau = \frac{1}{2} \mathbf{m}^{T} \Lambda_{b b}^{-1} \mathbf{m})$<br>
- 이 때 $-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T} \Sigma^{-1}(\mathbf{x}-\boldsymbol{\mu})$는 $\mathbf{x}_{b}$에 관련된 항이고 $\frac{1}{2} \mathbf{m}^{T} \Lambda_{b b}^{-1} \mathbf{m}$는 $\mathbf{x}_{a}$에 종속되어있는 항<br>
<br>
<br>

- 적분하는 부분은 다음과 같음<br>
  $\displaystyle \int \exp \left\{-\frac{1}{2}\left(\mathbf{x}_{b}-\Lambda_{b b}^{-1} \mathbf{m}\right)^{T} \Lambda_{b b}\left(\mathbf{x}_{b}-\Lambda_{b b}^{-1} \mathbf{m}\right)\right\} d \mathbf{x}_{b}$<br>
<br>
- 이 적분식은 정규화 되지 않은 가우시안 분포를 적분한 값으로 그 값의 역수를 취해주면 정규화 계수가 됨
- 그 이유는 평균과 분산이 각각 0과 1인 정규화된 가우시안 분포의 적분 값은 1이기 때문
- 또한 위의 적분식은 공분산 행렬에 행렬식에 의존하는 특정한 값이므로 독립적인 값으로 간주하고 계산해도 무방<br>
<br>
- 남은 부분은 $\alpha \beta \exp \left\{\tau+g\left(\mathbf{x}_{a}\right)+\text { const }\right\}$이 되고, 해당 행의 지수부를 살펴보면 다음과 같음<br>
$\begin{aligned} \tau+g\left(\mathbf{x}_{a}\right)+ \text{const} &= \frac{1}{2} \mathbf{m}^{T} \Lambda_{b b}^{-1} \mathbf{m}-\frac{1}{2} \mathbf{x}_{a}^{T} \Lambda_{a a} \mathbf{x}_{a}+\mathbf{x}_{a}^{T}\left(\Lambda_{a a} \boldsymbol{\mu}_{a}+\Lambda_{a b} \boldsymbol{\mu}_{b}\right)+ \text{const} \\ &=-\frac{1}{2} \mathbf{x}_{a}^{T}\left(\Lambda_{a a}-\Lambda_{a b} \Lambda_{b b}^{-1} \Lambda_{b a}\right) \mathbf{x}_{a}+\mathbf{x}_{a}^{T}\left(\Lambda_{a a}-\Lambda_{a b} \Lambda_{b b}^{-1} \Lambda_{b a}\right) \boldsymbol{\mu}_{a}+\mathrm{const} \end{aligned}$<br>
<br>
- 위의 $-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T} \Sigma^{-1}(\mathbf{x}-\boldsymbol{\mu}) = -\frac{1}{2} \mathbf{x}^{T} \Sigma^{-1} \mathbf{x}+\mathbf{x}^{T} \Sigma^{-1} \boldsymbol{\mu}+\text {const}$을 이용하여 평균, 공분산을 구하면 다음과 같음<br>
  - 공분산(Schur complement 이용): $\Sigma_{a}=\left(\Lambda_{a a}-\Lambda_{a b} \Lambda_{b b}^{-1} \Lambda_{b a}\right)^{-1} =\Sigma_{a a}$<br>
  - 평균벡터: $\Sigma_{a}\left(\Lambda_{a a}-\Lambda_{a b} \Lambda_{b b}^{-1} \Lambda_{b a}\right) \boldsymbol{\mu}_{a}=\boldsymbol{\mu}_{a}$<br>
<br>
- 따라서 다음과 같이 정리할 수 있음<br>
  $\mathbb{E}\left[\mathbf{x}_{a}\right]=\boldsymbol{\mu}_{a}$<br>
  $\operatorname{cov}\left[\mathbf{x}_{a}\right]=\Sigma_{a a}$

---

### 가우시안 분포를 위한 베이즈 정리

- 가우시안 주변확률분포 $p(x)$와 가우시안 조건부 확률분포 $p(y \mid x)$를 살펴보면
- 우선 조건부 확률분포 조건부 확률분포 $p(y \mid x)$의 평균벡터는 $x$에 대한 선형함수, 분산은 $x$에 독립적이므로 다음과 같이 표현 가능<br>
  $p(\mathbf{x}) =\mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}, \Lambda^{-1}\right)$  
  $p(\mathbf{y} \mid \mathbf{x}) =\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \mathbf{x}+\mathbf{b}, \mathbf{L}^{-1}\right)$<br>
<br>
- 결합확률분포 $\mathbf{z}=\left[\begin{array}{l}\mathbf{x} \\ \mathbf{y}\end{array}\right]$를 살펴보면($p(\mathbf{z})= p(\mathbf{x})p(\mathbf{y} \mid \mathbf{x})$ 양변에 로그를 취해주기)<br>
  $\ln p(\mathbf{z})= \ln p(\mathbf{x})+\ln p(\mathbf{y} \mid \mathbf{x}) =-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T} \Lambda(\mathbf{x}-\boldsymbol{\mu}) -\frac{1}{2}(\mathbf{y}-\mathbf{A x}-\mathbf{b})^{T} \mathbf{L}(\mathbf{y}-\mathbf{A} \mathbf{x}-\mathbf{b})+\text { const } $<br>
<br>
<br>
- 공분산을 구하기 위해 $\mathbf{z}$의 이차항은 다음과 같이 표현<br>
  $\displaystyle -\frac{1}{2} \mathbf{x}^{T}\left(\Lambda+\mathbf{A}^{T} \mathbf{L A}\right) \mathbf{x}-\frac{1}{2} \mathbf{y}^{T} \mathbf{L} \mathbf{y}+\frac{1}{2} \mathbf{y}^{T} \mathbf{L A x}+\frac{1}{2} \mathbf{x}^{T} \mathbf{A}^{T} \mathbf{L} \mathbf{y}=-\frac{1}{2}\left[\begin{array}{l}\mathbf{x} \\ \mathbf{y}\end{array}\right]^{T}\left[\begin{array}{cc}\Lambda+\mathbf{A}^{T} \mathbf{L A} & -\mathbf{A}^{T} \mathbf{L} \\ -\mathbf{L A} & \mathbf{L}\end{array}\right]\left[\begin{array}{l}\mathbf{x} \\ \mathbf{y}\end{array}\right]=-\frac{1}{2} \mathbf{z}^{T} \mathbf{R z}$<br>
  <br>
- 따라서 $\mathbf{R}$은 $\mathbf{z}$의 정확도 행렬<br>
  $\mathbf{R}=\left[\begin{array}{cc}\Lambda+\mathbf{A}^{T} \mathbf{L A} & -\mathbf{A}^{T} \mathbf{L} \\ -\mathbf{L A} & \mathbf{L}\end{array}\right]$<br>
<br>
- 공분산은 다음과 같음<br>
  $\operatorname{cov}[\mathbf{z}]=\mathbf{R}^{-1}=\left[\begin{array}{cc}\Lambda^{-1} & \Lambda^{-1} \mathbf{A}^{T} \\ \mathbf{A} \Lambda^{-1} & \mathbf{L}^{-1}+\mathbf{A} \Lambda^{-1} \mathbf{A}^{T}\end{array}\right]$<br>
  $\operatorname{cov}[\mathbf{y}]= \mathbf{L}^{-1}+\mathbf{A} \Lambda^{-1} \mathbf{A}^{T}$
<br>
<br>
- 평균벡터를 구하기 위해 $\mathbf{z}$의 일차항은 다음과 같이 표현<br>
  $\mathbf{x}^{T} \Lambda \boldsymbol{\mu}-\mathbf{x}^{T} \mathbf{A}^{T} \mathbf{L} \mathbf{b}+\mathbf{y}^{T} \mathbf{L} \mathbf{b}=\left[\begin{array}{l}\mathbf{x} \\ \mathbf{y}\end{array}\right]^{T}\left[\begin{array}{c}\Lambda \boldsymbol{\mu}-\mathbf{A}^{T} \mathbf{L b} \\ \mathbf{L b}\end{array}\right]$<br>
  <br>
  $\mathbb{E}[\mathbf{z}]=\mathbf{R}^{-1}\left[\begin{array}{c}\Lambda \boldsymbol{\mu}-\mathbf{A}^{T} \mathbf{L b} \\ \mathbf{L b}\end{array}\right]=\left[\begin{array}{c}\boldsymbol{\mu} \\ \mathbf{A} \boldsymbol{\mu}+\mathbf{b}\end{array}\right]$
  <br>
  $\mathbb{E}[\mathbf{y}] = \mathbf{A} \boldsymbol{\mu}+\mathbf{b}$
  <br>
  <br>
- 이에 따라 조건부 가우시안 분포의 평균과 공분산은 다음과 같음<br>
  $\Sigma_{x \mid y}=\Lambda_{x x}^{-1}$<br>
$\boldsymbol{\mu}_{x \mid y}=\boldsymbol{\mu}_{x}-\Lambda_{x x}^{-1} \Lambda_{x y}\left(\mathbf{y}-\boldsymbol{\mu}_{y}\right)=\Lambda_{x x}^{-1}\left\{\Lambda_{x x} \boldsymbol{\mu}_{x}-\Lambda_{x y}\left(\mathbf{y}-\boldsymbol{\mu}_{y}\right)\right\}$에서 $\Lambda$는 $\mathbf{R}$에서 값을 찾아 대입하면 되므로 다음을 유도 가능<br>
<br>
  $\begin{aligned} \mathbb{E}[\mathbf{x} \mid \mathbf{y}] &=\left(\Lambda+\mathbf{A}^{T} \mathbf{L} \mathbf{A}\right)^{-1}\left\{\mathbf{A}^{T} \mathbf{L}(\mathbf{y}-\mathbf{b})+\Lambda \boldsymbol{\mu}\right\} \\ \operatorname{cov}[\mathbf{x} \mid \mathbf{y}] &=\left(\Lambda+\mathbf{A}^{T} \mathbf{L} \mathbf{A}\right)^{-1} \end{aligned}$

---

### 가우시안 분포의 최대우도
- 임의의 관찰 데이터 $\mathbf{X}=\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right)^{T}$에서 $\mathbf{x}_{n}$은 서로 독립적으로 발생하며, 각각의 데이터는 가우시안 분포를 따름
- 관찰 데이터에 대해 우도함수를 표현하면 다음과 같음<br>
  $\displaystyle\ln p(\mathbf{X} \mid \boldsymbol{\mu}, \Sigma)=-\frac{N D}{2} \ln (2 \pi)-\frac{N}{2} \ln |\Sigma|-\frac{1}{2} \sum_{n=1}^{N}\left(\mathbf{x}_{n}-\boldsymbol{\mu}\right)^{T} \Sigma^{-1}\left(\mathbf{x}_{n}-\boldsymbol{\mu}\right)$<br>
<br>
- 우도를 최대화 하는 평균벡터 $\boldsymbol{\mu}_{ML}$을 구하면<br>
  $\mathbf{y} = \mathbf{x} - \boldsymbol{\mu}$라고 한다면<br>
  $\frac{\partial}{\partial \boldsymbol{\mu}}(\mathbf{x}-\boldsymbol{\mu})^{T} \Sigma^{-1}(\mathbf{x}-\boldsymbol{\mu})=\frac{\partial}{\partial \mathbf{y}} \mathbf{y}^{T} \Sigma^{-1} \mathbf{y} \frac{\partial \mathbf{y}}{\partial \boldsymbol{\mu}}=-2 \Sigma^{-1} \mathbf{y} \equiv-2 \Lambda \mathbf{y}$가 됨<br>
<br>
- 위의 우도함수를 $\boldsymbol{\mu}$로 미분하고 그 값을 0을 만족시키는 $\boldsymbol{\mu}$값이 $\boldsymbol{\mu}_{ML}$이 됨<br>
  $\displaystyle\frac{\partial}{\partial \boldsymbol{\mu}} \ln p(\mathbf{X} \mid \boldsymbol{\mu}, \Sigma)=-\frac{1}{2} \sum_{i=1}^{N}-2 \Lambda\left(\mathbf{x}_{i}-\boldsymbol{\mu}\right)=\Lambda \sum_{i=1}^{N}\left(\mathbf{x}_{i}-\boldsymbol{\mu}\right)=0$<br>
  $\displaystyle\boldsymbol{\mu}_{M L}=\frac{1}{N} \sum_{i=1}^{N} \mathbf{x}_{i}=\overline{\mathbf{x}}$<br>
<br>
- 이번에는 우도를 최대화 하는 공분산행렬 $\Sigma_{ML}$을 구해보면<br>
  우선 $\Lambda$와 관련된 항을 함수 $l$이라고 정의를 하면 다음과 같이 표현이 가능<br>
  $\displaystyle l(\Lambda)=\frac{N}{2} \ln |\Lambda|-\frac{1}{2} \sum_{n=1}^{N} \operatorname{tr}\left(\left(\mathbf{x}_{n}-\boldsymbol{\mu}\right)\left(\mathbf{x}_{n}-\boldsymbol{\mu}\right)^{T} \Lambda\right)=\frac{N}{2} \ln |\Lambda|-\frac{1}{2} \operatorname{tr}(\mathbf{S} \Lambda)$<br>
  갑작스럽게 trace연산이 나오긴 했으나 $\displaystyle-\frac{1}{2} \sum_{n=1}^{N}\left(\mathbf{x}_{n}-\boldsymbol{\mu}\right)^{T} \Sigma^{-1}\left(\mathbf{x}_{n}-\boldsymbol{\mu}\right)$의 값은 스칼라 값으로 계산하는 것이 아닌 1X1행렬로 계산<br>
  이 때, $\displaystyle\mathbf{S}=\sum_{i=1}^{N}\left(\mathbf{x}_{n}-\boldsymbol{\mu}\right)\left(\mathbf{x}_{n}-\boldsymbol{\mu}\right)^{T}$<br>
  <br>
  <br>
  $l$을 $\Lambda$로 미분해서 그 값을 0을 만족시키는 $\Lambda_{ML}$을 구하면<br>
  $\displaystyle\frac{\partial l(\Lambda)}{\partial \Lambda}=\frac{N}{2}\left(\Lambda^{-1}\right)^{T}-\frac{1}{2} \mathbf{S}^{T}=0$<br>
  $\displaystyle\left(\Lambda_{\mathrm{ML}}\right)^{-1}=\Sigma_{\mathrm{ML}}=\frac{1}{N} \mathbf{S}$<br>
  $\displaystyle\Sigma_{\mathrm{ML}}=\frac{1}{N} \sum_{n=1}^{N}\left(\mathbf{x}_{n}-\boldsymbol{\mu}\right)\left(\mathbf{x}_{n}-\boldsymbol{\mu}\right)^{T}$<br>
<br>
<br>
- 따라서 **관찰된 데이터에 대한 우도함수를 최대화 시키는 평균값과 공분산은 관찰된 데이터들의 평균과 공분산이라는 것을 알 수 있음**

---

### 가우시안 분포를 위한 베이지안 추론
- MLE방법으로 평균 혹은 분산 한 가지 값만 구할 수 있었음
- 베이지안 방법을 이용하면 확률분포 자체를 구할 수 있음
- 우도함수는 다음과 같음<br>
  $\displaystyle p(\mathbf{x} \mid \mu)=\prod_{n=1}^{N} p\left(x_{n} \mid \mu\right) =\frac{1}{\left(2 \pi \sigma^{2}\right)^{N / 2}} \exp \left\{-\frac{1}{2 \sigma^{2}} \sum_{n=1}^{N}\left(x_{n}-\mu\right)^{2}\right\}$<br>
- 사전확률, 사후확률<br>
  $p(\mu)=N\left(\mu \mid \mu_{0}, \sigma_{0}^{2}\right)$<br> 
  $p(\mu \mid \mathbf{x}) =\mathcal{N}\left(\mu \mid \mu_{N}, \sigma_{N}^{2}\right)$
- 따라서 관측값들을 이용하면 다음과 같이 평균과 분산을 표현할 수 있음<br>
  $\displaystyle \mu_{N} =\frac{\sigma^{2}}{N \sigma_{0}^{2}+\sigma^{2}} \mu_{0}+\frac{N \sigma_{0}^{2}}{N \sigma_{0}^{2}+\sigma^{2}} \mu_{M L} \\ \mu_{M L} =\frac{1}{N} \sum_{n=1}^{N} x_{n}$ <br> $\displaystyle\frac{1}{\sigma_{N}^{2}} =\frac{1}{\sigma_{0}^{2}}+\frac{N}{\sigma^{2}}$

---

### 느낀점
많이 어렵긴 하지만 수식을 다루는 것이 어려웠던 것이지 사실은 개념적으로 어려운 내용은 없었다.<br>
전체적인 흐름을 잡는 것을 바탕으로 차근차근 해나가면 절대 어려울 것은 없었다고 생각한다.<br>
기초적인 수학 내용이 끝났다. 앞으로는 ML에 대한 내용이 나오게될 텐데<br>
탄탄한 수학지식없이 유연하게 알고리즘을 작성할 수 없다는 사실을 생각한다면<br>
지속적으로 반복학습하는 것이 중요할 것 같다고 생각한다
