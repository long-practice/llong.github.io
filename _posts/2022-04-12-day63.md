---
title: 프로그래머스 인공지능 데브코스 63일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 13주차(22.3.7.(월) ~ 22.3.11.(금)) 
  Spark ML Pipeline
  NLP 개요, 언어모델, 단어 임베딩, 텍스트 전처리
tags:
- 자연어 처리(NLP)
- 문서분류
- 규칙 기반 모델
- Naive Bayes 분류기
- 단어 가방
- 스무싱

use_math: True
---

# 63일차(문서분류)

13주차(22.3.14.(월) ~ 22.3.8.(금)): 빅데이터 총정리, NLP
* 빅데이터 총정리
* NLP 기초(텍스트 전처리, 언어모델, 문서분류, 단어 임베딩) 강의
<br>
<br>

출처: 프로그래머스 인공지능 데브코스 3기 강의

### 문서분류
- 텍스트를 입력으로 받아, 텍스트가 어떤 종류의 범주에 속하는지 구분
- 어떤 기사를 읽었을 때, 스포츠 기사인지, 혹은 연예 기사인지, 혹은 정치 기사인지 등등<br>
<br>

- 다양한 분류 문제들이 존재
- 문서의 범주, 주제 분류(ex. 논문 주제 판별)
- 이메일 스팸 분류
- 감성 분류(ex. 리뷰 댓글에 대한 분위기 판별)
- 언어 분류<br>
<br>

#### 정의
- 입력: 문서($d$)와 분류될 클래스($C = \\{c_1, c_2, \cdots, c_j\\}$)
- 출력: 예측한 클래스 $c \in C$<br>
<br>
<br>

#### 규칙 기반 모델
- 단어들의 조합을 사용한 규칙들을 사용
- `A단어가 나타나거나 B단어와 C단어가 함께 나타난다면 1번 클래스에 속하는 문서이다`의 방식
- Precision은 높지만 Recall이 낮음<br>
  즉, 모델이 True라고 했던 것중 실제로 True인 것의 비율은 높지만 **실제 케이스가 True것을 올바르게 예측하는 비율은 떨어짐**
- 단어의 조합 규칙만으로 문서를 분류하는 것은 사실상 어렵고 쉽지 않음<br>
<br>

- **Snorkel**은 각각의 규칙을 "labeling function"으로 간주하고 텍스트 혹은 문서에 규칙들을 적용
- Graphical model의 일종인 factor graph를 사용해서 label이 없는 데이터에 점수를 부여<br>
  다시 표현하면 확률적 목표값을 생성하는 generative model
- 프로젝트 초기 labeled data가 부족하거나 클래스 정의 자체가 애매한 경우 매우 유용<br>
  (단, 규칙을 생성하는 것은 쉽다고 가정)
- 확률적 목표값이 생성된 후에 딥모델 등 다양한 모델 사용가능<br>
<br>
<br>

#### 적용 가능한 여러가지 모델
- Naive Bayes
- Logistic Regression
- Neural Networks
- k-Nearest Neighbors<br>
<br>


---

### Naive Bayes 분류기
- Naive Bayes 가정과 단어 가방 표현에 기반한 간단한 모델
- 문서 $d$와 클래스 $c$가 주어졌을 때<br>
  $\displaystyle P(c \mid d) = \frac{P(d \mid c) P(c)}{P(d)}$
- 이 때 어떤 클래스로 분류될지는 해당 클래스로 분류될 확률이 가장 높은 클래스에 따라 결정<br><br>
  $\begin{aligned} c_{\text {MAP }} &=\underset{c \in C}{\operatorname{argmax}} P(c \mid d) \Rightarrow \text{가장 높은 확률로 분류될 클래스} \\\\\\ &=\underset{c \in C}{\operatorname{argmax}} \frac{P(d \mid c) P(c)}{P(d)} \Rightarrow \text{베이즈 정리 적용} \\\\\\ &=\underset{c \in C}{\operatorname{argmax}} P(d \mid c) P(c) \Rightarrow \text{c와 무관한 d무시, 분모 제거, 우도와 사전확률의 곱으로 나타냄} \\\\\\ &=\underset{c \in C}{\operatorname{argmax}} P\left(x_{1}, x_{2}, \cdots, x_{n} \mid c\right) P(c) \Rightarrow \text{문서 d는 속성들(단어)} x_1, \cdots, x_n \text{로 나타냄}\end{aligned}$<br>
<br>
- 먼저 사전확률 $P(c)$를 살펴보면 `이전 데이터들에서 얼마나 클래스 c가 자주 발생하는가`에 대한 정보<br>
  즉, 말뭉치 내에서 상대적 빈도로 계산 가능(300개의 문서 중 $c_1$의 개수가 30개라면 $P(c_1) = 0.1$<br>
<br>
- 그러나 우도 $P(x_1, x_2, \cdots, x_n \mid c)$의 경우에는 쉽게 계산이 불가능<br>
  각각의 속성 $x$를 이산적이라고 가정할 때 속성들이 가질 수 있는 값의 개수가 $a$라면 $a^n \times \vert C \vert$만큼 파라미터 필요<br>
  즉, 데이터가 아주 많지 않으면 신뢰할 만한 정확도로 계산하기 힘듦<br>
<br>
<br>

#### 단어 가방
- 어떤 텍스트에서 등장하는 단어들의 빈도 수를 계산하여 정리하여 텍스트(문서)를 표현
- 예를 들어 어떤 벡터가 $(2, 3, 1, 11, \cdots, 5, 0, 0)$일 때 각각의 원소는 vocabulary 내에 있는 단어가 텍스트에 등장한 빈도 수<br>
  이 때 벡터의 크기는 vocabulary에 있는 단어의 수로 고정<br>
  따라서 위와 같이 벡터의 형태로 텍스트의 입력을 줄 수 있음<br>
<br>
- **단어 가방 가정**: 문서에서 단어 위치는 분류될 클래스의 확률에 영향을 주지 않는다(단어들의 빈도 수만 영향을 미친다고 가정)<br>
<br>
<br>

#### Naive Bayes가정
- **조건부 독립 가정**: 클래스가 주어지면 속성들은 독립적<br>
  $P(x_1, \cdots, x_n \mid c) = P(x_1 \mid c)P(x_2 \mid c) P(x_3 \mid c) \cdots P(x_n \mid c)$<br>
  위에서 $a^n \times \vert C \vert$만큼의 파라미터 개수가 아닌 $an \times \vert C \vert$만큼의 파라미터가 필요<br>
<br>

- 위의 분류기의 수식을 간편화하기 위해 단어 **가방 가정**과 **Naive Bayes 가정(조건부독립 가정)**을 적용
- 따라서 위의 식 $c_{MAP} = \underset{c \in C}{\operatorname{argmax}} P\left(x_{1}, x_{2}, \cdots, x_{n} \mid c\right) P(c)$은 다음과 같이 변형 가능<br>
<br>
  $\displaystyle c_{NB} = \underset{c_j \in C}{\operatorname{argmax}} P(c_j) \prod_{i \in positions} P(x_i \mid c_j)$<br>
<br>
<br>

#### Naive Bayes 분류기 모델의 파라미터
- Naive Bayes 분류기는 입력값에 관한 선형모델<br>
  $\displaystyle c_{NB} = \underset{c_j \in C}{\operatorname{argmax}} P(c_j) \prod_{i \in positions} P(x_i \mid c_j)$<br>
<br>

- 확률적 생성 모델을 살펴보면<br>
  $\displaystyle p(C_{k} \mid \mathbf{x})=\frac{p(\mathbf{x} \mid C_{k}) p(C_{k})}{\sum_{j} p(\mathbf{x} \mid C_{j}) p(C_{j})}\rightarrow \ln P(\mathbf{x} \mid C_k) = \frac{\exp (a_{k})}{\sum_{j} \exp (a_{j})}$<br>
  $a_k = \ln P(\mathbf{x} \mid C_k)P(C_k)$<br>
<br>

- $\mathbf{x}$가 $M$차원(문장에서 단어의 위치)$(\mathbf{x} = \{x_1, x_2, \cdots, x_M\})$, $x_m$이 $L$개의 상태(vocabulary에서의 단어의 개수)를 가질 수 있다고 가정<br>
  $x \in \mathbf{x}$는 $L$차원의 one-hot 인코딩이 된 벡터로 가정<br>
<br>

- Navie Bayes가정에 의거, $x$가 one-hot 인코딩이 된 벡터이므로 다음과 같이 표현<br>
  $\displaystyle P(\mathbf{x} \mid c_k) = \prod_{m = 1}^M P(x_m \mid c_k) = \prod_{m =1}^{M}\prod_{l = 1}^L \mu_{kml}^{x_{ml}}$<br>
  이 때 $\mu_{kml}$은 클래스가 $k$로 분류된 텍스트에서 $m$번째 위치한 단어가 vocabulary 상에서 $l$번째 위치한 단어가 될 확률<br>
  $\mu_{kml} = P(x_{ml} = 1 \mid c_k)$<br>
<br>
- 따라서 $a_k$를 다시 표현하면<br>
  $\displaystyle a_k = \ln P(c_k) + \sum_{m = 1}^M \sum_{l = 1}^L x_{ml}\ln\mu_{kml}$<br>
  이 때 $x_{ml}$이 입력이 될 것이고, $\mu_{kml}$이 파라미터가 됨($kml$개의 파라미터)<br>
<br>
  
- 확률적 생성 모델의 결정 경계는 $a_k$에 의해 결정
- $a_k$는 입력$x_{ml}$에 관해 선형적이므로 Naive Bayes모델은 선형 모델이라고 할 수 있음
- 그러나 이 식은 단순히 Naive Bayes 가정만 한 식이기 때문에 완벽한 Naive Bayes 분류기 모델은 아니라고 볼 수 있음<br>
<br>
<br>

#### 단어 가방 가정을 적용
- $P(x_{ml} = 1 \mid c_k) = \mu_{kml}$에서 $x_{ml} = 1$은 $m$번째 위치에 있는 단어가 vocabulary에서 $l$번째 단어인 사건
- 단어 가방 가정은 같은 클래스로 분류된 텍스트에서 단어가 등장할 확률은 단어의 위치와 무관하다는 것이므로 수식으로 표현하면<br>
  $ \mu_{kml} = \mu_{km^{\prime}l}, (m \ne m^{\prime})$<br>
  즉 $k$개의 클래스와 vocabulary 내 $L$개의 단어들만 필요하므로 $kl$개의 파라미터들 $\mu_{kl}$만 필요<br>
<br>

#### 예시
- `It was great`이라는 단어가 있고 분류할 클래스는 `positive`, `negative`
- vocabulary에는 4가지 단어 `(It, was, great, bad)`로 구성<br>
<br>

- 다음과 같이 단어가 벡터로 매칭
- $(1, 0, 0, 0)^T \rightarrow$ bad 
- $(0, 1, 0, 0)^T \rightarrow$ great
- $(0, 0, 1, 0)^T \rightarrow$ It
- $(0, 0, 0, 1)^T \rightarrow$ was<br>
<br>
- 입력 $\mathbf{x}$ `It was great`에서<br>
  $x_1 = (0, 0, 1, 0)^T$, $x_2 = (0, 0, 0, 1)^T$, $x_3 = (0, 1, 0, 0)^T$<br>
  $x_{11} = 0, x_{12} = 0, x_{13} = 1, x_{14} = 0$로 볼 수 있음<br>
<br>

- $\displaystyle P(\text{"It was great"} \mid c_1) = \prod_{m = 1}^M P(x_m \mid c_1) = P(x_1 \mid c_1)P(x_2 \mid c_2)P(x_3 \mid c_3)$<br>
  $P(x_1 \mid c_1) = \mu_{111}^{x_{11}} \times \mu_{112}^{x_{12}} \times \mu_{113}^{x_{13}} \times \mu_{114}^{x_{14}} = \mu_{113}$<br>
<br>

  
- 따라서 다음과 같이 확률값을 구할 수 있음<br>
  $\displaystyle P(\text{"It was great"} \mid c_1) = \prod_{m = 1}^M P(x_m \mid c_1) = \mu_{113}\mu_{124}\mu_{132}$<br>
<br>

- 단어 가방 가정을 적용하면<br>
  $\displaystyle P(\text{"It was great"} \mid c_1) = \mu_{113}\mu_{124}\mu_{132} \rightarrow \mu_{13}\mu_{14}\mu_{12}$<br>
<br>
<br>

#### 최대 로그 우도
- 어떤 문서가 $c_j$ 클래스에 속할 확률<br>
  $\displaystyle \hat{P}(c_{j})=\frac{\operatorname{doccount}(C=c_{j})}{N_{d o c}}$<br>
<br>

- 이에 따라 최대 우도는 다음과 같음(이 때 $\displaystyle \sum_{l = 1}^L \mu_{kl} = 1$를 생각하면 bigram의 최대우도를 구하는 것과 동일하게 생각할 수 있음)<br>
  $\displaystyle \hat{P}(w_{i} \mid c_{j})=\frac{\operatorname{count}(w_{i}, c_{j})}{\sum_{w \in V} \operatorname{count}(w, c_{j})}$<br>
<br>
<br>

#### 스무싱(smoothing)
- 학습 데이터의 희소성 때문에 $P(x_i \mid c_j) = 0$이 되어버리면 전체 확률이 0이 되어버릴 수 있음
- 이를 방지하기 위해 모든 단어들의 최소 빈도를 1로 설정하여 확률이 0이 아닌 아주 작은 값으로 계산이 될 수 있도록 변환<br>
  $\displaystyle \begin{aligned} \hat{P}(w_{i} \mid c) &=\frac{\operatorname{count}(w_{i}, c)+1}{\sum_{w \in V}(\operatorname{count}(w, c)+1)} \\ &=\frac{\operatorname{count}(w_{i}, c)+1}{\sum_{w \in V} \operatorname{count}(w, c)+|V|} \end{aligned}$<br>
<br>
<br>

---

### 요약
- Naive Bayes 모델은 단순한(naive) 모델은 아니다
- 적은 학습데이터로도 좋은 성능을 보임
- 학습과 추론하는 속도가 빠름
- 조건부 독립 가정이 실제 데이터에서 성립할 때 최적의 모델
- 문서 분류를 위한 **베이스라인(baseline) 모델**로 적합

### 느낀점
문서분류를 위한 Naive Bayes 모델에 대해 알아보았다. 수식이 너무 많이 나와 당황스러운 부분이 있었지만<br>
예제를 통해서 그리고 중간중간 변수들에 대한 의미를 곱씹어보면서 쉽게 수식을 이해할 수 있었다.<br>
강의가 비슷하게도 어렵지 않은 내용들을 복잡한 수식으로 표현한 느낌이 많이 들었다.<br>
앞으로도 어려운 수식이라고 무턱대고 겁먹고 도전하지 않는 모습을 보여서는 안될 것 같다.
