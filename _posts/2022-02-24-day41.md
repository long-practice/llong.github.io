---
title: 프로그래머스 인공지능 데브코스 41일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 9주차(22.2.7.(월) ~ 22.1.12.(토)) 
  신경망, 딥러닝 기초
  다층 퍼셉트론
  Weekly Mission
tags:
- 다층 퍼셉트론
- 특징 공간
- 계단함수
- 시그모이드 함수
- 오류 역전파
- 활성함수
- 다층 퍼셉트론의 구조
- 범용적 근사 이론
- 은닉층
- 다층 퍼셉트론의 동작
- 아키텍처
- 초기값
- 학습률
- 목적함수
- 훈련집합
- 평균 제곱 오차
- 연쇄 법칙(chain rule)
- 미니배치 방식
- 자동미분연산
- pytorch

use_math: True
---

# 41일차(신경망 기초 - 다층 퍼셉트론)

9주차(22.2.7.(월) ~ 22.2.12.(토)): 신경망 기초
* 신경망 기초
* 다층 퍼셉트론
* 딥러닝 기초
* monthly project

### 다층 퍼셉트론
- 퍼셉트론(선형 분류기)의 한계는 선형 분리가 불가능한 상황(XOR 분류 문제)에서는 적용하기 어려움
- 그런데 XOR 연산을 예로 들으면 XOR연산은 AND, OR, NOT 연산의 조합으로 구현이 가능
- 따라서 여러개의 퍼셉트론으로 XOR연산을 구현가능<br>
<br>
- 퍼셉트론 2개를 병렬 결합할 때를 생각해보면
- 원래 공간 $\mathbf{x} = (x_1, x_2)^T$를 새로운 특징 공간 $\mathbf{z} = (z_1, z_2)^T$로 변환
- 즉, 새로운 특징 공간 $\mathbf{z}$에서 선형 분리 가능
- 이러한 학습 과정은 사람이 수작업 특징 학습을 수행한 것과 유사 $\rightarrow$ 표현 학습<br>
<br>
- 변형된 새로운 특징 공간$\mathbf{z}$을 선형 분리 수행하는 새로운 퍼셉트론을 순차결합하면 다층 퍼셉트론이 됨
- 즉, 원래 공간에서 선형 분리가 안될 때 데이터들을 새로운 특징 공간으로 변환시킴으로써 선형 분리가 가능하게 만듦<br>
  - 행렬 곱: 회전
  - 편향: 이동
  - 비선형 함수: 왜곡

#### 다층 퍼셉트론의 핵심 아이디어
- 은닉층을 두고 특징 공간을 분류하는데 훨씬 유리한 새로운 특징 공간으로 변환
- 시그모이드 활성함수를 도입하여 연성 의사결정이 가능 (cf. 계단함수는 경성 의사결정(1 또는 0))
- 연성에서는 출력이 연속값인데 출력을 신뢰도로 간주함으로써 더 융통성 있게 의사결정 가능
- 오류 역전파 알고리즘 사용<br>
  - 다층 퍼셉트론은 여러 층이 순차적으로 이어진 구조
  - 역방향으로 진행하면서 한 번에 한 층씩 그레디언트를 계산하고 가중치를 갱신

#### 다층 퍼셉트론의 용량(capacity)
- 3개의 퍼셉트론을 결합한 경우를 생각하면
- 2차원 공간을 7개의 영역으로 나누고 각 영역을 3차원 점으로 변환
- 계단함수를 활성함수 $\tau$로 사용을 가정했으므로 **7개의 영역을 7개의 점으로 변환(3차원 공간으로 변환)**
- 따라서 일반화를 하면 **$p$개의 퍼셉트론이 결합되면 $p$차원 공간으로 변환**

---

### 활성함수
- 딱딱한 공간 분할과 부드러운 공간 분할(경성 의사결정 vs 연성 의사결정)<br>
  - 계단함수는 딱딱한(경성) 의사결정 $\rightarrow$ 영역을 점으로 변환
  - 그 외 활성함수(시그모이드, 하이퍼볼릭 탄젠트 시그모이드, rectifier, softplus, ...)는 부드러운(연성) 의사결정 $\rightarrow$ 영역을 영역으로 변환<br>
    부드러운 의사결정을 하는 활성함수는 **모든 구역에서 연속인 그래프 형태로 나타나짐**<br>
<br>

#### 시그모이드 함수
- 대표적인 비선형 함수로 S자 모양의 활성함수<br>
- 이진 시그모이드 함수: $\displaystyle \tau_1(x) = \frac{1}{1 + e^{\alpha x}}$
- 양극 시그모이드 함수: $\displaystyle\tau_2(x) = \frac{2}{1 + e^{\alpha x}} - 1$ 
- $\alpha$($e$의 지수부의 입력변수의 계수)가 커질수록 계단함수에 가까워짐
<br>
<br>

#### 활용 예
- 퍼셉트론은 계단함수
- 다층 퍼셉트론은 로지스틱 시그모이드와 하이퍼볼릭 탄젠트 함수<br>
  - 양 극단에 데이터를 배치하기 때문
- 심층학습은 ReLU(rectified linear activation)<br>
  - S자 모양의 넒은 포화곡선은 경사도 기반한 학습(오류 역전파)을 어렵게 함
  - 오류 역전파 알고리즘을 적용할 때 어떤 데이터를 입력 값이 크게 되면 그레디언트가 없어지는 경우가 발생(시그모이드 함수의 경우)
  - 따라서 손실이 발생했음에도 불구 중간에 그레디언트가 없어지게 됨(gradient vanish) 이에 따라 수 많은 층을 거쳐야하는 깊은 신경망 학습이 불가<br>
<br>

---

### 다층 퍼셉트론의 구조
- $d+1$개의 입력 노드($d$는 특징 개수), $c$개의 출력 노드($c$는 부류 개수)
- $p$개의 은닉 노드: $p$는 하이퍼 매개변수(사용자가 정해주는 매개변수)
- $p$가 너무 크면 과대적합, 너무 작으면 과소적합
- **다층 퍼셉트론은 입력층 - $p$개의 은닉층 - 출력의 $p$층 구조**
- 주로 3 ~ 4층을 사용하고 그 이상의 구조를 갖는 경우 깊은 신경망 학습<br>
![2.png](attachment:2.png)<br>
<br>

#### 범용적 근사 이론
- 하나의 은닉층은 함수의 근사
- 다층 퍼셉트론도 공간을 변환하는 근사 함수<br>
<br>

#### 얕은 은닉층의 구조
- 지수적으로 더 넓은 폭이 필요할 수 있음(더 많은 가중치를 필요로할 수 있음)
- 과대적합이 되기 쉬움
- 일반적으로 깊은 은닉층의 구조가 좋은 성능<br>
<br>

#### 은닉층 깊이에 따른 이점
- 지수의 표현<br>
  - 각 은닉층은 입력 공간을 어디서 접을지 지정 $\rightarrow$지수적으로 많은 선형적인 영역 <br>
<br>
- 즉, 은닉층을 통과했더니 데이터들이 간단하게 혹은 선형적으로 분리가 가능
- 하지만 은닉층에서의 결정 경계는 원래 공간에서 굉장히 복잡한 형태의 결정경계로 그려짐

---

### 다층 퍼셉트론의 동작
- 특징벡터 $\mathbf{x}$를 출력 벡터 $\mathbf{o}$로 사상하는 함수로 간주<br>
  - 2층 퍼셉트론: $\mathbf{o} = f(\mathbf{x}) = f_2(f_1(x))$
  - 3층 퍼셉트론: $\mathbf{o} = f(\mathbf{x}) = f_3(f_2(f_1(x)))$
  - 깊은 신경망: $\mathbf{o} = f_L( \cdots f_2(f_1(x))), L \ge 4$<br>
<br>
- 행렬로 표기<br>
  $\mathbf{o = \tau(U^2 \tau_h(U^1x))}$<br>
<br>
- 은닉층은 새로운 특징 공간으로 변환하는 역할을 하는데 다시 표현하면 특징 벡터를 분류에 더 유리한 새로운 특징 공간으로 변환하는 것을 뜻함
- 따라서 은닉층을 통과하는 과정은 데이터의 특징들을 계층화, 추상화하는 과정
- 현대 기계학습에서는 특징학습이라고 부름
- 심층학습은 더 많은 층을 거쳐 계층화된 특징학습을 함<br>
<br>

#### 다층 퍼셉트론 학습 과정
- 순방향 전파(입력층 $\rightarrow$ 은닉층 $\rightarrow$ 오차계산) $\rightarrow$ 전방 계산
- 역방향 전파(그레디언트 계산 $\rightarrow$ 출력층 $\rightarrow$ 입력층) $\rightarrow$ 오류 역전파
- 오류 역전파 계산 시 전방 계산 대비 약 1.5배 ~ 2배의 시간 소요 $\rightarrow$ 비교적 빠름
- 학습 알고리즘은 오류 역전파 반복, 시간복잡도는 $\Theta((cp + dp)nq)$<br>
  ($c$: 분류 수, $d$: 특징 차원, $p$: 은닉층 차원, $n$: 훈련집합의 크기, $q$: 세대 수)<br>
  학습 알고리즘의 시간복잡도는 주로 훈련집합 크기와 훈련을 얼마나 많이 반복하는지가 주로 결정

### 신경망의 경험적 개발에서 중요 쟁점
- 아키텍처: 은닉층과 은닉 노드 개수 선정<br>
  - 은닉층과 은닉 노드를 늘리면 신경망의 용량이 커지는 대신 추정할 매개변수가 많아지고 과대적합의 가능성
- 초기값: 최초 가중치를 초기할 때 값의 범위와 분포를 어떻게 설정해야 하는지
- 학습률: 처음부터 끝까지 같은 학습률을 사용할 것인지, 아니면 처음에는 큰 값으로 나중에는 점점 줄일 것인지
- 활성함수: 어떤 활성함수를 사용할 것인지, 특히 은닉층의 개수가 늘어남에 따라 그레디언트 소멸과 같은 문제를 어떤 활성함수를 사용함으로써 해결할 것인지

---

### 목적함수

#### 훈련집합
- 특징 벡터 집합 $\mathbb{X} = \{x_1, x_2, \cdots, x_n\}$과 부류 벡터 집합 $\mathbb{Y} = \{y_1, y_2, \cdots, y_n\}$
- 부류 벡터는 단발성(one-hot) 코드로 표현, 즉 $y_i = (0, 0, \cdots, 1, \cdots, 0)^T$
- 설계 행렬로 표현하면 다음과 같음<br>
  $\mathbf{X}=\left(\begin{array}{c}\mathbf{x}\_{1}^{\mathrm{T}} \\\\\\ \mathbf{x}\_{2}^{\mathrm{T}} \\\\\\ \vdots \\\\\\ \mathbf{x}\_{n}^{\mathrm{T}}\end{array}\right)$ $, \quad \mathbf{Y}=\left(\begin{array}{c}\mathbf{y}\_{1}^{\mathrm{T}} \\\\\\ \mathbf{y}\_{2}^{\mathrm{T}} \\\\\\ \vdots \\\\\\ \mathbf{y}\_{n}^{\mathrm{T}}\end{array}\right)$
- $\mathbf{Y} = f(\mathbf{X})$에서 모든 샘플을 올바르게 분류하는 함수 $f$를 찾는 일이 기계학습의 목표<br>
<br>

#### 평균 제곱 오차
- L2-norm을 사용하여 두 데이터 값의 차이를 구함
- 온라인 모드는 순차적으로 들어오는 데이터들에 대해 오차를 구함 $\displaystyle e=\frac{1}{2}\|\mathbf{y}-\mathbf{o}\|\_{2}^{2}$
- 배치 모드는 일정 사이즈의 데이터 셋에 대한 각 오차들의 평균을 구함 $\displaystyle e=\frac{1}{2 n} \sum_{i=1}^{n}\left\vert\mathbf{y}\_{i}-\mathbf{o}\_{i}\right\vert\_{2}^{2}$

---

### 오류 역전파 알고리즘의 설계
- 연쇄 법칙으로 구현
- 반복되는 부분식들을 저장하거나 재연산을 최소화(동적 프로그래밍)
- 경사도 계산을 할 때를 살펴보면 입력$w$와 은닉층$x, y$, 출력$z$에서 그레디언트는 $\displaystyle \frac{\partial z}{\partial w}$
- 위의 그레디언트를 은닉층에서의 미분을 포함시켜 연쇄법칙으로 표현을 한다면 다음과 같음<br>
  $\displaystyle \frac{\partial z}{\partial w} = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x} \frac{\partial x}{\partial w} = f^{\prime}(y) f^{\prime}(x) f^{\prime}(w) = f^{\prime}(f(f(w))) f^{\prime}(f(w)) f^{\prime}(w)$<br>
- 위의 식에서 보면 $f(w), f(f(w)), f(\cdots(f(w))$는 연산이 반복되는 부분<br>
<br>
- 아래 그림을 보면 $\displaystyle \frac{d z}{d w}$를 구하는 과정이 오류 역전파 알고리즘<br>
![1.png](attachment:1.png)<br>
<br>

- 위에서 언급한 목적함수를 다시 쓰면<br>
  $\displaystyle J(\Theta)=\frac{1}{2}\|\mathbf{y}-\mathbf{o}(\Theta)\|\_{2}^{2}$
- 2층 퍼셉트론의 경우 $\Theta = \{\mathbf{U}^1, \mathbf{U}^2\}$로 표현이 가능하고, $J(\Theta) = J(\{\mathbf{U}^1, \mathbf{U}^2\})$의 최저점을 찾아주는 경사 하강법은 다음과 같음<br>
  $\left.\begin{array}{l}\mathbf{U}^{1}=\mathbf{U}^{1}-\rho \frac{\partial J}{\partial \mathbf{U}^{1}} \\\\\\ \mathbf{U}^{2}=\mathbf{U}^{2}-\rho \frac{\partial J}{\partial \mathbf{U}^{2}}\end{array}\right\\}$<br>
<br>
- 따라서 오류 역전파 알고리즘은 출력의 오류를 역방향으로 전파하여 그레디언트(경사도)를 계산하는 알고리즘
- 반복되는 부분식들의 경사도의 지수적 폭발 혹은 사라짐을 피해야 함<br>

<br>

#### 여러가지 오류 역전파
- 단일 노드($\text{out} = f(\text{in})$)의 역전파<br>
  $\displaystyle\frac{\partial \mathcal{E}}{\partial \text {in}}=\frac{\partial \mathcal{E}}{\partial \text {out}} \cdot \frac{\partial \text {out}}{\partial \text {in}} =\frac{\partial \mathcal{E}}{\partial \mathrm{out}} \cdot f^{\prime}(\text{in})$<br>
<br>
- 곱셈($\text{out} = \text{in}\_1 \cdot \text{in}\_2$)의 역전파<br>
  $\displaystyle\frac{\partial \mathcal{E}}{\partial \text {in}\_1}=\frac{\partial \mathcal{E}}{\partial \text {out}} \cdot \frac{\partial \text {out}}{\partial \text {in}\_1} =\frac{\partial \mathcal{E}}{\partial \mathrm{out}} \cdot \text{in}\_2$<br>
  $\displaystyle\frac{\partial \mathcal{E}}{\partial \text {in}\_2}=\frac{\partial \mathcal{E}}{\partial \text {out}} \cdot \frac{\partial \text {out}}{\partial \text {in}\_2} =\frac{\partial \mathcal{E}}{\partial \mathrm{out}} \cdot \text{in}\_1$<br>
<br>
- 덧셈($\text{out} = \sum_i \text{in}\_i$)의 역전파<br>
  $\displaystyle\frac{\partial \mathcal{E}}{\partial \text {in}}=\frac{\partial \mathcal{E}}{\partial \text {out}} \cdot \frac{\partial \text {out}}{\partial \text {in}} =\frac{\partial \mathcal{E}}{\partial \mathrm{out}} \cdot 1 = \frac{\partial \mathcal{E}}{\partial \mathrm{out}}$<br>
<br>
- S자 모양 활성함수($\text{out} = \sigma(\text{in})$)<br>
  $\displaystyle\frac{\partial \mathcal{E}}{\partial \text {in}}=\frac{\partial \mathcal{E}}{\partial \text {out}} \cdot \frac{\partial \text {out}}{\partial \text {in}} =\frac{\partial \mathcal{E}}{\partial \mathrm{out}} \cdot \sigma^{\prime}(\text {in}) =\frac{\partial \mathcal{E}}{\partial \text { out }} \cdot[\sigma(\text{in})(1-\sigma(\text{in}))]$<br>
<br>
- 최대화($\displaystyle \text{out} = \max\_{i}\left\\{\text{in}\_{i}\right\\}$)<br>
  $\displaystyle\frac{\partial \mathcal{E}}{\partial \text {in}}=\frac{\partial \mathcal{E}}{\partial \text {out}} \cdot \frac{\partial \text {out}}{\partial \text {in}} = \begin{cases}\frac{\partial \mathcal{E}}{\partial \text { out }} & \text { if in }\_{i} \text { is max } \\\\\\ 0 & \text { otherwise }\end{cases}$<br>
<br>

#### 벡터의 경우 오류 역전파
- 입력이 벡터고 출력이 스칼라일 경우 출력을 입력에 대해서 미분할 때는 다음과 같음<br>
  $\displaystyle \nabla_{\mathbf{x}} z=\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)^{T} \nabla_{\mathbf{y}} z$<br>
<br>
- 즉, 출력($z$)에 대해 은닉층에서의 결과($\mathbf{y}$)로 미분을 하게 되면 그 결과도 벡터형태<br>
  $\nabla_{y} z=\left[\begin{array}{lll}\frac{\partial z}{\partial y_{1}} & \cdots & \frac{\partial z}{\partial y_{n}}\end{array}\right]^{T} \in \mathbb{R}^{n \times 1}$<br>
<br>
- 따라서 $\displaystyle \frac{\partial \mathbf{y}}{\partial \mathbf{x}}$는 야코비언 형태<br>
  $y=g(x): x \in \mathbb{R}^{m} \mapsto \mathbb{R}^{n} \ni y$에서<br>
  $\begin{aligned} \frac{\partial y}{\partial x} &=\left[\begin{array}{llc}\frac{\partial y}{\partial x_{1}} & \cdots & \frac{\partial y}{\partial x_{m}}\end{array}\right] \\\\\\ &=\left[\begin{array}{ccc}\frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{1}}{\partial x_{m}} \\\\\\ \vdots & \ddots & \vdots \\\\\\ \frac{\partial y_{n}}{\partial x_{1}} & \cdots & \frac{\partial y_{n}}{\partial x_{m}}\end{array}\right] \in \mathbb{R}^{n \times m} \end{aligned}$<br>
<br>
- 정리하면 다음과 같음<br>
  $\begin{aligned}\left(\frac{\partial y}{\partial x}\right)^{T} \nabla_{y} z &=\left[\begin{array}{ccc}\frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{n}}{\partial x_{1}} \\\\\\ \vdots & \ddots & \vdots \\\\\\ \frac{\partial y_{1}}{\partial x_{m}} & \cdots & \frac{\partial y_{n}}{\partial x_{m}}\end{array}\right]\left[\begin{array}{c}\frac{\partial z}{\partial y_{1}} \\\\\\ \vdots \\\\\\ \frac{\partial z}{\partial y_{n}}\end{array}\right] \\\\\\ &=\left[\begin{array}{c}\frac{\partial z}{\partial y_{1}} \frac{\partial y_{1}}{\partial x_{1}}+\cdots+\frac{\partial z}{\partial y_{n}} \frac{\partial y_{n}}{\partial x_{1}} \\\\\\ \vdots \\\\\\ \frac{\partial z}{\partial y_{1}} \frac{\partial y_{1}}{\partial x_{m}}+\cdots+\frac{\partial z}{\partial y_{n}} \frac{\partial y_{n}}{\partial x_{m}}\end{array}\right]=\left[\begin{array}{c}\frac{\partial z}{\partial x_{1}} \\\\\\ \vdots \\\\\\ \frac{\partial z}{\partial x_{m}}\end{array}\right]=\nabla_{x} z \end{aligned}$<br>
<br>
- 결론: 야코비언 행렬과 그레디언트를 곱한 연쇄 법칙으로 얻어서 구함(원리는 동일)

---

### 미니배치 방식
- 한 번에 $t$개의 샘플을 처리($t$는 미니배치 크기)<br>
  - $t = 1$: 확률론적 경사 하강법
  - $t = n$: 배치 경사 하강법
- 미니배치 방식은 보통 $t = 10n \~ 100n$<br>
  - 경사도의 잡음을 줄여주는 효과 때문에 수렴이 빠름
  - GPU를 사용한 병렬처리에도 유리
- **현대 기계 학습은 미니배치 기반의 확률론적 경사 하강법을 표준**

---

### 자동미분연산 실습
<br>
<br>

---

<br>

### 느낀점
다층 퍼셉트론에 대해서 알아보았는데 다양한 예제와 시각화 자료를 통해서 쉽게 이해할 수 있었다.<br>
또한 역전파 알고리즘이 어떻게 이루어지는지 잘 알게 되었으나<br>
실습과 이론은 서로 다른 영역이라는 것을 알 수 있었다.<br>
반복되는 실습을 통해서 코드작성에 익숙해져야할 것 같고 많은 경험을 쌓아야할 것 같다
