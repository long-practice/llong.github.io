---
title: 프로그래머스 인공지능 데브코스 60일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 13주차(22.2.28.(월) ~ 22.3.4.(금)) 
  Spark ML Pipeline
  NLP 개요, 언어모델, 단어 임베딩, 텍스트 전처리
tags:
- Spark mllib 모델 튜닝
- ML Pipeline
- 머신러닝 파일 포맷
- PMML

use_math: True
---
# 60일차(Spark ML Pipeline, Tuning, PMML)

13주차(22.3.14.(월) ~ 22.3.8.(금)): 빅데이터 총정리, NLP
* 빅데이터 총정리
* NLP 기초(텍스트 전처리, 언어모델, 문서분류, 단어 임베딩) 강의
<br>
<br>

출처: 프로그래머스 인공지능 데브코스 3기 강의

### Spark MLlib 모델 튜닝
- 최적의 하이퍼 파라미터를 구하는 과정
- 모델 선택의 중요한 부분은 테스트 방법(**교차 검증(Cross Validation)과 홀드아웃(Train-Validation Split) 테스트 방법**을 지원)
- 하나씩 테스트를 할 것인지 아니면 다수의 경우를 동시 테스트 할 것인지 결정해야 함<br>
<br>
- 아래 세 가지 입력을 줌으로써 모델을 튜닝
    - **Estimator**: ML 모델 혹은 ML Pipeline
    - **Evaluator**: ML 모델 성능을 나타내는 지표(metrics)<br>
      - evaluate 함수가 제공, 인자로 테스트셋의 결과가 들어있는 DataFrame과 파라미터가 제공
      - ML 알고리즘에 따라 다양한 성능지표가 존재
    - **Parameter**: 훈련 반복 횟수 등의 하이퍼 파라미터(parameter map)<br>
      - ParamGridBuilder를 이용하여 ParamGrid 타입의 변수 생성<br>
    - 최종적으로 가장 좋은 결과 모델을 리턴<br>
<br>




### ML Pipeline 사용절차
- 트레이닝 셋에 수행해야 하는 `feature transform`들을 생성
- 사용하고자 하는 머신러닝 모델 알고리즘(`Estimator`)을 생성
- 순서대로 Python 리스트에 추가하고, `Pipeline` 객체를 생성
- 모델 빌딩 방법은 2가지<br>
  - Pipeline의 fit함수를 호출하면서 트레이닝셋 DataFrame 지정
  - ML Tuning 객체로 지정해서 여러 하이퍼 파라미터를 테스트해보고 가장 좋은 결과를 선택<br>
<br>

#### 코드 예시

```
stringIndexer = StringIndexer(...)
imputer = Imputer(...)
assembler = VectorAssembler(...)
minmax_scaler = MinMaxScaler(...)
algo = LogisticRegression(...)

stages = [stringIndexer, imputer, assembler, minmax_scaler, algo]

pipeline = Pipeline(stages = stages)

lr_model = pipeline.fit(train)
lr_cv_predictions = lr_model.transform(test)
evaluator.evaluate(lr_cv_predictions)
```

### ML Tuning 사용 절차
- 테스트하고 싶은 머신러닝 알고리즘 객체 생성(혹은 ML Pipeline)
- `ParamGrid`를 만들어 테스트하고 싶은 하이퍼 파라미터 지정
- `Crossvalidation` 혹은 `TrainValidationSplit` 생성
- `fit` 함수 호출한 후 최선의 모델 선택
- `getEstimatorParamMaps`, `getEvaluator`의 조합으로 어느 하이퍼 파라미터 조합으로 최선의 결과를 냈는지 확인 가능<br>
<br>

#### 코드 예시
```
paramGrid = (ParamGridBuilder()
             .addGrid(algo.maxIter, [1, 5, 10])
             .build())
cv = CrossValidator(
     estimator = pipeline,
     estimatorParamMaps = paramGrid,
     evaluator = evaluator,
     numFolds=5)

cvModel = cv.fit(train)

lr_cv_predictions = cvModel.transform(test)
evaluator.evaluate(lr_cv_predictions)
```

---

### 실습 - Titanic 예제를 통한 ML Pipeline 구현

#### ML Pipeline 만들기


```python
from pyspark.ml.feature import Imputer, StringIndexer, VectorAssembler, MinMaxScaler

# Gender
stringIndexer = StringIndexer(inputCol = "Gender", outputCol = "GenderIndexed")

# Age
imputer = Imputer(strategy="mean", inputCols=["Age"], outputCols=["AgeImputed"])

# Vectorize
inputCols = ["Pclass", "SibSp", "Parch", "Fare", "AgeImputed", "GenderIndexed"]
assembler = VectorAssembler(inputCols=inputCols, outputCol="features")

# MinMaxScaler
minmax_scaler = MinMaxScaler(inputCol="features", outputCol="features_scaled")

stages = [stringIndexer, imputer, assembler, minmax_scaler]
```


```python
from pyspark.ml.classification import LogisticRegression

# algorithm
algo = LogisticRegression(featuresCol = "features_scaled", labelCol="Survived")
lr_stages = stages + [algo]
```


```python
from pyspark.ml import Pipeline
pipeline = Pipeline(stages = lr_stages)
```


```python
df = data.select(["Survived", "Pclass", "Gender", "Age", "SibSp", "Parch", "Fare"])
df.show(10)
```

    +--------+------+------+----+-----+-----+-------+
    |Survived|Pclass|Gender| Age|SibSp|Parch|   Fare|
    +--------+------+------+----+-----+-----+-------+
    |       0|     3|  male|22.0|    1|    0|   7.25|
    |       1|     1|female|38.0|    1|    0|71.2833|
    |       1|     3|female|26.0|    0|    0|  7.925|
    |       1|     1|female|35.0|    1|    0|   53.1|
    |       0|     3|  male|35.0|    0|    0|   8.05|
    |       0|     3|  male|null|    0|    0| 8.4583|
    |       0|     1|  male|54.0|    0|    0|51.8625|
    |       0|     3|  male| 2.0|    3|    1| 21.075|
    |       1|     3|female|27.0|    0|    2|11.1333|
    |       1|     2|female|14.0|    1|    0|30.0708|
    +--------+------+------+----+-----+-----+-------+
    only showing top 10 rows
    



```python
# pipeline 사용법 1
# fit 함수 호출하여 트레이닝 셋에 대해 학습
 
train, test = df.randomSplit([0.7, 0.3])

lr_model = pipeline.fit(train)
lr_cv_predictions = lr_model.transform(test)
evaluator.evaluate(lr_cv_predictions)
```




    0.8147465437788013



#### ML Tuning 적용


```python
from pyspark.ml.evaluation import BinaryClassificationEvaluator

evaluator = BinaryClassificationEvaluator(labelCol="Survived", metricName="areaUnderROC")
```


```python
# pipeline 사용법 2
# ML Tuning 적용하여 최적의 파라미터로 예측

from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

paramGrid = (ParamGridBuilder()
             .addGrid(algo.maxIter, [1, 5, 10])
             .build())

cv = CrossValidator(
    estimator=pipeline,
    estimatorParamMaps=paramGrid,
    evaluator=evaluator,
    numFolds=5
)
```


```python
cvModel = cv.fit(train)
lr_cv_predictions = cvModel.transform(test)
evaluator.evaluate(lr_cv_predictions)
```




    0.8045424621461483




```python
lr_cv_predictions.select("prediction", "survived").show(10)
```

    +----------+--------+
    |prediction|survived|
    +----------+--------+
    |       0.0|       0|
    |       0.0|       0|
    |       0.0|       0|
    |       0.0|       0|
    |       1.0|       0|
    |       1.0|       0|
    |       0.0|       0|
    |       0.0|       0|
    |       0.0|       0|
    |       0.0|       0|
    +----------+--------+
    only showing top 10 rows
    



```python
# 최적의 하이퍼 파라미터 확인
import pandas as pd

params = [{p.name: v for p, v in m.items()} for m in cvModel.getEstimatorParamMaps()]
pd.DataFrame.from_dict([
    {cvModel.getEvaluator().getMetricName(): metric, **ps}
    for ps, metric in zip(params, cvModel.avgMetrics)
])
```





  <div id="df-9d72414c-2a8f-43b1-8153-08b9c6dcc482">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>areaUnderROC</th>
      <th>maxIter</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.840481</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.861241</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.861871</td>
      <td>10</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-9d72414c-2a8f-43b1-8153-08b9c6dcc482')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-9d72414c-2a8f-43b1-8153-08b9c6dcc482 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-9d72414c-2a8f-43b1-8153-08b9c6dcc482');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>




#### GBT Classifier 알고리즘 이용


```python
from pyspark.ml.classification import GBTClassifier

gbt = GBTClassifier(featuresCol="features_scaled", labelCol="Survived")
# 기존 데이터 전처리 과정 + gbt 알고리즘 
gbt_stages = stages + [gbt]
```


```python
gbt_pipeline = Pipeline(stages = gbt_stages)
```


```python
gbt_paramGrid = (ParamGridBuilder()
                .addGrid(gbt.maxDepth, [2, 4, 6])
                .addGrid(gbt.maxBins, [20, 60])
                .addGrid(gbt.maxIter, [10, 20])
                .build())

gbt_cv = CrossValidator(
    estimator = gbt_pipeline,
    estimatorParamMaps=gbt_paramGrid,
    evaluator=evaluator,
    numFolds=5
)
```


```python
gbt_cvModel = gbt_cv.fit(train)
lr_gbt_cv_predictions = gbt_cvModel.transform(test)
evaluator.evaluate(lr_gbt_cv_predictions)
```

    Exception ignored in: <function JavaWrapper.__del__ at 0x7f926cec0290>
    Traceback (most recent call last):
      File "/usr/local/lib/python3.7/dist-packages/pyspark/ml/wrapper.py", line 42, in __del__
        if SparkContext._active_spark_context and self._java_obj is not None:
    AttributeError: 'Imputer' object has no attribute '_java_obj'





    0.8409809084924292



---

### 범용 머신러닝 모델 파일 포맷
- 모델을 설계하고 만드는 과정과 배포하고 실제로 사용하는 과정 사이에는 수많은 단계가 존재
- 이를 해결하기 위해 모델링하는 framework과 모델을 배포하는 framework를 하나로 통일(**AWS SageMaker**)
- 각 framework에서 만들어내는 머신러닝 모델 파일을 동일한 형태의 파일포맷으로 배포(**PMML**)<br>
<br>

- 다양한 머신러닝 개발 플랫폼: Scikit-Learn, PyTorch, Tensorflow 등 + Spark MLlib
- 이런 환경에서 통용되는 머신러닝 파일포맷 혹은 모듈: **PMML, MLeap**이 대표적
- 머신러닝 모델 서빙환경의 통일이 가능(그러나 공통 파일포맷이 지원해주는 기능이 미약해서 복잡한 모델의 경우 지원불가)<br>
<br>

### PMML(Predictive Model Markup Language)
- Machine Learning 모델을 마크업 언어로 표현해주는 XML 언어
- 간단한 입력 데이터 전처리와 후처리도 지원하지만 제약사항이 많음(복잡한 모델의 경우 지원불가)<br>
<br>

#### 전체적인 절차
1. `ML Pipeline`을 `PMML`파일로 저장<br>
  - `pyspark2pmml`파이썬 모듈 설치
  - `pyspark2pmml.PMMLBuilder`를 이용하여 `ML Pipeline`을 `PMML` 파일로 저장<br>
  <br>
  - 코드 예시(`cvModel`은 `ML Pipeline`, `train_fr`은 트레이닝셋 DataFrame)
    ```
    from pyspark2pmml import PMMLBuilder
    
    pmmlBuilder = PMMLBuilder(spark.sparkContext, train_fr, cvModel)
    pmmlBuilder.buildFile("Titanic.pmml")
    ```
    <br>
<br>

2. `PMML` 파일을 기반으로 모델 예측 API로 론치<br>
  - Openscoring framwork
  - AWS SageMaker
  - Flask + PyPMML<br>
<br>

3. API로 입력값을 전송하고 예측값을 받아오는 클라이언트 코드 작성<br>
  - 모델 로딩 및 예측 코드<br>
    ```
    from pypmml import Model
    model = Model.load('single_iris_dectree.pmml')
    
    model.predict({'sepal_length':5.1, 'sepal_width':3.5, 'petal_length':1.4, 'petal_width':0.2})
    ```

### 느낀점
지난주 과제를 하면서 ML pipeline을 어떻게 구성을 해야할까 막막했는데 막상 실습을 하고나서<br>
어떻게 하면 좋을지 알 수 있었다. 굉장히 코드가 간편해져서 놀라웠고, 유용하게 사용할 수 있을 것 같다.<br>
나중에 현업에서 막막할 때 기록한 것들을 보면서 잘 적용할 수 있을 것 같다.<br>
<br>
그러나 현업에서는 위 코드를 이용해서 응용을 해야하기 때문에 결국에는 기본 개념을 잘 알아야 할 것 같고, <br>
반드시 복습을 통해서 공부를 계속해야 할 것 같다.
