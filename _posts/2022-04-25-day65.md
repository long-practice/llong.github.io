---
title: 프로그래머스 인공지능 데브코스 65일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 14주차(22.3.21.(월) ~ 22.3.26.(토)) 
  NLP(Transformer, BERT)
  Visual Recognition(물체 인식, Faster RCNN, YOLO)
tags:
- 자연어 처리(NLP)
- subword 임베딩
- sentencepiece
- SGNS
- Negative Sampling

use_math: True
---

# 65일차(weekly mission)

14주차(22.3.21.(월) ~ 22.3.26.(토)): NLP, Visual Recognition
* NLP(Transformer, BERT)
* Visual Recognition(물체 인식, Faster RCNN, YOLO)
<br>
<br>

출처: 프로그래머스 인공지능 데브코스 3기 강의

### subword 임베딩
- 하나의 단어를 더 작은 의미있는 단위로 토큰화
- 희귀 단어, 신조어에 대한 문제를 완화시킬 수 있음
- 언어의 특성에 따라 어느정도 의미있는 단위로 나누는 것이 가능

#### 코드


```python
# 내부 단어 분리를 위한 유용한 패키지인 구글의 sentencepiece 설치
!pip install sentencepiece
```

<br>

```python
import sentencepiece as spm

corpus = 'example.txt'
prefix = 'example'
vocab_size = 20000 # 만들고자하는 vocabulary의 크기

# 아래와 같이 tokenizer 모델을 훈련시키게 되면 {prefix}.model이라는 vocabulary 파일이 현재 디렉토리에 생성
spm.SentencePieceTrainer.train(
    f"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size + 7} + 
    " --model_type=bpe" +
    " --max_sentence_length=999999" + # 문장 최대 길이
    " --pad_id=0 --pad_piece=[PAD]" + # pad (0)
    " --unk_id=1 --unk_piece=[UNK]" + # unknown (1)
    " --bos_id=2 --bos_piece=[BOS]" + # begin of sequence (2)
    " --eos_id=3 --eos_piece=[EOS]" + # end of sequence (3)
    " --user_defined_symbols=[SEP], [CLS], [MASK]") # 사용자 정의 토큰
```

<br>

```python
vocab_file = "./example.model" # 생성된 vocabulary 파일
vocab = spm.SentencePieceProcessor() # 모델을 실행시키는 객체
vocab.load(vocab_file) # 모델을 불러오기
```

<br>

```python
# 임베딩 예시
lines = [
    "Being winter, was cold",
    "Can be the 'White Christmas in this winter?"
    "Be careful the cough in winter."
]

for line in lines:
    pieces = vocab.encode_as_pieces(line)
    # 토큰화 결과를 출력
    # ['_Be', '_careful', '_the', '_cou', 'gh', '_in', '_winter', '.'] 이런식으로 토큰화
    
    ids = vocab.encode_as_ids(line)
    # 토큰과 해당 토큰이 vocabulary에서 인덱스가 어떤지를 알려줌
    # [525, 5378, 17, 523, 82, 61, 3012, 11972]
```
<br>
<br>

### SGNS(Skip-Gram with Negative Sampling)
- Skip-gram은 중심 단어로부터 주변 단어를 예측하는 모델
- 그러나 SGNS는 중심 단어와 주변 단어가 모두 입력이 되고, 이 두 단어가 실제 윈도우 크기 내에 존재하는 이웃관계인지 확률을 예측
- Negative Sampling을 하면 중심 단어와 주변 단어(레이블 1)인 단어와 관계가 없는 단어(레이블 0)를 테이블에 입력
- 따라서 중심 단어, 주변 단어 혹은 관계가 없는 단어를 임베딩을 하고 임베딩 벡터들 간 연산하여 예측값을 구할 수 있음<br>
<br>
- Negative Sampling은 연산량에서 큰 이점을 발휘할 수 있음
- Negative Sampling되는 정도만큼만 Softmax함수 연산이 이루어지기 때문

<br>
<br>

### 느낀점
어려운 과제였던 것 같다. 특히 SGNS의 개념을 정확히 이해하는데도 오래 걸렸지만,<br>
Negative Sampling을 사용하지 않고 multi-class classification문제로 풀어보는 것도 어려웠다.<br>
특히 개념적으로 심층적으로 이해하지 못하면 구현하지 못하는데<br>
다시 한 번 개념을 살펴보고 과제를 풀어봐야할 것 같다.
