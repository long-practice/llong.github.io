---
title: 프로그래머스 인공지능 데브코스 9일차
layout: post
description: 2주차(21.12.13.(월) ~ 21.12.17.(금) 인공지능 수학
  선형대수
  자료의 정리
  확률과 확률분포
  추정, 검정, 엔트로피
tags:

- 통계적 추론
- 표본분포
- 표본평균
- 중심극한정리
- 모평균의 추정
- 점추정
- 구간추정
- 신뢰구간
- 모비율의 추정
- 통계적 검정
- 가설검정
- 귀무가설
- 대립가설
- 유의수준
- 검정통계량
- 임계값
- 기각역
- 엔트로피
- 자기정보
- 교차엔트로피
- 손실함수
- 제곱합
- 분류문제

use_math: true


---

# 9일차(표본분포, 추정, 검정, 교차엔트로피)

2주차(21.12.13.(월) ~ 21.12.17.(금)): 인공지능 수학
* 선형대수, 자료의 정리, 확률과 확률분포, 추정, 검정, 엔트로피 강의
* Jupyter Notebook, numpy를 이용한 실습

### 통계적 추론
* 전수조사는 실질적으로 불가능한 경우가 대부분이기에 **표본집단에 대한 조사가 이루어지고 모집단의 해석**이 이루어짐
* 표본조사는 반드시 오차가 발생하기 때문에 적절한 표본추출방법을 선택해야하고, 표본과 모집단의 관계를 잘 확인해야함
* 단순랜덤추론, 난수표 사용, 랜덤넘버 생성기 등의 방법이 있음


```python
# 랜덤넘버 생성기 구현(1~10의 10가지 수)
import random
print(*[random.randint(1, 10) for _ in range(10)])
```

    2 2 6 3 8 5 6 1 6 7


### 표본분포
* 표본조사를 통해 파악하고자 하는 정보: 모수
* 모수의 종류로는 모평균, 모분산, 모비율 등이 있음
* 모수를 추정하기 위해 표본을 선택하여 표본 평균이나 표본 분산등을 계산
* 표본 평균이나 표본 분산과 같은 표본의 특성값을 통계량이라고 함
* 표본의 평균도 어떻게 표본을 선택하느냐에 따라 값이 달라지기 때문에 하나의 확률변수가 됨
* 즉, 표본 평균이 가질 수 있는 값 또한 확률분포가 되므로, **통계량의 확률분포가 곧 표본분포**

### 표본평균
* 모평균을 알아낼 때 사용되는 통계량
* 모평균이 $\mu$이고, 모분산이 $\sigma^2$인 정규모집단에서의 표본 평균은 $\bar{x} = \frac{1}{n}\displaystyle\sum_{i = 1}^{n}x_i$이 되고 표본분포 $\overline{X}$는 정규분포를 따름 $\overline{X} \sim N(\mu, \frac{\sigma^2}{n})$


```python
import numpy as np

# 10000개의 표본 평균
# np.random.normal은 평균이 0, 분산이 1인 정규분포에서 size개의 수를 추출
xbar = [np.mean(np.random.normal(size = 10)) for _ in range(10000)]
# 표본 평균들의 평균, 분산
# 실제로 평균은 0에 가까운 수가 나오고, 분산은 원래 분산의 1/10에 해당하는 수가 나왔음
print("mean %f, var %f"%(np.mean(xbar), np.var(xbar)))
```

    mean 0.003542, var 0.101968


---


```python
# 10000개의 표본 평균, 평균이 10, 분산이 9(표준편차 = 3)인 정규분포에서 10개의 수를 추출
xbar = [np.mean(np.random.normal(loc = 10, scale = 3, size = 10)) for _ in range(10000)]
# 평균은 10에 가까운 수, 분산은 원래 분산의 1/10 = 0.9에 가까운 수
print("mean %f, var %f"%(np.mean(xbar), np.var(xbar)))

import matplotlib.pyplot as plt
# 히스토그램으로 출력
h = plt.hist(xbar,range(5,15))
```

    mean 9.988291, var 0.897739


<img src = "https://user-images.githubusercontent.com/83870423/146624659-d1aa0c11-ab49-481c-b1e7-f8600c6e8404.png">
    


### 중심극한정리
* 평균이 $\mu$이고, 분산이 $\sigma^2$인 ~~정규~~**모집단**에서의 표본 평균은 $\bar{x} = \frac{1}{n}\displaystyle\sum_{i = 1}^{n}x_i$이 됨
* 그러나 n이 충분히 큰 경우(주로 $n \ge 30$) 표본분포 $\overline{X}$는 근사적으로 정규분포를 따름 $\overline{X} \sim N(\mu, \frac{\sigma^2}{n})$


```python
# 0 ~ 10 사이의 수 3개를 선택하여 표본평균을 10000번을 냄
# 평균은 5, 분산은 8.3정도 나옴
n = 3
# 정규모집단이 아닌 임의의 모집단에서 값을 추출
xbar = [np.mean(np.random.rand(n) * 10) for _ in range(10000)]
# 평균은 그대로, 분산은 n을 나눈 값에 가까운 값이 나온다는 것 확인
print("mean %f, var %f"%(np.mean(xbar), np.var(xbar)))

print("n = ", n)
h = plt.hist(xbar, range(0, 10))
```

    mean 4.986451, var 2.769055
    n =  3


<img src = "https://user-images.githubusercontent.com/83870423/146624814-bd5a9abb-fc2a-41ee-a41b-dfb063cb8a1a.png">

    


---


```python
n = 10
xbar = [np.mean(np.random.rand(n) * 10) for _ in range(10000)]
print("mean %f, var %f"%(np.mean(xbar), np.var(xbar)))

print("n = ", n)
h = plt.hist(xbar, range(0, 10))
```

    mean 5.006318, var 0.839486
    n =  10


<img src = "https://user-images.githubusercontent.com/83870423/146624841-5b8281eb-608f-4ab7-9700-cef1da405e9d.png">

    


---


```python
# n이 커질수록 정규분포에 가까워짐
n = 30
xbar = [np.mean(np.random.rand(n) * 10) for _ in range(10000)]
print("mean %f, var %f"%(np.mean(xbar), np.var(xbar)))

print("n = ", n)
h = plt.hist(xbar, range(0, 10))
```

    mean 5.004146, var 0.285262
    n =  30


<img src = "https://user-images.githubusercontent.com/83870423/146624864-99e4be45-2188-4564-8c41-c82c5136f059.png">



---


```python
# 0 ~ 10 사이의 수 3개를 선택하여 표본평균을 10000번을 냄
# 지수분포, 평균은 3, 분산은 9
n = 2
xbar = [np.mean(np.random.exponential(scale = 3, size = n)) for _ in range(10000)]
# 평균은 그대로, 분산은 n을 나눈 값에 가까운 값이 나온다는 것 확인
print("mean %f, var %f"%(np.mean(xbar), np.var(xbar)))

print("n = ", n)
h = plt.hist(xbar, range(0, 10))
```

    mean 3.021416, var 4.689154
    n =  2




<img src = "https://user-images.githubusercontent.com/83870423/146624876-184f72bf-e375-47d5-b6f8-46019db76473.png">

    


---


```python
n = 10
xbar = [np.mean(np.random.exponential(scale = 3, size = n)) for _ in range(10000)]
print("mean %f, var %f"%(np.mean(xbar), np.var(xbar)))

print("n = ", n)
h = plt.hist(xbar, range(0, 10))
```

    mean 2.995051, var 0.878456
    n =  10




<img src = "https://user-images.githubusercontent.com/83870423/146624888-3722c63a-3797-4adf-93b9-ad12b468f391.png">

    


---


```python
# n이 충분히 커지면 정규분포에 근사한다는 것을 확인
n = 30
xbar = [np.mean(np.random.exponential(scale = 3, size = n)) for _ in range(10000)]
print("mean %f, var %f"%(np.mean(xbar), np.var(xbar)))

print("n = ", n)
h = plt.hist(xbar, range(0, 10))
```

    mean 2.997094, var 0.308101
    n =  30




<img src = "https://user-images.githubusercontent.com/83870423/146624900-bb1a598a-81f2-4a19-b4fa-1b6207dc602e.png">



### 모평균의 추정
* 점추정: 표본 평균이 점 추정값(추정량)
* 값 하나 가지고 평균을 추정하는 것이기에 표본에 따라 값이 극단적으로 변할 수 있어서 신뢰도가 많이 떨어짐
* 구간추정: 모평균$\mu$의 $100(1-\alpha)\%$ 신뢰구간<br>
  $\mu$의 추정량 $\pm z_{\alpha / 2}$(추정량의 표준편차)<br>

* 정규분포에서 $\sigma$를 알 때 신뢰구간은 다음과 같음
<center>$(\bar{x} - z_{\alpha / 2}\frac{\sigma}{\sqrt{n}}, \bar{x} + z_{\alpha / 2}\frac{\sigma}{\sqrt{n}})$</center><br>
* 그러나 위의 방식은 실용적이지 못함
* 정규분포가 아니거나 모평균의 표준편차는 알려져 있지않기 때문<br>
<br>
* 그러나 **중심극한정리를 이용하게될 경우 신뢰구간**은 다음과 같이 정의할 수 있음
<center>$(\bar{x} - z_{\alpha / 2}\frac{s}{\sqrt{n}}, \bar{x} + z_{\alpha / 2}\frac{s}{\sqrt{n}})$</center>
* 이 때 $s$는 표본표준편차로 **선택한 표본에 대해 계산이 가능**하기 때문에 유용하게 사용될 수 있음


```python
# 30개의 표본에 대한 95% 신뢰 구간
# xbar = [random.randint(90, 120) / 10 for _ in range(30)]
w = [10.9, 11.6, 11.7, 11.8, 11.4, 11.1, 12.0, 9.0, 10.0, 11.9, 11.2, 9.0, 9.7, 9.3, 9.6, 9.2, 11.6, 9.2, 11.4, 9.4, 11.7, 9.9, 11.6, 11.2, 10.6, 9.9, 12.0, 9.8, 11.2, 9.4]
xbar = np.mean(w)
sd = np.std(w, ddof = 1)
print('평균:', xbar)
print('표준편차:', sd)

import scipy.stats
alpha = 0.05
zalpha = scipy.stats.norm.ppf(1 - alpha/2)
print('zalpha', zalpha)

left = xbar - zalpha*sd/(len(w) ** 0.5)
right = xbar + zalpha*sd/(len(w) ** 0.5)
print(f"신뢰구간:({left:.4f}, {right:.4f})")
```

    평균: 10.609999999999998
    표준편차: 1.0597169433391165
    zalpha 1.959963984540054
    신뢰구간:(10.2308, 10.9892)


### 모비율의 추정
* 점추정: n개의 표본에서 특정 속성을 갖는 표본의 개수로 $\hat{p} = \frac{X}{n}$으로 나타냄
* 모평균이 점추정과 마찬가지로 신뢰도가 떨어지는 특징이 있음<br>
<br>
* 구간추정: n이 충분히 클 때($n\hat{p} > 5, n(1 - \hat{p}) > 5$ 일 때)<br>
  평균: $n\hat{p}$, 분산: $n\hat{p}(1 - \hat{p})$으로 나타낼 수 있음<br>
  확률분포 X는 다음과 같은 정규분포를 따름 ($X\sim N(n\hat{p}, n\hat{p}(1 - \hat{p}))$)
  <br><br>
* X를 표준화시키면 다음과 같음
<center>$ Z = \frac{X - n\hat{p}}{\sqrt{n\hat{p}(1 - \hat{p})}} = \frac{\hat{p} - p}{\sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}}$</center><br>
* 따라서 신뢰구간은 다음과 같음
<center>$ P(\lvert Z \rvert \le z_{\alpha /2}) = P(-z_{\alpha / 2} \le \frac{\hat{p} - p}{\sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}} \le z_{\alpha /2}) = 1 - \alpha$<br></center>
* 모비율 p의 $100(1 - \alpha)\%$ 신뢰구간은 다음과 같이 정의할 수 있음
<center>$\left(\hat{p} - z_{\alpha/2}\sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}, \hat{p} + z_{\alpha/2}\sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}\right)$</center>


```python
# 성인남성 1000명에 대해 430명이 흡연을 하고 있었음
# 흡연률에 대한 90% 신뢰구간을 추정
x, n = 430, 1000
p_hat = x / n
sd = (p_hat*(1 - p_hat) / n) ** 0.5
print('평균:', p_hat)
print('표준편차:', sd)

alpha = 0.1
zalpha = scipy.stats.norm.ppf(1 - alpha/2)
print('zalpha', zalpha)

left = p_hat - zalpha*sd
right = p_hat + zalpha*sd
print(f"신뢰구간:({left:.4f}, {right:.4f})")
```

    평균: 0.43
    표준편차: 0.015655669899432602
    zalpha 1.6448536269514722
    신뢰구간:(0.4042, 0.4558)


### 통계적 검정
* 통계적 가설검정이란 표본평균 $\bar{x}$가 $\mu$보다 얼마나 커야 모평균 $\mu$가 $\mu_0$보다 크다고할 것인지를 검증하는 것
* 예를 들면 우리나라 남성의 평균 수명은 75세라고 했는데 의학기술의 발달로 수명이 늘어났다고 하는데 이 가설이 과연 맞는 것일까?


### 가설 검정
* 귀무가설: 차이가 없거나 의미있는 차이가 없는 경우의 가설 ($H_0 : \mu = \mu_0$)
* 대립가설: 귀무가설에 대립하는 가설 ($H_0 : \mu \ne \mu_0$ or $H_0 : \mu > \mu_0$ or $H_0 : \mu < \mu_0$)
* 랜덤하게 선택한 표본에서 지금의 표본평균$\overline{X}$가 나올확률의 계산이 필요(그래야 귀무가설의 참, 거짓을 판단할 수 있음)<br>
  이를 **검정통계량**이라고 함
* 확률이 낮다는 기준점 $\alpha$(유의수준)가 필요
* 유의수준을 설정하는 이유는 검정통계량도 선택된 표본에 대한 통계량을 바탕으로 계산된 값이기 때문에<br>
  가설을 검증하는데 항상 올바르지는 않음. 따라서 "귀무가설이 맞지만 탈락될 확률"로써 유의수준을 설정
* 따라서 표본평균 $\overline{X}$를 정규분포로 변환하여 검정통계량이 기각역 내에 있는지 없는지 파악
* 검정통계량을 구하는 방법은 다음과 같음
<center>$Z = \frac{\overline{X} - n}{s /\sqrt{n}} \sim N(0, 1)$</center>

### 검정 단계
1. $H_0, H_1$ 설정
2. 유의수준 $\alpha$
3. 검정통계량 계산
4. 기각역 또는 임계값 계산
5. 주어진 데이터로부터 유의성 판정


```python
# 예제 1
# 과자를 생산하는 A회사에서는 과자의 평균 중량이 60g이라고 주장
# 소비자들은 평균 중량은 60g보다 작을 것이라 주장
# 이에 표본 100개를 뽑아서 중량을 구했고, 51g이 나왔으며, 과거의 데이터를 분석해보니 표준편차는 20g
# 이 때 과자의 평균 중량에 대해서 어느 주장이 더 타당한지 유의수준 1%에서 검정

# 1. 귀무가설, 대립가설을 설정
# 귀무가설(H_0): mu >= 60
# 대립가설(H_1): mu < 60

# 2. 유의수준 alpha
alpha = 0.01

# 3. 검정통계량 계산
n = 100
xbar = 51
mu, sd = 60, 20
print('평균: ', xbar)
print('표준편차: ', sd)

z = (xbar - mu)/(sd/(n ** 0.5))
print('검정통계량: ', z)

# 기각역, 임계값 계산
# 이 때, 기각역은 그래프 왼쪽 부분이므로 음수처리
cri = -scipy.stats.norm.ppf(1 - alpha)
print('임계값: ', cri)
```

    평균:  51
    표준편차:  20
    검정통계량:  -4.5
    임계값:  -2.3263478740408408


검정통계량이 기각역 내에 있기 때문에(-4.5 < -2.326...)
귀무가설 거짓!(과자의 평균 중량은 60g미만)

---


```python
# 예제 2
# 어느 게임기의 평균 수명은 500일이라고 알려짐
# 그러나 일부에서는 500일이 아니라고하여 36개의 표본에 대해 조사를하였더니 510일이 나왔음
# 그동안 수집한 자료를 분석한 결과 표준편차는 36일
# 어느 의견이 더 타당한지 유의수준 5%에서 검정

# 1. 귀무가설, 대립가설을 설정
# 귀무가설(H_0): mu == 500
# 대립가설(H_1): mu != 500

# 2. 유의수준 alpha
alpha = 0.05

# 3. 검정통계량 계산
n = 36
xbar = 510
mu, sd = 500, 36
print('평균: ', xbar)
print('표준편차: ', sd)

z = (xbar - mu)/(sd/(n ** 0.5))
print('검정통계량: ', z)

# 기각역, 임계값 계산
# 양측검정이므로 기각역을 구할 때에는 alpha의 절반에 해당하는 값을 표준정규분포표에서 찾아야 함
cri = scipy.stats.norm.ppf(1 - alpha/2)
print('임계값: ', cri)
```

    평균:  510
    표준편차:  36
    검정통계량:  1.6666666666666667
    임계값:  1.959963984540054


검정통계량이 기각역 내에 존재하지 않기 때문에(-1.95 < 1.667 < 1.95)
귀무가설은 참!(게임기의 평균수명은 500일)

---

### 엔트로피
* 자기정보($i(A)$)는 다음과 같이 정의
<center>$i(A) = log_b(\frac{1}{P(A)}) = -log_bP(A)$</center><br>
* 자기정보는 확률이 높은 사건에 대하여 정보가 많지 않지만 낮은 사건에 대하여 정보가 많음<br>
  즉, 자주 일어나지 않은 일에 대해서 더 많은 정보가 담겨있다는 뜻
* 예를 들어 문자를 주고 받을 때 자주 사용하는 문자는 적은 데이터로 자주 사용하지 않은 문자는 많은 데이터로 표현
* 엔트로피는 자기정보의 평균으로 다음과 같이 정의
<center>$H(X) = \sum_{j}{P(A_j)i(A_j)} = -\sum_{j}P(A_j)log_2(P(A_j))$</center><br>
* 엔트로피는 주로 평균비트 수를 표현하는데 사용하고, 데이터 압축에 사용함

### 교차엔트로피
* 확률분포 P에 대한 확률분포 Q의 교차 엔트로피$H(P, Q)$는 다음과 같이 표현
<center>$H(P, Q) = \sum_j{P(A_j)i(A_j)} = -\sum_j{P(A_j)}log_2{Q(A_j)} = -\sum_{x \in X}P(x)log_2Q(x)$</center><br>
* 이 값은 P와 Q가 얼마나 비슷한지를 표현, 즉, 교차엔트로피와 엔트로피 값이 같을 경우 P와 Q가 정확히 일치하는 것을 뜻함
* 몇 가지 예를 들어보면<br>
  1. $P = [1.0, 0], Q_1 = [0.8, 0.2]$인 경우 $H(P, Q) = -log_20.8 = 0.3219$
  2. $P = [1.0, 0], Q_1 = [0.5, 0.5]$인 경우 $H(P, Q) = -log_20.5 = 1$
  3. $P = [1.0, 0], Q_1 = [0.2, 0.8]$인 경우 $H(P, Q) = -log_20.2 = 2.32$<br>
  따라서 **기존 실제 값과 차이가 나게 예측을 하게되는 경우 교차엔트로피 값이 크게 나오게됨**<br>
  이를 줄여주는 방향으로 모델이 학습해야 함!

### 손실함수
* 분류문제는 주어진 대상이 A, B, C중 어느 것인지 판단하는 문제
* 기계학습에서 주어진 대상이 각 그룹에 속할 확률이 원래 데이터(엔트로피)와 비교했을 때 얼마나 다른지 측정해야할 필요가 있음<br>
<br>

* 손실함수는 제곱합으로도 구현이 가능하면 정의는 아래와 같음
<center>$\sum{(p_i - q_i)^2}$</center>
* 확률이 다를수록 큰 값을 갖게되고, 이는 손실함수 정의에 부합
* 다만 학습 속도가 느리다는 단점이 있음<br><br>

* 교차엔트로피는 제곱합의 단점인 학습 속도가 빠르다는 특징이 있음
* 이 때문에 분류문제에서 주로 사용함
* 모델이 예측한 최적의 값은 엔트로피 값이지만 실제로는 엔트로피 값과는 차이가 있는 값으로 예측하게 됨<br>
  교차엔트로피의 최소값이 기존 P의 엔트로피 값이라면 교차엔트로피 값을 최소값으로 줄여나가는 방향으로 모델이 학습하게 됨


```python
# 교차엔트로피 구현
def crossEntrophy(P, Q):
    return sum([-p*np.log2(q) for p, q in zip(P, Q)])

P = list(map(float, input('P = ').split()))
print('P =', P)
print()
while True:
    Q = list(map(float, input('Q = ').split()))
    if len(Q) == len(P):
        print('Q =', Q)
        print('C.E = ',crossEntrophy(P, Q))
        print()
    else:
        break
```

    P = 1.0 0
    P = [1.0, 0.0]
    
    Q = 0.8 0.2
    Q = [0.8, 0.2]
    C.E =  0.3219280948873623
    
    Q = 0.5 0.5
    Q = [0.5, 0.5]
    C.E =  1.0
    
    Q = 0.2 0.8
    Q = [0.2, 0.8]
    C.E =  2.321928094887362
    
    Q = 


### 느낀점
직관적으로 이해하기 어려운 용어들이 많이 등장했으나 실제로 예를 들어서 접근해보면 그리 어려운 내용들은 아니었다고 생각함.
특히 가설검정쪽이 최초에는 이해가 안되고 암울했었으나 예제를 풀어보면서 굉장히 재미를 느꼈던 부분이었음
마찬가지로 여러 예제를 풀어보면서 다양한 모듈 내 함수들을 적재적소에 사용할 줄 알아야 나중에 구현을 잘할 수 있겠다고 느꼈음.
다른 사람들이 이전에 짜놓았던 코드들을 보고 공통적으로 많이 사용하는 기법들을 익혀야겠다고 느꼈음
