---
title: 프로그래머스 인공지능 데브코스 31일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 7주차(22.1.17.(월) ~ 22.1.21.(금)) 
  확률이론
  결정이론과 선형회귀
  확률분포
  캐글 경진대회
  weekly mission을 통한 실습
tags:
- 결정이론
- 이진분류
- 기대손실
- 손실행렬
- 선형회귀
- 선형 기저함수 모델
- 다항 기저함수
- 가우시안 기저함수
- 규제화
- 릿지 회귀(L2 규제)
- 라쏘 회귀(L1 규제)

use_math: True
---
# 31일차(ML_basics - 결정이론, 선형회귀)

7주차(22.1.17.(월) ~ 22.1.21.(토)): ML_basic1
* 확률이론
* 결정이론
* 선형회귀
* 확률분포
* kaggle 경진대회
* Weekly Mission

### 결정이론
- 새로운 $x$가 주어졌을 때 확률 모델$P(x, t)$에 기반해 최적의 결정(분류)을 내리는 것
- 추론단계: 결합확률분포 $P(x, C_k)$를 구하는 것
- 결정단계: 상황에 대한 확률이 주어졌을 때 어떻게 최적의 결정을 내릴 것인지 결정<br>
<br>
- $X$: X-ray 이미지
- $C_1$: 암에 걸린 경우, $C_2$: 암에 거리지 않을 확률
- $\displaystyle P(C_k \mid x) = \frac{P(C_k \mid x)}{P(x)} = \frac{P(x \mid C_k}{\sum_{k} P(C_k, x)} = \frac{P(x \mid C_k)P(C_k)}{P(x)}$
- $P(C_k \mid x) $ 를 최대화시키는 $k$를 구하는 것이 좋은 결정<br>
<br>
- 이진분류의 경우
- 결정영역: $\mathcal{R}\_i = \\{x : \mbox{pred} (x) = C_i \\} $
- 분류오류 확률: $\displaystyle P(\mbox{mis}) = P(x \in \mathcal{R}\_1, C_2) + P(x \in \mathcal{R}\_2, C_1) = \int_{\mathcal{R}\_1} P(x, C_2) dx + \int_{\mathcal{R}\_2} P(x, C_1) dx$
- 즉, 분류오류 확률은 암환자에게 암이 아니라고 진단할 확률과 암환자가 아닌 사람에게 암이라고 진단하는 확률의 합

- 위의 식을 조금 더 일반화 해보면, 즉, Multiclass일 경우(이 경우에는 오류 확률이 아닌 정확하게 맞는 확률을 계산)
- $P( \mbox{correct} ) = \displaystyle \sum_{k = 1}^{K}P(x \in \mathcal{R}\_k, C_k) = \sum_{k = 1}^{K}\int_{\mathcal{R}\_k}P(x, C_k) dx$

### 결정이론의 목표(분류의 목표)
- 결합확률분포가 $P(x, C_k)$가 주어졌을 때 최적의 결정영역 $\mathcal{R}_1, \cdots, \mathcal{R}_k$를 찾는 것
- $\hat{C}(x)$는 $x$가 주어졌을 때 1 ~ k값을 돌려주는 함수
- $ x \in \mathcal{R}_j \Leftrightarrow \hat{C}(x) = j$
- 따라서 최적의 함수 $\hat{C}(x) = j$를 찾는 방법과 동일

### 기대손실
- 모든 결정이 동일한 리스크를 갖는 것은 아님<br>
  (예를 들면 암환자에게 암이 아니라고 진단(위험) vs 암환자가 아닌데 암이라고 진단)<br>
<br>
- 따라서 각각의 결정에 따른 손실 값을 할당
- 손실행렬($L_{kj}$: $C_k$에 속하는 $x$를 $C_j$로 분류할 때 발생하는 손실(또는 비용)

- 데이터에 대한 모든 지식이 확률분포로 표현
- 어떤 데이터 샘플 $x$의 실제 클래스를 결정론적으로 알고 있는 것이 아닌 그것의 확률만 알 수 있음을 가정
- 즉, 우리가 관찰할 수 있는 샘플은 확률분포 $P(x, C_k)$를 통해 생성된 것으로 간주
- 이에 따라 기대손실 $\mathbb{E}(L)$은 **실제 class $k$를 class $j$로 분류했을 때의 확률의 총 합으로 정의**할 수 있고 다음과 같이 표현
- $\mathbb{E}(L) = \displaystyle\sum_{k}\sum_{j}\int_{\mathcal{R}\_j}L_{kj}P(x, C_k)dx$
- 기대손실을 최소화 시켜야 우수한 분류를 할 수 있음

- $\mathbb{E}(L) = \displaystyle \sum_{k}\sum_{j}\int_{\mathcal{R}\_j}L_{kj}P(x, C_k)dx = $ 
  $ \int_{\mathcal{X}} \( \sum_{k = 1}^{K}L_{k\hat{C}(x)}P(C_k | x) \) p(x)dx$ 로 표현이 가능
- 이 때 $\mathbb{E}(L)$은 $\hat{C}(x)$의 값에 따라 변하므로 $\hat{C}(x)$의 범함수(functional)라고 할 수 있음
- 따라서 범함수를 최소화 시키면 됨<br>
  $\begin{cases}L_{kj} = 1 (if k = j) \\ L_{kj} = 0 (if k \ne j) \end{cases}$ 이므로 다음이 성립
  
- $\hat{C}(x) = \mbox{argmin} \displaystyle\sum_{k = 1}^{K}L_{kj}P(C_k \mid x) = \sum_{k = 1}^{K}P(C_k \mid x) - P(C_j \mid x) = 1 - P(C_j \mid x)$
- 따라서 기대손실을 최소화 시키는 것은 범함수 $\hat{C}(x)$를 최대화 시키는 것과 동일
- $\hat{C}(x) = \mbox{argmin} \\{1 - P(C_j \mid x) \\} \rightarrow \mbox{argmax} P(C_j \mid x)$

#### 회귀 문제
- $ t \in \mathcal{R}, L(t, y(x)) = \{y(x) - t\}^2$일 때 
- $F[y] = \mathbb{E}[L] = \int_{\mathcal{R}} \int{\mathcal{X}} \{y(t) - t\}^2P(x, t)dxdt = \int_{\mathcal{X}}\left(\int_{\mathcal{R}}\{y(t) - t\}^2P(t|x)dt\right)p(x)dx$에서
- $x$를 위한 최적의 예측값 $y(x) = \mathbb{E}_t[t|x]$임을 증명

- 오일러-라그랑주 방정식을 이용
- $F[y] = \int_{a}^{b}G(x, y(x), y^\prime(x))dx$에서<br>
  $\frac{\partial{F}}{\partial{y(x)}} = \frac{\partial{G}}{\partial{y}} - \frac{d}{dx}\frac{\partial{G}}{\partial{y^\prime}}$이 성립<br>
<br>
- 따라서 $\frac{\partial{F}}{\partial{y(x)}} = 0$을 만족하는 $y(x)$를 구하면 증명 가능 

- 손실함수 분해
- $\{y(x) - t\}^2 = \[\{y(x) - \mathbb{E}(t|x)\} + \{\mathbb{E}(t|x) - t\}\]^2$로 분해
- $F(y)$를 계산할 때 위의 분해된 식에서 생기는 교차항($2\{y(x) - \mathbb{E}(t|x)\}\{\mathbb{E}(t|x) - t\}$)의 계산 값이 0이 됨을 보이면 됨

### 결정이론 정리

#### 확률모델에 의존할 경우
- 생성모델(generative model): 각 클래스 $C_k$에 대해 분포 $P(x|C_k)$와 사전확률 $P(C_k)$를 구하여 베이즈 정리를 이용하여 사후확률을 구함
- $P(C_k |x) = \frac{P(x|C_k)P(C_k)}{P(x)}, \quad P(x) = \sum_{k}P(x|C_k)P(C_k)$
- 사후확률이 주어졌기 때문에 분류를 위한 결정은 쉽게 이루어질 수 있음
- 결합분포에서 데이터를 샘플링해서 생성할 수 있으므로 이런 방식을 생성모델이라고 함<br>
<br>
- 식별모델(discirminative model): 모든 분포를 다 계산하지 않고 오직 사후확률 $P(C_k|x)$를 구함<br>
<br>

#### 판별함수에 의존할 경우
- 판별함수(discriminant function): 입력 $x$를 클래스로 할당하는 판별함수를 찾는 것으로 확률값 계산은 하지 않음<br>
<br>

#### 회귀문제의 경우
- 결합분포 $P(x, t)$를 구하는 추론 문제를 먼저 풀고 조건부확률분포 $P(t|x)$를 구함
- 주변화(marinalize)를 통해 $\mathbb{E}_t[t|x]$를 구함
- $y(x)$ 구하기

---


```python
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
import numpy as np
```

### 선형회귀
- 주어진 데이터를 직선을 사용해 모델링<br>
  $ y = ax + b$
- 기울기(slope): $a$, y절편(intercept): $b$


```python
# 기울기가 2, y절편이 -5인 직선
rng = np.random.RandomState(1)

# 50개의 난수 생성(0 ~ 1)
x = 10 * rng.rand(50)
y = 2 * x - 5 + rng.randn(50)
plt.scatter(x, y)
plt.show()
```


    
<img width = "1200" src = "https://user-images.githubusercontent.com/83870423/150921089-3885e9a4-a027-4153-80a2-aec35cac3312.png">



```python
# Scikit-Learn의 LinearRegression estimator를 사용해서 위 데이터를 가장 잘 표현하는 직선을 표현
from sklearn.linear_model import LinearRegression

# 절편도 같이 학습
model = LinearRegression(fit_intercept = True)
model.fit(x[:, np.newaxis], y)

xfit = np.linspace(0, 10, 1000)
yfit = model.predict(xfit[:, np.newaxis])

plt.scatter(x, y)
# 빨간선으로 근사한 직선 표현
plt.plot(xfit, yfit, color = "Red")
plt.show()
```


                                  
<img width = "1200" src = "https://user-images.githubusercontent.com/83870423/150921175-3f34d120-dab2-45ad-81e0-d1b40b60953d.png">
    


                                                                                                                                  
                                                                                                                                  

```python
# np.newaxis에 대해
print(x.shape)
# 차원을 하나 늘려줌
print(x[:, np.newaxis].shape)
```

    (50,)
    (50, 1)


모델 학습이 끝난 후 학습된 파라미터들은 `model._[파라미터 이름]`형식으로 저장


```python
# 기울기와 y절편 출력
print("Model slope: ", model.coef_[0])
print("Model intercept: ", model.intercept_)
```

    Model slope:  2.0272088103606953
    Model intercept:  -4.998577085553204


`LinearRegression` estimator는 위의 예제와 같은 1차원 입력뿐만 아니라<br>
다차원 입력을 사용한 선형모델을 다룰 수 있음($ y = a_0 +a_1x_1 + a_2x_2 + \cdots $)<br>
<br>
기하학적으로 초평면으로 데이터를 표현한 것이라고 할 수 있음


```python
# 다차원 평면에 대한 선형회귀
rng = np.random.RandomState(1)
X = 10 * rng.rand(100, 3)
y = 0.5 + np.dot(X, [1.5, -2., 1.])

model.fit(X, y)
print('weight: ', model.coef_)
print('intercept: ',model.intercept_)
```

    weight:  [ 1.5 -2.   1. ]
    intercept:  0.500000000000012


### 선형 기저함수 모델(Linear Basis Function Models)
- 비선형 데이터를 선형함수로 모델링하는 한 가지 방법은 기저함수(Basis function)을 사용하는 것<br>
- $ y = a_0 + a_1x_1 + a_2x_2 + a_3x_3 + \cdots$<br>
- 이 때 $x_1, x_2, x_3$등을 1차원 $x$로 부터 생성할 수 있으며, (x_n = f_n(x)) f_n을 기저함수(basis function)라고 부름<br>
<br>
- 예를 들어 $f_n(x) = x^n$이라는 기저함수를 사용하면 모델은 $y = a_0 + a_1x+a_2x^2 + a_3x^3 +\cdots $형태로 구성<br>
<br>
- 입력 데이터($x$)에 대해서는 선형이 아니지만 계수($a_n$)에 관해서는 선형함수임을 주목
- 1차원 변수인 $x$를 기저함수를 통해 다차원으로 확장시킴으로써 선형모델(linear regression) 사용 가능

### 다항 기저함수(Polynomial Basis Functions)
- $f_n(x) = x^n$형태의 함수를 다항 기저함수(polynomial basis functions)라고 부름
- Scikit-Learn에서 `PolynomialFeatures`라는 transformer를 포함


```python
from sklearn.preprocessing import PolynomialFeatures
x = np.array([2, 3, 4])
poly = PolynomialFeatures(3, include_bias = False)
poly.fit_transform(x[:, None])
```




    array([[ 2.,  4.,  8.],
           [ 3.,  9., 27.],
           [ 4., 16., 64.]])



- `PolynomialFeatures`가 1차원 array를 3차원 array로 변환시켰고 각각의 데이터에 대해 제곱, 세제곱을 한 결과를 할당


```python
# 7차원 변환
from sklearn.pipeline import make_pipeline
poly_model = make_pipeline(PolynomialFeatures(7),
                           LinearRegression())
```

- 다차원 변환을 사용하면 복잡한 데이터를 모델링할 수 있게 됨


```python
# sin 함수 사용, 데이터 생성
x = 10 * rng.rand(50)
y = np.sin(x) + 0.1 * rng.randn(50)
plt.scatter(x, y)
plt.show()
```



                         
<img width = "1200" src = "https://user-images.githubusercontent.com/83870423/150921266-2aa87296-347a-4d89-8ea5-376fa3e7889f.png">




```python
# 모델링
poly_model.fit(x[:, np.newaxis], y)
yfit = poly_model.predict(xfit[:, np.newaxis])

# 7차원 다항식으로 모델링한 결과 출력
# sin함수와 거의 유사하게 나옴
plt.scatter(x, y)
plt.plot(xfit, yfit)
plt.show()
```


<img width = "1200" src = "https://user-images.githubusercontent.com/83870423/150921410-947fd4d8-e0ee-4a11-ad2d-e6907b252bb6.png">
    


### 가우시안 기저함수(Gaussian Basis Functions)
- 가우시안 기저함수는 다음과 같이 정의<br>
  $exp\left\{-\frac{(x - \mu_j)^2}{2s^2}\right\}$<br>
- $\mu_j$는 함수의 위치, $s$는 폭을 결정
- 주어진 데이터를 여러 개의 가우시안 기저함수의 합으로 표현
- 가우시안 기저함수는 Scikit-Learn에 포함되어 있지 않지만 어렵지 않게 구현할 수 있음


```python
# 가우시안 기저함수로 모델링
from sklearn.base import BaseEstimator, TransformerMixin

class GaussianFeatures(BaseEstimator, TransformerMixin):
    
    def __init__(self, N, width_factor = 2.0):
        self.N = N
        self.width_factor = width_factor
        
    @staticmethod
    def _gauss_basis(x, y, width, axis = None):
        arg = (x - y) / width
        return np.exp(-0.5 * np.sum(arg ** 2, axis))
    
    def fit(self, X, y= None):
        self.centers_ = np.linspace(X.min(), X.max(), self.N)
        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])
        return self
    
    def transform(self, X):
        return self._gauss_basis(X[:, :, np.newaxis], self.centers_, self.width_, axis = 1)

    
# 20개의 가우시안 기저함수 생성
# 각 가우시안 기저함수를 선형조합하여 모델링
gauss_model = make_pipeline(GaussianFeatures(20),
                            LinearRegression())

gauss_model.fit(x[:, np.newaxis], y)
yfit = gauss_model.predict(xfit[:, np.newaxis])

# 모델링한 결과 그리기
plt.scatter(x, y)
plt.plot(xfit, yfit)
plt.xlim(0, 10)
plt.show()
```


<img width = "1200" src = "https://user-images.githubusercontent.com/83870423/150921478-683f6abf-b049-4376-8b9e-495f6eeda4cb.png">

    


### 규제화(Regularization)
- 기저함수를 사용함으로써 데이터를 모델링
- 많은 기저함수를 사용하여, 즉, 복잡한 형태로 그래프를 그리다보면 **과대적합** 발생
- 모델이 필요이상으로 flexible해지면서 **데이터가 없는 곳에서 극단적인 값**을 가지는 것을 확인 가능


```python
# 과대적합 확인
# 30개의 가우시안 기저함수 생성
model = make_pipeline(GaussianFeatures(30),
                      LinearRegression())

model.fit(x[:, np.newaxis], y)

plt.scatter(x, y)
plt.plot(xfit, model.predict(xfit[:, np.newaxis]), color = 'Red')

gauss_model.fit(x[:, np.newaxis], y)
plt.plot(xfit, gauss_model.predict(xfit[:, np.newaxis]), color = 'Blue')

plt.legend(['30 gaussian basis Functions', '20 gaussian basis Functions'])
plt.xlim(0, 10)
plt.ylim(-4.0, 1.5)
plt.show()
```


    
<img width = "1200" src = "https://user-images.githubusercontent.com/83870423/150921528-80ca3d2c-f9e7-4c3d-b790-71c79f494764.png">



```python
def basis_plot(model, title = None):
    fig, ax = plt.subplots(2, sharex=True)
    model.fit(x[:, np.newaxis], y)
    ax[0].scatter(x, y)
    ax[0].plot(xfit, model.predict(xfit[:, np.newaxis]))
    ax[0].set(xlabel='x', ylabel='y', ylim = (-1.5, 1.5))
    
    if title:
        ax[0].set_title(title)
        
    ax[1].plot(model.steps[0][1].centers_,
               model.steps[1][1].coef_)
    ax[1].set(xlabel='basis location',
              ylabel='coefficient', 
              xlim=(0, 10))
    
model = make_pipeline(GaussianFeatures(30), LinearRegression())
basis_plot(model)
```


    
<img width = "1200" src = "https://user-images.githubusercontent.com/83870423/150921554-e0ad9165-c633-4939-a7f4-d362fced89ad.png">


- 두 번째 그래프는 각각의 가우시안 기저함수의 크기(계수값)
- 과대 적합이 일어나는 영역에서 인접한 기저함수들의 값이 극단으로 가면서 서로 상쇄(좌측 상단에 단위 1e6 주목)<br>
<br>
- 따라서 큰 계수값에 대해 페널티를 부여해서 과대적합을 어느 정도 극복 가능 $\rightarrow$ **규제**

### 릿지 회귀(Ridge Regression) ($L_2$ Regularization)
- 가장 자주 쓰이는 형태의 규제로 릿지 회귀는 다음과 같이 정의<br>
  $ P = \alpha \displaystyle\sum_{n=1}^{N} \theta_n^2$<br>
- $\alpha$는 규제 강도 조절 파라미터
- 이 형태의 규제는 Scikit-Learn의 `Ridge` estimator에서 사용


```python
# 릿지 회귀, 즉, 규제를 통해 과대적합을 해결하는 모습 확인
# 두 번째 그래프의 단위가 없는 것을 확인
from sklearn.linear_model import Ridge
model = make_pipeline(GaussianFeatures(30), Ridge(alpha = 0.1))
basis_plot(model, title = 'Ridge Regression')
```


<img width = "1200" src = "https://user-images.githubusercontent.com/83870423/150921598-26baf50f-3a54-411b-8fdd-120292163d36.png">

    


### 라쏘 회귀(Lasso Regression($L_1$ Regularization)
- 또 하나 자주 쓰이는 규제방법으로 각 계수들의 절대값의 합을 제한<br>
  $ P = \alpha\displaystyle\sum_{n = 1}^{N}\left\vert \theta_n \right\vert$<br>
- 이 방법은 sparse한 모델을 생성(많은 계수들이 0이 됨)
- $\alpha$ 값으로 규제강도 조절 가능
- 이 형태의 규제는 Scikit-Learn의 `Lasso` estimator에서 사용


```python
# 많은 계수들의 값이 0이 된 것을 확인 할 수 있음
from sklearn.linear_model import Lasso
model = make_pipeline(GaussianFeatures(30), Lasso(alpha = 0.001))
basis_plot(model, title = "Lasso Regression")
```


<img width = "1200" src = "https://user-images.githubusercontent.com/83870423/150921707-30a272c4-1ba6-41d7-8cfe-7456fed72446.png">
    


### 느낀점
직접 코드를 통해 회귀에 대해 알아보았고 조금은 이해가 잘 가지 않았던 과소적합, 과대적합, 규제화에 대한 개념이 잡혔다.<br>
그러나 결국에 코드를 통해 원하는 것들을 구현해내야 하기 때문에 각각의 다양한 파라미터 값들이 무엇을 나타내는지 잘 살펴볼 필요가 있다고 생각한다<br>
또한 코드로 보여줘야 하지만 본질은 기본 개념에 있고, 응용은 기본에서 시작된다는 점을 잊지말고 학습해 나가야겠다.<br>
그리고 수식을 다루다 코드를 다루다보면 조금은 수월한 느낌이 있긴 하지만 직접 수식을 통한 증명을 해보면서 기본 개념을 갈고 닦아야 겠다는 생각도 했다
