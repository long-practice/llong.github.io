---
title: 프로그래머스 인공지능 데브코스 45일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 10주차(22.2.14.(월) ~ 22.2.18.(토)) 
  선형회귀
  선형분류
  Weekly Mission
tags:
- 심층 학습(Deep Learning)
- Alexnet
- VGGNet
- GoogLeNet
- NIN 구조
- 보조 분류기
- ResNet
- 지름길 연결(skip connection)
- ImageNet
- ILSVRC
- 생성모델
- GAN

use_math: True
---
# 45일차(신경망 기초 - Deep Learning 기초)

10주차(22.2.14.(월) ~ 22.2.18.(금)): CNN & RNN
* Deep Learning 기초
* CNN Models
* Deep Learning 최적화
* RNN
* weekly mission

출처: 프로그래머스 인공지능 데브코스 3기 강의

### CNN 신경망 사례연구
- AlexNet
- VGGNet
- GoogLeNet
- ResNet

### 영상분류: 과거에는 매우 어렵고 도전적
- ImageNet: 22,000여 부류에 대해 부류별로 수백 ~ 수만장의 사진을 인터넷에서 수집하여 1,500만 장의 사진을 구축하고 공개<br>
<br>
- ILSVRC: CVPR 학술대회에서 개최하는 대회로 이미지 시각화 분류 인식 대회<br>
- 1000가지 부류에 대해 분류, 검출, 위치 지정 문제
- 120만 장의 훈련집합, 5만 장의 검증집합, 15만 장의 테스트집합
- 우승한 모델은 코드와 학습된 가중치를 공개하여 널리 사용되는 표준 신경망으로 채택

### AlexNet
#### 구조
- 컨볼루션층 5개와 완전연결(Fully Connected)층 3개
- 8개 층에 290,400 - 186,624 - 64,896 - 43,264 - 4,096 - 4,096 - 1,000 노드 배치
- 컨볼루션층은 200만개, FC층은 6500만개 가량의 매개변수
- FC층에 30배 많은 매개변수 $\rightarrow$ **향후 CNN은 FC층의 매개변수를 줄이는 방향으로 발전**<br>
![1.png](attachment:1.png)
- 당시 GPU의 메모리 크기 제한으로 인해 GPU#1, GPU#2으로 분할하여 학습 수행
- 3번째 컨볼루션 층은 GPU#1과 GPU#2의 결과를 함께 사용(GPU#1: 색과 관련되지 않은 특징 추출, GPU#2: 색과 관련된 특징 추출)
- 컨볼루션 층 큰 보폭으로 다운샘플링

#### 성공한 요인
##### 외적요인
- ImageNet이라는 대규모 사진 데이터
- GPU를 사용한 병렬처리

##### 내적 요인
- 활성함수로 ReLU사용
- 지역 반응 정규화 기법 적용<br>
  - 인간 신경망 측면억제 모방, ReLU 활성화 규제
  - 1번째, 3번재 최대 풀링 전 적용
- 과대적합 방지하기 위해 여러 규제 기법 적용<br>
  - 데이터 확대(잘라내기와 반전으로 2048배 확대)
  - 드롭아웃(완전결층에서 사용)
- 테스트 단계에서 앙상블 적용<br>
  - 입력된 영상을 잘라내기와 반전을 통해 데이터의 수를 증가하고<br>
    증가된 영상들의 예측 평균으로 최종 인식
  - 2~3% 만큼 오류율 감소 효과
  
---

### VGGNet
#### 핵심 아이디어
- 3 x 3 크기의 작은 커널 사용
- 신경망을 더욱깊게 만듦(신경망의 깊이가 어떤 영향을 주는지 확인)
- 컨볼루션층을 8 ~ 16개를 두어 AlexNet의 5개에 비해 2 ~ 3배 깊어짐

#### 구조
- 16층짜리 VGG-16 (CONV 13층 + FC 3층)
![2.png](attachment:2.png)<br>

#### 작은 커널의 이점
- GoogLeNet의 인셉션 모듈처럼 이후 깊은 신경망 구조에 영향
- 큰 크기의 커널은 여러 개의 작은 크기 커널로 분해 가능
- 매개변수의 수는 줄면서 신경망은 깊어지는 효과<br>
  5 x 5 입력에 5 x 5 커널을 적용하게되면 매개변수의 개수는 25개<br>
  3 x 3 커널을 적용하게되면 매개변수의 개수는 9 + 9 = 18개(커널 내부 원소의 수 x 입력에 대해 커널이 적용되는 횟수)<br>
  즉, 3 x 3 영역에 대해 컨볼루션 계산이 이루어지기 위해 9개의 가중치 필요<br>
  계산된 컨볼루션 값에 대해서도 9개의 가중치 필요, 총 18개 가중치 필요
- n x n 커널은 1 x n 혹은 n x 1 커널로 분해될 수 있으며, n이 클수록 매개변수의 수는 줄어드는 효과가 큼<br>
<br>

#### 1 x 1 커널
- 신경망 속의 신경망(NIN)에서 유래
- 차원을 통합하고 차원을 축소하는 효과로 인해 연산량이 감소
- m x n의 특징 맵 8개에 1 x 1 커널을 4개 적용하게 되면 m x n 특징 맵 4개가 출력<br>
  따라서 8개의 특징 맵이 4개의 특징맵으로 축소됨<br>
  ![3.png](attachment:3.png)
  
---

### GoogLeNet
#### 핵심 아이디어
- 인셉션 모듈: 수용장의 다양한 특징을 추출하기 위해 NIN의 구조를 확장하여 복수의 병렬적인 컨볼루션 층을 가짐
- 마이크로 네트워크로 MLPConv 대신 네 종류의 컨볼루션 연산을 사용하여 다양한 특징을 추출
- 1 x 1 컨볼루션을 사용하여 차원 축소((매개변수의 수 = 특징 맵의 수)를 줄임 + 깊은 신경망)
- 3 x 3, 5 x 5 같은 다양한 크기의 컨볼루션을 통해 다양한 특징들을 추출

#### NIN 구조
- 기존 컨볼루션 연산을 MLPConv 연산으로 대체
- 커널 대신 비선형 함수를 활성함수를 포함하는 MLP를 사용하여 특징 추출에 유리(FC를 적용함으로써 추출한 패치에 대해 여러가지 경로가 생성)<br>
<br>
- 신경망의 미소 신경망이 주어진 수용장의 특징을 추상화 시도<br>
<br>
- 전역 평균 풀링 사용
- MLPConv가 부류 수만큼 특징 맵을 생성하면, 특징 맵 각각을 평균하여 출력 노드에 입력
- 특징 맵에서 출력 노드까지 VGG 모델의 경우 FC로 인해 매개변수가 많았던 반면,<br>
  NIN에서는 특징 맵 전역에 대해 평균 풀링을 진행하기 때문에 매개변수가 필요하지 않음
![4.png](attachment:4.png)

#### GoogLeNet의 구조
- 인셉션 모듈을 9개 결합한 구조<br>
  - 1 x 1, 3 x 3, 5 x 5 컨볼루션, 3 x 3 Max Pooling을 통한 특징맵 추합
  - 1 x 1 컨볼루션을 통한 차원 축소
- 매개변수가 있는 층 22개, 없는 층(풀링) 5개로 총 27개 층
- 완전 연결층은 1개에 불과
- 백만 개의 매개변수를 가지며, VGGNet의 완전 연결층에 비하면 1%에 불과<br>
![5.png](attachment:5.png)

#### 보조 분류기
- 원 분류기의 오류 역전파 결과와 보조 분류기의 오류 역전파 결과를 결합하여 경사 소멸 문제 완화
- 학습할 때 도우미 역할, 추론할 때 제거

### GoogLeNet 구현

#### 라이브러리 불러오기


```python
import torch
from torch import Tensor
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim import lr_scheduler
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt
from typing import Optional, Tuple,List, Callable, Any

import time
import os
import copy
from collections import namedtuple
```

---

#### 모델 블록


```python
__all__ = ['GoogLeNet', 'googlenet', 'GoogLeNetOutputs', '_GoogLeNetOutputs']

model_urls = {
    'googlenet': 'https://download.pytorch.org/models/googlenet-1378be20.pth',
}

GoogLeNetOutputs = namedtuple('GoogLeNetOutputs', ['logits', 'aux_logits2', 'aux_logits1'])
GoogLeNetOutputs.__annotations__ = {'logits': Tensor, 'aux_logits2': Optional[Tensor],
                                    'aux_logits1': Optional[Tensor]}

_GoogLeNetOutputs = GoogLeNetOutputs

def googlenet(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> "GoogLeNet":
    if pretrained:
        if 'transform_input' not in kwargs:
            kwargs['transform_input'] = True
        if 'aux_logits' not in kwargs:
            kwargs['aux_logits'] = False
        if kwargs['aux_logits']:
            warnings.warn('auxiliary heads in the pretrained googlenet model are NOT pretrained, '
                          'so make sure to train them')
        original_aux_logits = kwargs['aux_logits']
        kwargs['aux_logits'] = True
        kwargs['init_weights'] = True
        model = GoogLeNet(**kwargs)
        state_dict = load_state_dict_from_url(model_urls['googlenet'],
                                              progoress = progress)
        model.load_state_dict(state_dict)
        if not original_aux_logits:
            model.aux_logits = False
            model.aux1 = None
            model.aux2 = None
        return model
    
    return GoogLeNet(**kwargs)    
```

---

#### Conv2d 블록


```python
class BasicConv2d(nn.Module):
    def __init(self, in_channels: int, out_channels: int, **kwargs: Any) -> None:
        super(BasicConv2d, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channesl, bias=True, **kwargs)
        self.bn = nn.BatchNorm2d(out_channels, eps =0.001)
        
    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        return F.relu(x, inplace = True)
```

---

#### Inception 모듈


```python
class Inception(nn.Module):
    def __init__(self, in_channels: int, 
                 ch1x1: int, 
                 ch3x3red: int, ch3x3: int, 
                 ch5x5red: int, ch5x5: int, 
                 pool_proj: int, 
                 conv_block: Optional[Callable[..., nn.Module]] = None) -> None:
        super(Inception, self).__init__()
        if conv_block is None:
            conv_block = BasicConv2d
        self.branch1 = conv_block(in_channels, ch1x1, kernel_size = 1)
        
        self.branch2 = nn.Sequential(
            conv_block(in_channels, ch3x3red, kernel_size=1),
            conv_block(ch3x3red, ch3x3, kernel_size=3, padding=1)
        )
        
        self.branch3 = nn.Sequential(
            conv_block(in_channels, ch5x5red, kernel_size=1),
            conv_block(ch5x5red, ch5x5, kernel_size=3, padding=1)
        )
        
        self.branch4 = nn.Sequential(
            nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True),
            conv_block(in_channels, pool_proj, kernel_size=1)
        )
        
    def _forward(self, x):
        branch1 = self.branch1(x)
        branch2 = self.branch2(x)
        branch3 = self.branch3(x)
        branch4 = self.branch4(x)
        print('size of branch1', torch.tensor(branch1).shape)
        print('size of branch2', torch.tensor(branch2).shape)
        print('size of branch3', torch.tensor(branch3).shape)
        print('size of branch4', torch.tensor(branch4).shape)
        outputs = [branch1, branch2, branch3, branch4]
        return outputs
    
    def forward(self, x):
        outputs = self._forward(x)
        return torch.cat(outputs, 1)
```

---

#### Auxiliary Classifier(보조 분류기)


```python
class InceptionAux(nn.Module):
    def __init__(self, in_channels: int, num_classes: int, 
                 conv_block: Optional[Callable[..., nn.Module]] = None) -> None:
        super(InceptionAux, self).__init__()
        if conv_block is None:
            conv_block = BasicConv2d
        self.conv = conv_block(in_channels, 128, kernel_size = 1)
        
        self.fc1 = nn.Linear(2048, 1024)
        self.fc2 = nn.Linear(1024, num_classes)
        
    def forward(self, x):
        # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14
        x = F.adaptive_avg_pool2d(x, (4, 4))
        # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4
        x = self.conv(x)
        # N x 128 x 4 x 4
        x = torch.flatten(x, 1)
        # N x 2048
        x = F.relu(self.fc1(x), inplace = True)
        # N x 1024
        x = F.dropout(x, 0.7, training = self.training)
        # N x 1024
        x = self.fc2(x)
        # N x 1000 (num_classes)
        
        return x
```

---

#### GoogleNet
1. _forward 함수에서 인셉션 함수를 지나기 전 컨볼루션 연산을 진행하는 부분
2. 인셉션을 진행하는 부분
3. 보조 분류기를 통하여 학습을하는 과정
위 세가지 과정을 주목하여 볼 것


```python
class GoogLeNet(nn.Module):
    __constants__ = ['aux_logits', 'transform_input']

    def __init__(
        self,
        num_classes: int = 1000,
        aux_logits: bool = True,
        transform_input: bool = False,
        init_weights: Optional[bool] = None,
        blocks: Optional[List[Callable[..., nn.Module]]] = None
    ) -> None:
        super(GoogLeNet, self).__init__()
        if blocks is None:
            blocks = [BasicConv2d, Inception, InceptionAux]
        if init_weights is None:
            # warnings.warn('The default weight initialization of GoogleNet will be changed in future releases of '
            #              'torchvision. If you wish to keep the old behavior (which leads to long initialization times'
            #              ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)
            init_weights = True
        assert len(blocks) == 3
        conv_block = blocks[0]
        inception_block = blocks[1]
        inception_aux_block = blocks[2]

        self.aux_logits = aux_logits
        self.transform_input = transform_input

        self.conv1 = conv_block(3, 64, kernel_size=7, stride=2, padding=3)
        self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True)
        self.conv2 = conv_block(64, 64, kernel_size=1)
        self.conv3 = conv_block(64, 192, kernel_size=3, padding=1)
        self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True)

        self.inception3a = inception_block(192, 64, 96, 128, 16, 32, 32)
        self.inception3b = inception_block(256, 128, 128, 192, 32, 96, 64)
        self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True)

        self.inception4a = inception_block(480, 192, 96, 208, 16, 48, 64)
        self.inception4b = inception_block(512, 160, 112, 224, 24, 64, 64)
        self.inception4c = inception_block(512, 128, 128, 256, 24, 64, 64)
        self.inception4d = inception_block(512, 112, 144, 288, 32, 64, 64)
        self.inception4e = inception_block(528, 256, 160, 320, 32, 128, 128)
        self.maxpool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.inception5a = inception_block(832, 256, 160, 320, 32, 128, 128)
        self.inception5b = inception_block(832, 384, 192, 384, 48, 128, 128)

        if aux_logits:
            self.aux1 = inception_aux_block(512, num_classes)
            self.aux2 = inception_aux_block(528, num_classes)
        else:
            self.aux1 = None  # type: ignore[assignment]
            self.aux2 = None  # type: ignore[assignment]

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.dropout = nn.Dropout(0.2)
        self.fc = nn.Linear(1024, num_classes)

        if init_weights:
            self._initialize_weights()

    def _initialize_weights(self) -> None:
        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                torch.nn.init.trunc_normal_(m.weight, mean=0.0, std=0.01, a=-2, b=2)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def _transform_input(self, x: Tensor) -> Tensor:
        if self.transform_input:
            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5
            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5
            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5
            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)
        return x

    def _forward(self, x: Tensor) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]:
        # 인셉션 모듈을 통과하기 전 컨볼루션을 진행하는 부분
        # N x 3 x 224 x 224
        x = self.conv1(x)
        # N x 64 x 112 x 112
        x = self.maxpool1(x)
        # N x 64 x 56 x 56
        x = self.conv2(x)
        # N x 64 x 56 x 56
        x = self.conv3(x)
        # N x 192 x 56 x 56
        x = self.maxpool2(x)
        
        
        # 인셉션 통과
        # N x 192 x 28 x 28
        x = self.inception3a(x)
        # N x 256 x 28 x 28
        x = self.inception3b(x)
        # N x 480 x 28 x 28
        x = self.maxpool3(x)
        # N x 480 x 14 x 14
        x = self.inception4a(x)
        # N x 512 x 14 x 14
        aux1: Optional[Tensor] = None
        # 보조 분류기 부분
        # 학습을 진행할 때 수행
        if self.aux1 is not None:
            if self.training:
                aux1 = self.aux1(x)
        
        # 다시 인셉션을 통과
        x = self.inception4b(x)
        # N x 512 x 14 x 14
        x = self.inception4c(x)
        # N x 512 x 14 x 14
        x = self.inception4d(x)
        # N x 528 x 14 x 14
        aux2: Optional[Tensor] = None
        if self.aux2 is not None:
            if self.training:
                aux2 = self.aux2(x)

        x = self.inception4e(x)
        # N x 832 x 14 x 14
        x = self.maxpool4(x)
        # N x 832 x 7 x 7
        x = self.inception5a(x)
        # N x 832 x 7 x 7
        x = self.inception5b(x)
        # N x 1024 x 7 x 7

        # Global average Pooling 진행
        x = self.avgpool(x)
        # N x 1024 x 1 x 1
        x = torch.flatten(x, 1)
        # N x 1024
        x = self.dropout(x)
        x = self.fc(x)
        # N x 1000 (num_classes)
        
        # 보조 분류기에서 구한 값도 함께 반환
        return x, aux2, aux1

    @torch.jit.unused
    def eager_outputs(self, x: Tensor, aux2: Tensor, aux1: Optional[Tensor]) -> GoogLeNetOutputs:
        if self.training and self.aux_logits:
            return _GoogLeNetOutputs(x, aux2, aux1)
        else:
            return x   # type: ignore[return-value]

    def forward(self, x: Tensor) -> GoogLeNetOutputs:
        x = self._transform_input(x)
        x, aux1, aux2 = self._forward(x)
        aux_defined = self.training and self.aux_logits
        if torch.jit.is_scripting():
            if not aux_defined:
                warnings.warn("Scripted GoogleNet always returns GoogleNetOutputs Tuple")
            return GoogLeNetOutputs(x, aux2, aux1)
        else:
            return self.eager_outputs(x, aux2, aux1)


class Inception(nn.Module):

    def __init__(
        self,
        in_channels: int,
        ch1x1: int,
        ch3x3red: int,
        ch3x3: int,
        ch5x5red: int,
        ch5x5: int,
        pool_proj: int,
        conv_block: Optional[Callable[..., nn.Module]] = None
    ) -> None:
        super(Inception, self).__init__()
        if conv_block is None:
            conv_block = BasicConv2d
        self.branch1 = conv_block(in_channels, ch1x1, kernel_size=1)

        self.branch2 = nn.Sequential(
            conv_block(in_channels, ch3x3red, kernel_size=1),
            conv_block(ch3x3red, ch3x3, kernel_size=3, padding=1)
        )

        self.branch3 = nn.Sequential(
            conv_block(in_channels, ch5x5red, kernel_size=1),
            # Here, kernel_size=3 instead of kernel_size=5 is a known bug.
            # Please see https://github.com/pytorch/vision/issues/906 for details.
            conv_block(ch5x5red, ch5x5, kernel_size=3, padding=1)
        )

        self.branch4 = nn.Sequential(
            nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True),
            conv_block(in_channels, pool_proj, kernel_size=1)
        )

    def _forward(self, x: Tensor) -> List[Tensor]:
        branch1 = self.branch1(x)
        branch2 = self.branch2(x)
        branch3 = self.branch3(x)
        branch4 = self.branch4(x)

        outputs = [branch1, branch2, branch3, branch4]
        return outputs

    def forward(self, x: Tensor) -> Tensor:
        outputs = self._forward(x)
        return torch.cat(outputs, 1)


class InceptionAux(nn.Module):

    def __init__(
        self,
        in_channels: int,
        num_classes: int,
        conv_block: Optional[Callable[..., nn.Module]] = None
    ) -> None:
        super(InceptionAux, self).__init__()
        if conv_block is None:
            conv_block = BasicConv2d
        self.conv = conv_block(in_channels, 128, kernel_size=1)

        self.fc1 = nn.Linear(2048, 1024)
        self.fc2 = nn.Linear(1024, num_classes)

    def forward(self, x: Tensor) -> Tensor:
        # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14
        x = F.adaptive_avg_pool2d(x, (4, 4))
        # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4
        x = self.conv(x)
        # N x 128 x 4 x 4
        x = torch.flatten(x, 1)
        # N x 2048
        x = F.relu(self.fc1(x), inplace=True)
        # N x 1024
        x = F.dropout(x, 0.7, training=self.training)
        # N x 1024
        x = self.fc2(x)
        # N x 1000 (num_classes)

        return x


class BasicConv2d(nn.Module):

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        **kwargs: Any
    ) -> None:
        super(BasicConv2d, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)
        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)

    def forward(self, x: Tensor) -> Tensor:
        x = self.conv(x)
        x = self.bn(x)
        return F.relu(x, inplace=True)
```

---

#### 모델 형태


```python
ex_net = GoogLeNet()
# ex_net
```

---

#### 데이터 불러오기


```python
from torchvision import datasets, models, transforms
# CIFAR-10 dataset
transform = transforms. Compose([
    transforms.Pad(4),
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32),
    transforms.ToTensor()
])

train_dataset = torchvision.datasets.CIFAR10(root='./dataset/',
                                             train=True,
                                             transform=transform,
                                             download=True)

test_dataset = torchvision.datasets.CIFAR10(root='./dataset/',
                                            train=False,
                                           transform=transforms.ToTensor())

# Data loader
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                           batch_size=100,
                                           shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                          batch_size=100,
                                          shuffle=False)
```

    Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./dataset/cifar-10-python.tar.gz



      0%|          | 0/170498071 [00:00<?, ?it/s]


    Extracting ./dataset/cifar-10-python.tar.gz to ./dataset/



```python
dataiter = iter(train_loader)
images, labels = dataiter.next()

# output은 결과 값
output = ex_net(images)
# output
```

---

#### Pretrained 모델을 사용한 학습


```python
# 위에서 작성한 모델과 같은 모델을 불러올 수 있음
model = torch.hub.load('pytorch/vision:v0.6.0', 'googlenet', pretrained = True)
# print(model)
```

    Downloading: "https://github.com/pytorch/vision/archive/v0.6.0.zip" to /Users/llong/.cache/torch/hub/v0.6.0.zip
    Downloading: "https://download.pytorch.org/models/googlenet-1378be20.pth" to /Users/llong/.cache/torch/hub/checkpoints/googlenet-1378be20.pth



      0%|          | 0.00/49.7M [00:00<?, ?B/s]



```python
# 가속기 설정
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# print(device)

num_ftrs = model.fc.in_features

# CIFAR-10의 부류 개수는 10개 이기 때문에
# 모델의 마지막 FC층의 출력은 10이되어야 함
model.fc = nn.Linear(num_ftrs, 10)

model = model.to(device)
# print(model)
```

---

#### 학습 진행


```python
num_epochs = 80
learning_rate = 0.001
criterion =nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9)
exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)


# 데이터 확장
transform = transforms.Compose([
    transforms.Pad(4),
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32),
    transforms.ToTensor()
])
```


```python
def train_model(model, criterion, dataloaders, optimizer, scheduler, num_epochs=25):
    since = time.time()
    
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0
    
    for epoch in range(num_epochs):
        print(f'Epoch {epoch}/{num_epochs - 1}')
        print('-' * 10)
        
        # 각 에포크마다 학습 단계와 검증 단계를 갖음
        for phase in ['train', 'val']:
            # 모델을 각각 학습 모드와 평가 모드로 설정
            if phase == 'train':
                model.train()
            else:
                model.eval()
            
            # 최초 매개변수 초기화
            running_loss = 0.0
            running_corrects = 0
            
            for inputs, labels in dataloaders:
                inputs = inputs.to(device)
                labels = labels.to(device)
                
                optimizer.zero_grad()
                
                # 순전파
                # 학습 시 연산 기록 추적
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)
                    
                    # 학습 단계인 경우 역전파와 최적화
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()
                
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)
                
            if phase == 'train':
                scheduler.step()
            
            epoch_loss = running_loss / len(dataloaders)
            epoch_acc = running_corrects.double() / len(dataloaders)
            
            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')
            
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
                
        print()
    
    time_elapsed = time.time() - since
    print(f'Training complete in {time_elapsed // 60}m {time_elapsed % 60}s')
    print(f'Best val Acc: {best_acc:4f}')
    
    model.load_state_dict(best_model_wts)
    return model  
```


```python
# 아래 코드로 학습이 가능
# model_ft = train_model(model, criterion, train_loader, optimizer, exp_lr_scheduler, num_epochs=25)
```

---

#### 모델 검증


```python
with torch.no_grad():
    correct, total = 0, 0
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.to(device)
        outputs = model_ft(images)
        _, predicted =. orch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    
    print(f'Accuracy of. he model on. he test images: {100 * correct / total} %')
```

### ResNet
- 잔류(잔차) 학습이라는 개념을 적용: 모듈 단위로, 부분 학습
- 성능 저하를 피하면서 층수를 대폭 늘림
- 잔류 학습은 지름길이 연결된 $\mathbf{x}$를 더한 $\mathbf{F(x) + x}$에 $\boldsymbol{\tau}$를 적용, $\mathbf{F(x)}$는 잔류(잔차)<br>
  $\mathbf{y} = \boldsymbol{\tau}(\mathbf{F}(\mathbf{x}) + \mathbf{x})$<br>
![6.png](attachment:6.png)<br>
<br>

#### 지름길 연결을 두는 이유
- 깊은 신경망 최적화<br>
  - 단순 학습의 관점의 변화를 통한 신경망 구조 변화
  - 단순 구조의 변경으로 매개변수 수에 영향 없음
  - 덧셈 연산만 증가하므로 전체 연산량 증가도 미비
- 깊어진 신경망으로 인해 정확도 개선 가능
- 경사 소멸 문제 해결<br>
$\displaystyle \frac{\partial \mathcal{E}}{\partial \mathbf{x}_{l}}=\frac{\partial \mathcal{E}}{\partial \mathbf{x}_{L}} \frac{\partial \mathbf{x}_{L}}{\partial \mathbf{x}_{l}}=\frac{\partial \mathcal{E}}{\partial \mathbf{x}_{L}}\left(1+\frac{\partial}{\partial \mathbf{x}_{l}} \sum_{i=l}^{L-1} \mathbf{F}\left(\mathbf{x}_{i}\right)\right)$에서 $\displaystyle\frac{\partial}{\partial \mathbf{x}_{l}} \sum_{i=l}^{L-1} \mathbf{F}\left(\mathbf{x}_{i}\right)$ 이 -1이 될 가능성이 거의 없기 때문<br>

---

### 생성모델
- 주어진 데이터에 대해 값을 예측하는 것: 분별 모델 $\rightarrow P(y \mid \mathbf{x})$
- 생성모델은 주어진 레이블 값이 주어질 때 데이터 분포가 어떻게 되는지<br>
  혹은, 데이터 자체의 분포는 어떻게 되는지, 혹은 결합 확률분포가 어떻게 되는지를 추정<br>
  $P(\mathbf{x}), P(\mathbf{x} \mid y), P(\mathbf{x}, y)$를 추정<br>
<br>
- 현실에 내재한 데이터 발생 분포$P_{data}(\mathbf{x})$는 알아낼 수 없음
- 따라서 데이터 분포를 모방하는 모델의 확률 분포$P_{model}(\mathbf{x};\Theta)$를 암시적으로 표현

### GAN의 원리
- 생성기 G와 분별기 D의 대립구도<br>
  - G는 가짜 샘플 생성(위조지폐범)
  - D는 가짜와 진짜를 구별(경찰)
- GAN의 목표는 G가 만들어내는 샘플을 D가 구별하지 못하는 수준까지 학습<br>
  즉, 위조지폐범이 완벽한 위조지폐를 만들어 경찰이 구분못하는 수준까지 학습

### 느낀점
이전 실습에서 구현했던 Alexnet, VGGNet뿐만 아니라 GoogLeNet, ResNet에 대해 알아보았다.<br>
추가적으로 생성모델과 GAN에 대해 대략적으로 알게되었는데 흥미로운 부분인 것 같다.<br>
실습간에는 GoogLeNet을 구현했는데 확실히 많이 복잡했다.<br>
2014년 ILSVRC에서 우승한 모델이지만 VGGNet이 더 각광받은 이유를 알 수 있을 것 같다..(많이 복잡하긴 하다)<br>
코드를 전부 명세하긴 했지만 실제로 이해하는 부분은 일부분이었다.<br>
그럼에도 불구하고 전부를 명세한 이유는 나중에 내가 트러블 슈팅을 할 때 좋은 참고자료가 될 수 있을 것 같다.<br>
전반적으로 인셉션 과정 보조분류기 과정을 비롯하여 모델이 학습하는 과정을 다시 톺아보며 개념과 코드를 일치해가는 과정을 반복해야할 것 같다
