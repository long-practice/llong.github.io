---
title: 프로그래머스 인공지능 데브코스 32일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 7주차(22.1.17.(월) ~ 22.1.21.(금)) 
  확률이론
  결정이론과 선형회귀
  확률분포
  캐글 경진대회
  weekly mission을 통한 실습
tags:
- 확률분포
- 밀도추정
- 이항분포
- 베르누이 분포
- 빈도주의
- 베이지안
- 베타 분포
- 켤레 사전분포
- 예측 분포
- 다항변수
- 디리클레 분포

use_math: True
---
# 32일차(ML_basics - 확률분포)

7주차(22.1.17.(월) ~ 22.1.21.(토)): ML_basic1
* 확률이론
* 결정이론
* 선형회귀
* 확률분포
* kaggle 경진대회
* Weekly Mission

### 밀도추정(Density Estimation)
- N개의 관찰 데이터 $x_1 ~ x_N$이 주어졌을 때 분포함수 $P(x)$를 찾는 것<br>
    1. $P(x)$를 파라미터화된 분포로 가정. 회귀문제에서는 $P(t \mid x)$, 분류문제에서는 $P(t \mid x)$을 추정
    2. 분포의 파라미터를 구하기<br>
      - 빈도주의 방법: 어떤 기준(주로 우도)를 최적화시키는 과정을 통해 파라미터 값을 구함
      - 베이지안 방법: 파라미터의 사전확률을 가정하고 베이즈 정리을 이용하여 파라미터의 사후확률을 구함
    3. 파라미터를 예측하면 예측이 가능<br>
- 켤레 사전분포(conjugate prior): 사후확률이 사전확률과 동일한 함수 형태

### 이항분포
- 이항 확률 변수 $x \in \{0, 1\}$(ex. 동전 던지기 등)
- $P(x = 1 \mid \mu) = \mu, P(x = 0 \mid \mu) = 1 - \mu$
- 따라서 확률분포는 $P(x) = Bern(x \mid \mu) = \mu^x(1-\mu)^{1-x}$가 되고 베르누이 분포(Bernoulli Distribution)이라고 함<br>
<br>
- 기댓값: $\mathbb{E}(x) = \{0 \times (1 - \mu)\} + \{1 \times \mu\} =\mu$
- 분산: $\mbox{var}(x) = \{\mathbb{E}(x)\}^2 - \mathbb{E}(x^2) = \mu^2 - \mu = \mu(1 - \mu)$<br>
<br>
- 우도함수: $x$값을 $N$번 관찰한 결과 $\mathcal{D} = \{x_1, \cdots, x_n\}$, 이 때, $x$가 독립적으로 $P(x \mid \mu)$에서 뽑혀질 때
- $P(\mathcal{D} \mid \mu) = \displaystyle\prod_{n = 1}^{N}P(x_n \mid \mu) = \displaystyle\prod_{n = 1}^{N}\mu^{x_n}(1- \mu)^{1-x_n}$<br>
<br>

#### 빈도주의 방법
- $\mu$값을 $P(\mathcal{D} \mid \mu)$를 최대화 시키는 값으로 구할 수 있음
- $\ln P(\mathcal{D} \mid \mu) = \displaystyle\sum_{n = 1}^N\{x_n\ln\mu + (1 - x_n)\ln(1 - \mu)\}$에서 <br>
  양 변을 $\mu$에 대해 편미분<br>
  $\displaystyle\frac{\partial}{\partial \mu}\ln P(\mathcal{D} \mid \mu) = \sum_{n = 1}^N \\{\frac{x_n}{\mu} - \frac{1 - x_n}{1 - \mu} \\} = 0$을 만족하는 $\mu$값은 $\displaystyle \mu^{ML} = \frac{m}{N}$ 이 때 $m$은 $x = 1$인 경우의 수<br>
<br>
- $N$이 너무 작을 경우 과대적합 발생, 예를 들어보면 동전을 3번 던졌는데 3번 모두 앞면이 나왔다면 **다음 번 동전을 던질 때의 기댓값은 항상 앞면**<br>
<br>

#### 베이지안 방법
- $\mathcal{D} = \{x_1, \cdots, x_n\}$일 때 이항변수 $x$가 1인 경우를 $m$번 관찰했을 때<br>
  확률분포는 다음과 같음<br>
  $\mbox{Bin}(x|N, \mu) = \displaystyle{N \choose m}\mu^m(1 - \mu)^{N - m}$<br>
<br>
- 기댓값: $\mathbb{E}(x) = N\mu$
- 분산: $\mbox{var}(x) = N\mu(1 - \mu)$<br>
<br>
- 데이터를 보는 관점을 살펴보면 베르누이 시험의 반복($x_1, \cdots, x_n$) 각각이 확률변수
- $x = 1$ 등장횟수: 하나의 확률변수 $m$
- 베이지안 방법의 경우 데이터의 우도를 계산해야 하나 이항분포의 경우 우도함수가 $m$으로 표현이 가능하므로 계산이 간편

### 베타 분포
- 베이지안 방법으로 문제 해결하기 위해 켤레 사전분포로 베타분포를 사용<br>
  $\mbox{Beta}(\mu |a, b) = \displaystyle\frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}\mu^{a - 1}(1 - \mu)^{b - 1}$<br>
<br>
- 이 때 감마함수 $\Gamma(x) = \displaystyle\int_0^{\infty}\mu^{x - 1}e^{-\mu} \ d\mu$로 계승(factorial)을 실수로 확장<br>
  다음과 같은 성질이 있음$\Gamma(n) = (n - 1)!$(위의 식을 부분적분을 해보면 쉽게 증명 가능)<br>
<br>
<br>
- 베타분포와 정규화 됨을 증명<br>
  $\displaystyle \int_0^1Beta(\mu|a, b) d\mu = 1 \leftrightarrow \int_0^1\mu^{a - 1}(1 - \mu)^{b - 1} d\mu = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a + b)}$임을 증명<br><br>
  $\Gamma(a)\Gamma(b) =\displaystyle\int_0^{\infty}x^{a - 1}e^{-x}dx \displaystyle\int_0^{\infty}y^{b - 1}e^{-y}dy = \displaystyle\int_0^{\infty} \displaystyle\int_0^{\infty}x^{a-1}y^{b-1}e^{-(x+y)}dydx$에서<br>
  $t = x + y$로 치환하여 정리하고 $y$ 혹은 $x$를 $t\mu$로 치환하여 식을 정리하면 증명 가능<br>
<br><br>
- 기댓값: $\mathbb{E}(\mu) = \displaystyle\int_0^1 \mu\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1 - \mu)^{b - 1}d\mu = \displaystyle\frac{a}{a + b}$<br>
<br>
- 분산: $\mbox{var}(\mu) = \mathbb{E}(\mu^2) - \{\mathbb{E}(\mu)\}^2 = \displaystyle\frac{ab}{(a + b)^2(a + b + 1)}$<br>
<br>

#### $\mu$의 사후확률
- 베이즈 정리 이용(사후확률 $\propto$ 우도 $\times$ 사전확률)
- $m$: $x = 1$인 경우 관찰, $l$: $x = 0$인 경우 관찰, $a$: $x = 1$이었던 경우, $b$: $x = 1$이었던 경우
- $P(m, l \mid \mu)$: 우도, $P(\mu \mid a, b)$: 사전확률<br>
- 따라서 사후확률은 $P(m, l\mid \mu)P(\mu \mid a, b)$과 같음<br>
<br>

- 사후확률 $P(\mu \mid m, l, a, b) = \displaystyle\frac{\mbox{Bin}(m \mid N,\mu)\mbox{Beta}(\mu \mid a, b)}{\displaystyle\int_0^1\mbox{Bin}(m \mid N,\mu)\mbox{Beta}(\mu \mid a, b) d\mu} = \displaystyle\frac{\Gamma(m + a + l + b)}{\Gamma(m + a)\Gamma(l + b)}\mu^{m + a - 1}(1 - \mu)^{l + b - 1}$
- $a$가 $m$만큼, $b$가 $l$만큼 늘어난 효과 발생, 사후확률이 다음 번 데이터에 대해서는 사전확률로 간주가 가능
- 즉, 연속적인 업데이트가 가능

#### 예측분포
- $\displaystyle P(x = 1 \mid \mathcal{D}) = \int_0^1P(x = 1 \mid \mu)P(\mu \mid \mathcal{D})d\mu = \int_0^1\mu P(\mu \mid \mathcal{D})d\mu = \mathbb{E}(\mu \mid \mathcal{D}) = \frac{\Gamma(m + a)}{\Gamma(m + a + l + b)}$
- 사전 지식($a, b$)이 반영되기 때문에 빈보주의의 극단적인 예를 피할 수 있음<br>
<br>

- 가령 동전을 3번 던졌는데 3번 다 앞면이 나왔을 경우 앞으로 빈도주의 측면에서 예측은 무조건 앞면만 나온다고 예측하겠지만<br>
  베이지안 방식으로의 예측은 사전에 앞면이 2번 뒷면이 1번 나온다는 사실을 반영을 하게되면 $\frac{5}{6}$의 확률로 앞면이 나온다고 예측
  
---

### 다항변수(Multinomial Variables)

#### 빈도주의 방법
- $K$개의 상태를 가질 수 있는 확률변수를 $K$차원 벡터 $x$(하나의 원소만 1이고 나머지는 0)로 나타낼 수 있음
- $x$를 위해 베르누이 분포를 다음과 같이 일반화 가능<br>
  $p(x \mid \mu) = \displaystyle\prod_{k = 1}^{K}\mu_k^{x_k}$ (단, $\sum_k \mu_k = 1$)<br>
<br>
- 기댓값: $\mathbb{E}(x \mid \mu) = \sum_xP(x \mid \mu) = (\mu_1, \cdots, \mu_M)^T = \mu$
- 우도함수: $P(\mathcal{D} \mid \mu) = \displaystyle\prod_{n =1}^{N} \prod_{k = 1}^{K}\mu_k^{x_{nk}} =  \prod_{k = 1}^K\mu_k^{(\sum_n x_{nk})} =  \prod_{k = 1}^{K}\mu_k^{m_k}$ (단, $m_k =\sum_nx_{nk}$)
- $\mu$의 최대우도 추정치를 구하기 위해서는 로그우도함수를 최대화시키는 $\mu_k$를 구해야 함
- 라그랑주 승수 $\lambda$를 이용하여 최대화<br>
  $\ln P(\mathcal{D} \mid \mu) = \displaystyle\sum_{k = 1}^{K}m_k\ln\mu_k + \lambda\(\displaystyle\sum_{k = 1}^{K}\mu_k - 1 \)$에서 $\mu_k$로 미분하고 0으로 만드는 $\mu_k$값이 로그우도함수를 최대화시키는 $\mu_k^{ML}$<br>
  따라서 $\mu_k^{ML} = \displaystyle\frac{m_k}{N}$

#### 베이지안 방법
- 파라미터 $\mu$와 전체 관찰개수 $N$이 주어졌을 때 $m_1, \cdots, m_K$의 분포를 다항분포(multinomial distribution)이라고 함
- $\mbox{Mult}(m_1, \cdots, m_K|\mu, N) = \displaystyle{N \choose m_1m_2\cdots m_K}\displaystyle\prod_{k=1}^{K}\mu_k^{m_k}$<br>
  $\displaystyle{N \choose m_1m_2\cdots m_K} = \displaystyle\frac{N!}{m_1!m_2!\cdots m_K!}$<br>
  $\displaystyle\sum_{k=1}^{K}m_k = N$<br>
<br>
- 디리클레 분포(Dirichlet distribution): 다항분포를 위한 켤레사전분포<br>
  $\mbox{Dir}(\mu|\alpha) = \displaystyle\frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)\cdots \Gamma(\alpha_K)}\displaystyle\prod_{k=1}^{K}\mu_k^{\alpha_k -1}$ 이 때 $\alpha_0 = \displaystyle\sum_{k=1}^{K}\alpha_k$<br>
<br>
- 디리클레 분포 정규화는 귀납법으로 증명<br>
  증명 간 다음 결과를 사용<br>
  $ \int_L^U(x - L)^{a - 1}(U - x)^{b - 1}dx = (U - L)^{a + b - 1}\displaystyle\frac{\Gamma(a)\Gamma(b)}{\Gamma(a + b}$<br>
  위의 적분식은 $\displaystyle t = \frac{x - L}{U - L}$로 치환하여 도출 가능<br>
  <br>
  $L = 0, U = 1- \mu_1$를 적용하여<br>
  $ \displaystyle\int_0^{1 - \mu_1}\mu_1^{\alpha_1 - 1}\mu_2^{\alpha_2 - 1}(1 -\mu_1 - \mu_2) ^{\alpha_3 - 1}d\mu_2 = \mu_1^{\alpha_1 - 1}(1 - \mu_1)^{\alpha_2 + \alpha_3 - 1}\displaystyle\frac{\Gamma(\alpha_2)\Gamma(\alpha_3)}{\Gamma(\alpha_2 + \alpha_3)}$<br>
  <br>
  $ \displaystyle\int_0^{1-\mu_1}\displaystyle\int_0^{1 - \mu_1}\mu_1^{\alpha_1 - 1}\mu_2^{alpha_2 - 1}(1 -\mu_1 - \mu_2) ^{\alpha_3 - 1}d\mu_2d \mu_1 = \displaystyle\frac{\Gamma(\alpha_1)\Gamma(\alpha_2)\Gamma(\alpha_3)}{\Gamma(\alpha_1 + \alpha_2 + \alpha_3)}$<br>
<br>

#### $\mu$의 사후확률(posterior)<br>
  $ P(\mu|\mathcal{D}, \alpha) = \mbox{Dir}(\mu|\alpha + m) = \displaystyle\frac{\Gamma(\alpha_0 + N)}{\Gamma(\alpha_1 + m_1)\cdots \Gamma(\alpha_K + m_K)}\displaystyle\sum_{k = 1}^{K}\mu_k^{\alpha_k + m_k - 1}$<br>
- 이 때 $\alpha_k$를 $x_k = 1$에 대한 사전관찰 개수라고 생각할 수 있으며, 베타분포와 마찬가지로 연속적인 업데이트가 가능

---


```python
%matplotlib inline
import matplotlib.pyplot as plt
from IPython.display import Math, Latex
from IPython.core.display import Image
```


```python
import seaborn as sns
sns.set(color_codes = True)
sns.set(rc = {'figure.figsize':(5, 5)})
import numpy as np
```


```python
import warnings
warnings.filterwarnings(action = 'ignore')
```

---


```python
# Uniform Distribution 이용
from scipy.stats import uniform
```


```python
n = 10000
start = 10
width = 20
data_uniform = uniform.rvs(size = n, loc = start, scale = width)

# 10부터 10 + 20까지 랜덤한 수 10000개 추출
data_uniform
```




    array([21.8597643 , 21.59134358, 20.32610463, ..., 24.47749441,
           10.27477632, 15.31554628])




```python
# 10부터 30까지 수의 분포를 확인할 수 있음
ax = sns.distplot(data_uniform,
                  bins = 100,
                  kde = True,
                  color = 'skyblue',
                  hist_kws = {"linewidth": 15, 'alpha': 1})
ax.set(xlabel='Uniform Distribution ', ylabel='Frequency')
plt.show()
```


    
<img width = "600" src = "https://user-images.githubusercontent.com/83870423/151226693-91e05af5-2e66-479b-99ba-de586a81414a.png">


---


```python
# 베르누이 분포
from scipy.stats import bernoulli
data_bern = bernoulli.rvs(size = 10000, p = 0.8)

# 데이터가 0과 1로 이루어짐
data_bern[:100]
```




    array([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
           1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
           0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
           1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
           1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1])




```python
# 0의 개수는 2002개, 1의 개수는 7998개
# p = 0.8이 거의 성립
np.unique(data_bern, return_counts = True)
```




    (array([0, 1]), array([2058, 7942]))




```python
ax = sns.distplot(data_bern,
                  kde = False,
                  color= 'skyblue',
                  hist_kws = {"linewidth": 15, 'alpha': 1})
ax.set(xlabel = "Bernoulli Distribution", ylabel = "Frequency")
```




    [Text(0.5, 0, 'Bernoulli Distribution'), Text(0, 0.5, 'Frequency')]




    
<img width = "600" src = "https://user-images.githubusercontent.com/83870423/151226767-30e32b33-3878-4900-b414-c94765128ff0.png">
    


---


```python
# 베타 분포
from scipy.stats import beta
a, b = 0.1, 0.1
data_beta = beta.rvs(a, b, size = 10000)
data_beta
```




    array([5.95584057e-04, 1.58238168e-04, 9.99978095e-01, ...,
           9.98418491e-01, 9.99999933e-01, 9.99993579e-01])




```python
# 시각화
# a, b의 값을 조정하면서 그래프 개형을 확인
# b의 값이 올라갈수록 그래프는 0에 가까운 곳에 가파르게 형성
ax = sns.distplot(data_beta,
                  kde = False,
                  color= 'skyblue',
                  hist_kws = {"linewidth": 15, 'alpha': 1})
ax.set(xlabel = "Beta Distribution", ylabel = "Frequency")
```




    [Text(0.5, 0, 'Beta Distribution'), Text(0, 0.5, 'Frequency')]




    
<img width = "600" src = "https://user-images.githubusercontent.com/83870423/151226816-2ec8a988-584f-45ff-a672-f3040ab29df1.png">
    


---


```python
# 다항 분포
from scipy.stats import multinomial
data_multinomial = multinomial.rvs(n =1, p =[0.2, 0.1, 0.3, 0.4], size = 10000)

# 각각의 행에 벡터가 생성
# 각각의 벡터는 0.2, 0.1, 0.3, 0.4 확률로 각각의 위치에 1이 등장
data_multinomial[:10]
```




    array([[0, 0, 1, 0],
           [1, 0, 0, 0],
           [0, 0, 0, 1],
           [1, 0, 0, 0],
           [0, 0, 1, 0],
           [0, 0, 0, 1],
           [0, 0, 1, 0],
           [0, 0, 1, 0],
           [1, 0, 0, 0],
           [0, 0, 1, 0]])




```python
# 각각의 행에서 0.2, 0.1, 0.3, 0.4의 확률로 벡터가 생성된 것을 확인
for i in range(4):
    print(np.unique(data_multinomial[:, i], return_counts=True))
```

    (array([0, 1]), array([8022, 1978]))
    (array([0, 1]), array([8997, 1003]))
    (array([0, 1]), array([7013, 2987]))
    (array([0, 1]), array([5968, 4032]))


---

### 느낀점
오늘도 조금 어려운 부분이 있었다. 특히 베타 분포, 디리클레 분포가 조금 어려웠고 복잡했지만<br>
실제로 정의에 입각하여 문제를 풀어나가면 결코 어려운 부분은 아니라고 생각한다.<br>
무엇보다도 직접 실습을 통해서 눈으로 데이터가 형성되는 것을 확인해보면 어려운 개념은 아니지만<br>
관련된 이론을 직접 증명하는 것은 가까운 주말에 시간을 여유롭게 가지고 처음부터 끝까지 해보는게 좋을 것 같다<br>
