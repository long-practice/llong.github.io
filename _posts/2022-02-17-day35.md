---
title: 프로그래머스 인공지능 데브코스 35일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 7주차(22.1.17.(월) ~ 22.1.21.(금)) 
  선형회귀
  선형분류
  Weekly Mission
tags:
- 선형회귀
- 기저함수
- 최소제곱법
- 최소제곱법의 기하학적 의미
- 온라인 학습
- 규제화
- 편향-분산 분해
- 편향-분산 trade-off
- 베이지안 선형회귀
- 예측 분포

use_math: True
---
# 35일차(ML_basics - 선형회귀)

8주차(22.1.24.(월) ~ 22.1.28.(토)): ML_basic2
* 선형회귀
* 선형분류
* Weekly Mission

출처: 프로그래머스 인공지능 데브코스 3기 강의

### 선형회귀
- 목표: 관찰 값 $x$을 바탕으로 실수 범위의 값 $t$를 예측
- 가장 기본적인 선형 모델식은 다음과 같음<br>
  $y(\mathbf{x}, \mathbf{w})=w_{0}+w_{1} x_{1}+\ldots w_{D} x_{D}$<br>
  $\displaystyle y(\mathbf{x}, \mathbf{w})=w_{0}+\sum_{j=1}^{M-1} w_{j} \phi_{j}(x)$
- 이 모델의 파라미터는 $\mathbf{w}$벡터(가중치) $\mathbf{w}=\left(w_{0}, \ldots, w_{D}\right)^{T}$
- $\phi_{j}(x)$는 기저함수로 $y(\mathbf{x}, \mathbf{w})$가 $x$에 대해 선형 식이었던 것이 $x$에 대해 비선형이될 수 도 있음
- $w_{0}$는 바이어스(절편)으로 $\phi_{0}(x) = 1$로 정의하여 간단하게 표현하기도 함<br>
  $\displaystyle y(\mathbf{x}, \mathbf{w})=\sum_{j=0}^{M-1} w_{j} \phi_{j}(x)=\mathbf{w}^{T} \phi(\mathbf{x})$

### 기저함수
- 기저함수의 형태는 여러가지가 있음<br>
  1. 다항식 기저함수<br>
     $\phi_{j}(x)=x^{j}$<br>
<br>
  2. 가우시안 기저함수<br>
     $\displaystyle \phi_{j}(x)=\exp \\{-\frac{(x-\mu_{j})^{2}}{2 s^{2}}\\}$<br>
<br>
  3. 시그모이드 기저함수<br>
     $\displaystyle \phi_{j}(x)=\sigma\left(\frac{x-\mu_{j}}{s}\right)$<br>
     $\displaystyle \sigma(a)=\frac{1}{1+\exp (-a)}$<br>
<br>
    

### 최대우도와 최소제곱법
- 곡선 근사할 때 최소제곱법을 활용하고, 가우시안 노이즈 모델을 이용하여 최대우도(MLE)를 구했었음
- 가우시안 노이즈가 포함된 타겟 $t$를 표현하면<br>
  $t=y(\mathbf{x}, \mathbf{w})+ \epsilon$<br>
  - $y(\mathbf{x}, \mathbf{w})$는 결정론적 함수
  - $\epsilon$은 가우시안 분포$\mathcal{N}\left(\epsilon \mid 0, \beta^{-1}\right)$를 따르는 노이즈 확률변수
- 따라서 관찰 값($\mathbf{x}$)에 대한 예측 값($t$)의 분포는 평균이 $y(\mathbf{x}, \mathbf{w})$이고, 분산이 $\beta^{-1}$인 가우시안 분포로 다음과 같음<br>
  $p(t \mid \mathbf{x}, \mathbf{w}, \beta)=\mathcal{N}\left(t \mid y(\mathbf{x}, \mathbf{w}), \beta^{-1}\right)$<br>
<br>
- 제곱합이 손실함수로 사용되는 경우 새로운 입력 값에 대한 최적의 예측 값은 $t$의 조건부 기댓값
- $t$가 위의 분포를 따를 때 조건부 기댓값은 다음과 같이 표현이 가능<br>
  $\displaystyle \mathbb{E}[t \mid \mathbf{x}]=\int t p(t \mid \mathbf{x}) d t=y(\mathbf{x}, \mathbf{w})$
- 위의 식을 그림으로 보면 쉽게 이해가 가능<br>

![%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-02-06%20%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB%201.45.11.png](attachment:%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-02-06%20%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB%201.45.11.png)<br>

- 각각의 입력에 대해 출력 값이 모두 독립적으로 발현된다고 가정
- 샘플 데이터를 얻을 수 있을 확률(우도 함수)는 다음과 같이 정의<br>
  $\displaystyle p(\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \beta)=\prod_{n=1}^{N} \mathcal{N}\left(t_{n} \mid \mathbf{w}^{T} \phi\left(\mathbf{x}_{n}\right), \beta^{-1}\right)$<br>
<br>
- 양변에 로그를 취해주면<br>
  $\displaystyle \ln p(\mathbf{t} \mid \mathbf{w}, \beta)=\sum_{n=1}^{N} \ln \mathcal{N}(t_{n} \mid \mathbf{w}^{T} \phi(\mathbf{x}\_{n}), \beta^{-1})=\frac{N}{2} \ln \beta-\frac{N}{2} \ln (2 \pi)-\beta E_{D}(\mathbf{w})$<br>
  $\displaystyle E_{D}(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\\{t_{n}-\mathbf{w}^{T} \phi(\mathbf{x}\_{n})\\}^{2}$
- 따라서 로그우도함수를 최대화 시키는 $\mathbf{w}$는 $E_{D}(\mathbf{w})$을 최소화시키는 값과 동일
- $\mathbf{w}$를 편미분을 하게되면 그레디언트 벡터를 얻을 수 있음<br>
  $\displaystyle \nabla \ln p(\mathbf{t} \mid \mathbf{w}, \beta)=\beta \sum_{n=1}^{N}\\{t_{n}-\mathbf{w}^{T} \phi(x_{n})\\} \phi(x_{n})^{T}$
- 위의 식의 값을 0을 만족시키는 $\mathbf{w}\_{ML}$를 구하면(무어-펜로즈 역행렬로도 표현 가능)<br>
  $\mathbf{w}\_{M L}=\left(\Phi^{T} \Phi\right)^{-1} \Phi^{T} \mathbf{t} = \Phi^{\dagger} \mathbf{t}$<br>
<br>
- 위의 식을 일반 방정식(normal equation)이라고 하며, 파라미터 추정 방식이 업데이트 방식이 아닌 일반 방정식의 풀이로 구해짐<br>
  $\displaystyle \Phi=(\begin{array}{cccc}\phi_{0}(x_{1}) & \phi_{1}(x_{1}) & \cdots & \phi_{M-1}(x_{1}) \\ \phi_{0}(x_{2}) & \phi_{1}(x_{2}) & \cdots & \phi_{M-1}(x_{2}) \\ \vdots & \vdots & \ddots & \vdots \\ \phi_{0}(x_{N}) & \phi_{1}(x_{N}) & \cdots & \phi_{M-1}(x_{N})\end{array})$<br>
- $\Phi$는 $N \times M$크기의 행렬로 $N$개의 데이터가 $M$개의 기저함수를 가지는 것을 의미하며 **디자인 행렬**이라고 부름<br>
<br>
- 마찬가지 방법으로 우도함수를 최대화시키는 로그우도함수를 바이어스($w_0$)와 분산($\beta$)를 구해보면 다음과 같음<br>
  $\displaystyle E_{D}(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\\{t_{n}-w_{0}-\sum_{j=1}^{M-1} w_{j} \phi_{j}(\mathbf{x}\_{n})\\}^{2}$<br>
  $\displaystyle w_{0}=\bar{t}-\sum_{j=1}^{M-1} w_{j} \bar{\phi}\_{j}$<br>
  $\displaystyle \bar{t}=\frac{1}{N} \sum_{n=1}^{N} t_{n}, \quad \bar{\phi}\_{j}=\frac{1}{N} \sum_{n=1}^{N} \phi_{j}(\mathbf{x}\_{n})$<br>
  <br>
  $\displaystyle \frac{1}{\beta_{\mathrm{ML}}}=\frac{1}{N} \sum_{n=1}^{N}\\{t_{n}-\mathbf{w}\_{\mathrm{ML}}^{T} \phi(\mathbf{x}\_{n})\\}^{2}$

### 최소제곱법의 기하학적 의미

- 벡터의 사영은 벡터 $\mathbf{t} \in \mathbb{R}^m$의 $\operatorname{span}\left(\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right\}\right)\left(\mathbf{x}_{i} \in \mathbb{R}^{m}\right)$의 사영은 $\operatorname{span}\left(\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right\}\right)$에 속한 벡터들 중 $\mathbf{t}$에 가장 가까운 벡터로 정의
- 다음과 같이 표현 가능<br>
  $\operatorname{Proj}\left(\mathbf{t} ;\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right\}\right)=\operatorname{argmin}_{\mathbf{v} \in \operatorname{span}\left(\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right\}\right)}\|\mathbf{t}-\mathbf{v}\|_{2}$ <br>
<br>
- $\operatorname{Proj}(\mathbf{t} ; \mathbf{A})$은 행렬 $\mathbf{A}$의 치역으로의 사영을 의미하고, 이 때 $\mathbf{A}$가 선형 독립이라면(full-rank, 역행렬이 존재할 시) 다음이 성립<br>
  $\operatorname{Proj}(\mathbf{t} ; \mathbf{A})=\operatorname{argmin}_{\mathbf{v} \in \mathcal{R}(\mathbf{A})}\|\mathbf{t}-\mathbf{v}\|_{2}=\mathbf{A}\left(\mathbf{A}^{T} \mathbf{A}\right)^{-1} \mathbf{A}^{T} \mathbf{t}$<br>
<br>
- 한편 최대우도를 만족하는 $\mathbf{w}_{M L}=\left(\Phi^{T} \Phi\right)^{-1} \Phi^{T} \mathbf{t} = \Phi^{\dagger} \mathbf{t}$이므로<br>
  벡터의 사영은 디자인 행렬($\Phi$)과 $\mathbf{w}_{M L}$곱으로 표현이 가능<br>
<br>
- 아래 그림을 보면 쉽게 이해가 가능<br>
  $\mathcal{S}$는 $\varphi_1, \varphi_2$벡터에 의해 만들어진 치역이고 $\mathbf{y}$는 $\mathbf{t}$를 치역에 사영시킨 벡터를 표현<br>
  ![2.png](attachment:2.png)<br>

### 온라인 학습(Sequential Learning)

- 위에서 살펴본 MLE방식은 전체 데이터를 한 번에 읽고 한 번에 처리하는 배치(batch) 방식
- 데이터 규모가 크거나 순차적인 데이터 입력이 이루어지는 경우에 위의 방식으로는 적용이 쉽지 않음<br>
<br>
- 이를 해결하기위해 확률적 경사하강법(Stochastic Gradient Descent)를 적용
- 배치 방식과는 다르게 몇 개의 샘플($n$개)를 이용하여 손실함수를 구하고, 가중치의 변화량을 구해냄<br>
  에러함수: $E=\sum_{n} E_{n}$<br>
  가중치: $\mathbf{w}^{(\tau+1)}=\mathbf{w}^{(\tau)}-\eta \nabla E_{n}$<br>
<br>
- 제곱합 에러함수($\displaystyle E_{D}(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\left\{t_{n}-\mathbf{w}^{T} \phi\left(\mathbf{x}_{n}\right)\right\}^{2}$)의 경우는 다음과 같이 가중치 갱신을 표현<br>
<br>
  $\mathbf{w}^{(\tau+1)}=\mathbf{w}^{(\tau)}+\eta\left(t_{n}-\mathbf{w}^{(\tau) T} \phi_{n}\right) \phi_{n}$ 이 때 $\phi_{n}=\phi\left(\mathbf{x}_{n}\right)$


```python
# 실습
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
import numpy as np

from sklearn.linear_model import LinearRegression
model = LinearRegression(fit_intercept=True)
```


```python
# 1000개의 데이터, 3개의 가중치
N, M = 1000, 3
rng = np.random.RandomState(1)
rng.rand(N, M).shape

# 1000 X 3 행렬(1000개의 데이터와 각 데이터마다 3개의 가중치)
X = 10 * rng.rand(N, M)
# 내적연산 후 행렬의 크기는 1000 X 1 행렬
np.dot(X, [1.5, -2., 1.]).shape
```




    (1000,)




```python
# y = -1.5 * x_1 + (-2) * x_2 + x_3 + 0.5
rng = np.random.RandomState(1)
X = 10 * rng.rand(N, M)
y = 0.5 + np.dot(X, [1.5, -2., 1.])

model.fit(X, y)

print('모델이 예측한 절편과 가중치 값')
print(model.intercept_)
print(model.coef_)
```

    모델이 예측한 절편과 가중치 값
    0.4999999999999658
    [ 1.5 -2.   1. ]


---


```python
# Normal Equations
import numpy.linalg as LA
normal_equation_solution = LA.inv(X.T@X)@X.T@y
normal_equation_solution
```




    array([ 1.52825484, -1.96886193,  1.03058857])



위에서 구한 가중치 값과 굉장히 유사한 것을 알 수 있음

---


```python
# Small Memory Normal Equations (Online)
A = np.zeros([M, M])
b = np.zeros([M, 1])

# 데이터를 한꺼번에 저장해서 연산하는 것이 아닌
# 하나씩 하나씩 불러와서 연산을 진행하기 때문에 메모리 소모가 적음
for i in range(N):
    A = A + X[i, np.newaxis].T@X[i, np.newaxis]
    b = b + X[i, np.newaxis].T*y[i]
    
solution = LA.inv(A)@b
solution
```




    array([[ 1.52825484],
           [-1.96886193],
           [ 1.03058857]])



결과는 위와 동일

---


```python
# SGD
w = np.zeros([M, 1])

# 학습률과 반복횟수
eta = 0.001
n_iter = 500
```


```python
# 결과는 동일한 것을 알 수 있음
for i in range(n_iter):
    i = i % N
    neg_gradient = (y[i] - w.T@X[i, np.newaxis].T) * X[i, np.newaxis].T
    w = w + eta * neg_gradient
print(w)
```

    [[ 1.51033015]
     [-1.93792375]
     [ 1.0123695 ]]


위에서 ```neg_gradient```는 $\left(t_{n}-\mathbf{w}^{(\tau) T} \phi_{n}\right) \phi_{n}$을 의미, 마찬가지로 결과는 동일

---

### 규제화
- 규제는 학습 시 과대적합(overfitting)을 해결하기 위해 사용
- 에러함수는 다음과 같이 정의<br>
  $E(\mathbf{w})=E_{D}(\mathbf{w})+\lambda E_{W}(\mathbf{w})$<br>
<br>
- 가장 단순한 형태의 에러함수는<br>
  $\displaystyle E_{W}(\mathbf{w})=\frac{1}{2} \mathbf{w}^{\mathbf{T}} \mathbf{w}$<br>
  $\displaystyle E_{D}(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\left\{t_{n}-\mathbf{w}^{T} \phi\left(\mathbf{x}_{n}\right)\right\}^{2}$<br>
  $\displaystyle E(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\left\{t_{n}-\mathbf{w}^{T} \phi\left(\mathbf{x}_{n}\right)\right\}^{2}+\frac{1}{2} \mathbf{w}^{\mathbf{T}} \mathbf{w}$<br>
<br>
- 이 때 $\mathbf{w}$의 최적값은 위의 에러 함수를 미분하고 그 값을 0으로 만드는 값으로 다음과 같음<br>
  $\mathbf{w}=\left(\lambda I+\Phi^{T} \Phi\right)^{-1} \Phi^{T} \mathbf{t}$<br>
<br>
- 위의 예는 L2 규제의 경우이고 일반화를 시키면 다음과 같이 표현 가능<br>
  $\displaystyle E(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\left\{t_{n}-\mathbf{w}^{T} \phi\left(\mathbf{x}_{n}\right)\right\}^{2}+\frac{1}{2} \sum_{j=1}^{M}\left|w_{j}\right|^{q}$
  
![3.png](attachment:3.png)<br>
- 위의 그림은 왼쪽은 $q = 2$, 오른쪽은 $q = 1$인 경우이고, 파라미터로는 $w_0, w_1$ 두 가지만 사용한 예시
- 파란 원의 중심은 $E_D(\mathbf{w})$를 최소로 만드는 $w_0, w_1$ 
- 거리가 일정한 점들(자취가 원)은 모두 같은 값의 $E_D(\mathbf{w})$를 갖음<br>
<br>
- 규제화를 하게되면 $w_0, w_1$은 노란색 내부의 영역에 해당하는 값만 갖게됨
- 따라서 가장 만족하는 규제화 지점($\mathbf{w}$)은 $w^{*}$이 됨<br>
- 노란색 영역의 범위는 원점을 기준으로 크기가 결정되기 때문에 파라미터 값을 작게 만들어줌
- 노란색 영역의 범위는 상수 값으로 다음과 같이 표현이 가능<br>
  $\displaystyle \sum_{j=1}^{M}\left|w_{j}\right|^{q} \leq \eta$<br>
<br>
- $\eta$와 $\lambda$는 서로 역수의 관계가 있음
- $\eta$가 커지게 되면 $\mathbf{w}$는 $E(\mathbf{w})=E_{D}(\mathbf{w})$에서 얻어진 $\mathbf{w}$에 가깝게 됨
- 반면, $\eta$가 작아지게 되면 파라미터들의 값들은 0에 가까워지게 됨(노란색 영역이 점점 0에 가까운 원으로 줄어들게 되므로)

### 편향-분산 분해
- 작은 크기의 데이터 샘플에 복잡한 모델을 사용하게 되면 과대적합 현상 발생<br>
  - 파라미터 수를 줄이게되면 모델의 유연성이 떨어지게됨
  - 이를 해결하기 위해 규제화를 도입<br>
<br>
- 과대적합이 일어나는 현상에 대해 분석하기 이전에
- 제곱합 손실함수가 주어졌을 때의 최적 예측값<br>
  $h(\mathbf{x})=\mathbb{E}[t \mid \mathbf{x}]=\int t p(t \mid \mathbf{x}) \mathrm{d} t$
- 이에 따라 손실함수의 기댓값은<br>
  $\mathbb{E}[L]=\int\{y(\mathbf{x})-h(\mathbf{x})\}^{2} p(\mathbf{x}) \mathrm{d} \mathbf{x}+\iint\{h(\mathbf{x})-t\}^{2} p(\mathbf{x}, t) \mathrm{d} \mathbf{x} \mathrm{d} t$
- 두번째 항은 $y(\mathbf{x})$와는 관련이 없으므로 노이즈에 관련된 항<br>
<br>
- 첫번째 항을 살펴보면 $\{y(\mathbf{x})-h(\mathbf{x})\}^{2}$에서 $y(\mathbf{x})$는 $h(\mathbf{x})$와 값이 동일해져야 함
- 충분한 데이터를 가지고 있으면 $h(\mathbf{x})$에 근사하는 $y(\mathbf{x})$를 구할 수 있음
- 그러나 제한된 데이터셋 $\mathcal{D}$만 주어질 경우 $h(\mathbf{x})$를 정확히 알 수 없음
- 따라서 파라미터화된 함수 $y(\mathbf{x, w})$를 사용하여 최대한 손실함수의 기댓값을 최소화하고자 함<br>
<br>
- 제한된 데이터만으로 불확실성을 표현해야함<br>
  - 베이지안 방법: 모델 파라미터 $\mathbf{w}$의 사후확률분포를 구함
  - 빈도주의 방법: 모델 파라미터 $\mathbf{w}$의 점추정값을 구하고 여러개의 데이터 셋을 가정했을 때 평균적인 손실을 계산하는 가상의 실험을 통해 점추정값의 불확실성을 계산<br>
- 특정 데이터셋 $\mathcal{D}$의 손실과 기댓값을 다음과 같이 표현<br>
  $L(\mathcal{D})=\{y(\mathbf{x} ; \mathcal{D})-h(\mathbf{x})\}^{2}$<br>
  $\displaystyle \mathbb{E}[L(\mathcal{D})]=\int\{y(\mathbf{x} ; \mathcal{D})-h(\mathbf{x})\}^{2} p(\mathbf{x}) \mathrm{d} \mathbf{x}+$ noise<br>
<br>
- 여러 개의 데이터 셋의 평균적인 손실을 살펴보면<br>
  $\displaystyle \frac{1}{L} \sum_{l=1}^{L} \left[ \int\left\{y\left(\mathbf{x} ; \mathcal{D}^{(l)}\right)-h(\mathbf{x})\right\}^{2} p(\mathbf{x}) \mathrm{d} \mathbf{x}+ \text{noise} \right] =\int \mathbb{E}_{\mathcal{D}}\left[\{y(\mathbf{x} ; \mathcal{D})-h(\mathbf{x})\}^{2}\right] p(\mathbf{x}) \mathrm{d} \mathbf{x}+ \text{noise}$<br>
  주어진 우측 식의 적분 안에 있는 식을 전개<br>
  $\begin{aligned} \{y(\mathbf{x} ; \mathcal{D})-h(\mathbf{x})\}^{2} &= \left\{y(\mathbf{x} ; \mathcal{D})-E_{\mathcal{D}} [ y(\mathbf{x} ; \mathcal{D}) ]+E_{\mathcal{D}}[y(\mathbf{x} ; \mathcal{D})]-h(\mathbf{x})\right\}^{2} \\ &=\left\{y(\mathbf{x} ; \mathcal{D})-E_{\mathcal{D}}[y(\mathbf{x} ; \mathcal{D})]\right\}^{2}+\left\{E_{\mathcal{D}}[y(\mathbf{x} ; \mathcal{D})]-h(\mathbf{x})\right\}^{2} +2\left\{y(\mathbf{x} ; \mathcal{D})-E_{\mathcal{D}}[y(\mathbf{x} ; \mathcal{D})]\right\}\left\{E_{\mathcal{D}}[y(\mathbf{x} ; \mathcal{D})]-h(\mathbf{x})\right\} \end{aligned}$<br>
  <br>
  따라서 다음이 성립되는 것을 알 수 있음<br>
  $\mathbb{E}_{\mathcal{D}}\left[\{y(\mathbf{x} ; \mathcal{D})-h(\mathbf{x})\}^{2}\right] = \left\{\mathbb{E}_{\mathcal{D}}[y(\mathbf{x} ; \mathcal{D})]-h(\mathbf{x})\right\}^{2}+\mathbb{E}_{\mathcal{D}}\left[\left\{y(\mathbf{x} ; \mathcal{D})-\mathbb{E}_{\mathcal{D}}[y(\mathbf{x} ; \mathcal{D})]\right\}^{2}\right]$<br>

<br>

- 정리하면 손실의 기대값을 다음과 같이 정의할 수 있음<br>
  $Expectedloss =(\text { bias })^{2}+ variance + noise $<br>
<br>

- 각각의 값들은 다음과 같음<br>
  $\displaystyle (\text { bias })^{2}=\int\left\{E_{D}[y(x ; D)]-h(x)\right\}^{2} p(x) d x$<br>
  $\displaystyle \text {variance} =\int E_{D}\left[\left\{y(x ; D)-E_{D}[y(x ; D)]\right\}^{2}\right] p(x) d x$<br>
  $\displaystyle \text {noise} =\iint\{h(x)-t\}^{2} p(x, t) d x d t$<br>
<br>

- 우리의 목표는 기대손실을 작은 값을 갖도록 만들어야 함<br>
  noise는 관찰 데이터 자체에 포함된 에러로 우리가 제어하기 힘든 값, 따라서 편향과 분산을 적절히 trade-off해야 함<br>
<br>
- 엄격한 모델은 낮은 편향, 높은 분산
- 유연한 모델은 높은 편향, 낮은 분산

### 편향-분산 trade-off

![4.png](attachment:4.png)

- 위 그림에서 보면 $\lambda$가 큰 경우 규제화가 많이 일어나게 되고 이에따라 자유도가 감소하는 모습을 확인할 수 있음<br>
<br>
- 즉, $\lambda$가 커질수록 분산이 작아지고, 실제 값들과의 차이가 점점 많이 나는 것을 확인할 수 있음(편향이 점점 높아짐)
- 따라서 실제 예측할 수 있는 범위가 제한적이라 최종 결과가 잘못 도출될 가능성이 높음
- 하지만 그래프가 안정된 형태를 보이는데 데이터 샘플이 없는 경우에 이러한 안정된 모델을 선호하기도 함<br>
<br>
- 반면 $\lambda$가 낮아질수록 분산이 커지지만 편향이 점점 낮아짐
- 하지만 실제 예측값과 굉장히 유사한 모습을 확인할 수 있음
- 그러나 개별 샘플별 구한 모델이 서로 차이가 많이 나는 것을 확인할 수 있음
- 샘플 수가 충분하지 못한다면 실제 결과와 큰 편차를 보이는 예측을 할 수 있겠으나 샘플 수가 충분하다면 이 방식을 선호

---

### 베이지안 선형회귀
- 모델의 파라미터에 대한 사전 분포를 다룰 예정
- $\mathbf{w}$의 사전확률을 다음과 같은 가우시안 분포라고 하면<br>
  $p(\mathbf{w})=\mathcal{N}\left(\mathbf{w} \mid \mathbf{m}_{0}, \mathbf{S}_{0}\right)$<br>
<br>
- 이 때 우도는 다음과 같음<br>
  $\begin{aligned} p(\mathbf{t} \mid \mathbf{w}) &=p\left(t_{1}, \ldots, t_{N} \mid \mathbf{w}\right) \\ &=\prod_{n=1}^{N} \mathcal{N}\left(t_{n} \mid \mathbf{w}^{T} \phi\left(\mathbf{x}_{n}\right), \beta^{-1}\right) \\ &=\mathcal{N}\left(\mathbf{t} \mid \Phi \mathbf{w}, \beta^{-1} \mathbf{I}\right) \end{aligned}$<br>
<br>

- 앞서 가우시안 분포에 대해 학습한 내용을 다시 생각해보면<br>
  $p(\mathbf{x})=\mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}, \Lambda^{-1}\right)$<br>
  $p(\mathbf{y} \mid \mathbf{x})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \mathbf{x}+\mathbf{b}, \mathbf{L}^{-1}\right)$<br>
  $\mathbb{E}[\mathbf{x} \mid \mathbf{y}]=\left(\Lambda+\mathbf{A}^{T} \mathbf{L} \mathbf{A}\right)^{-1}\left\{\mathbf{A}^{T} \mathbf{L}(\mathbf{y}-\mathbf{b})+\Lambda \boldsymbol{\mu}\right\}$<br>
  $\operatorname{cov}[\mathbf{x} \mid \mathbf{y}]=\left(\Lambda+\mathbf{A}^{T} \mathbf{L} \mathbf{A}\right)^{-1}$<br>
<br>
- 위의 식들을 $p(\mathbf{w})=\mathcal{N}\left(\mathbf{w} \mid \mathbf{m}_{0}, \mathbf{S}_{0}\right)$과 $p(\mathbf{t} \mid \mathbf{w}) =\mathcal{N}\left(\mathbf{t} \mid \Phi \mathbf{w}, \beta^{-1} \mathbf{I}\right) $에 적용하면<br>
  즉, 위의 식에 다음을 대입<br>
  $\begin{aligned} \mathbf{x} &=\mathbf{w} & \mathbf{y} &=\mathbf{t} & \Lambda^{-1} &=\mathbf{S}_{0} & \mathbf{L}^{-1} &=\beta^{-1} \mathbf{I} & \mathbf{A} &=\Phi & \boldsymbol{\mu} &=\mathbf{m}_{0} \end{aligned}$<br>
<br>
- 이에 따라 사후 확률 분포, 평균과 분산은 다음과 같이 표현이 가능<br>
  사후 확률 분포: $p(\mathbf{w} \mid \mathbf{t})=\mathcal{N}\left(\mathbf{w} \mid \mathbf{m}_{N}, \mathbf{S}_{N}\right)$<br>
  분산: $\mathbf{S}_{N} =\left(\Lambda+\mathbf{A}^{T} \mathbf{L} \mathbf{A}\right)^{-1} =\left(\mathbf{S}_{0}^{-1}+\beta \Phi^{T} \Phi\right)^{-1} \rightarrow \mathbf{S}_{N}^{-1}=\mathbf{S}_{0}^{-1}+\beta \Phi^{T} \Phi$<br>
  평균: $\mathbf{m}_{N} =\mathbf{S}_{N}\left\{\mathbf{A}^{T} \mathbf{L}(\mathbf{y}-\mathbf{b})+\Lambda \boldsymbol{\mu}\right\} =\mathbf{S}_{N}\left\{\Phi^{T} \beta \mathbf{I} \mathbf{t}+\mathbf{S}_{0}^{-1} \mathbf{m}_{0}\right\} =\mathbf{S}_{N}\left\{\mathbf{S}_{0}^{-1} \mathbf{m}_{0}+\beta \Phi^{T} \mathbf{t}\right\}$<br>
<br>
- 여기에서 사전 확률 분포를 평균이 0, 공분산이 $\alpha^{-1} \mathbf{I}$인 가우시안 분포라고 가정하면<br>
  즉, $p(\mathbf{w} \mid \alpha)=\mathcal{N}\left(\mathbf{w} \mid \mathbf{0}, \alpha^{-1} \mathbf{I}\right)$일 때
  분산: $\mathbf{S}_{N}^{-1}=\alpha \mathbf{I}+\beta \Phi^{T} \Phi$<br>
  평균: $\mathbf{m}_{N}=\beta \mathbf{S}_{N} \Phi^{T} \mathbf{t}$<br>
<br>
- 이 때의 사후 확률의 로그 값은 다음과 같음<br>
  $\displaystyle \ln p(\mathbf{w} \mid \mathbf{t})=-\frac{\beta}{2} \sum_{n=1}^{N}\left\{t_{n}-\mathbf{w}^{T} \phi\left(\mathbf{x}_{n}\right)\right\}^{2}-\frac{\alpha}{2} \mathbf{w}^{T} \mathbf{w}+\mathrm{const}$<br>
  위의 식은 최소제곱해의 오차에 규제항이 추가된 형태이고,<br>
  베이지안 모델이 빈도주의로 표현했을 때 보다 일반적인 표기이고 강력한 방법론

---

### 예측 분포
- 실제로는 $\mathbf{w}$에 대한 예측보다는 새로 들어오는 데이터($\mathbf{x}$)에 대한 예측이 빈번하게 발생
- 새로운 입력 $\mathbf{x}$에 대해 예측 값 $t$를 예측<br>
  $\displaystyle p(t \mid \mathbf{t}, \alpha, \beta)=\int p(t \mid \mathbf{w}, \beta) p(\mathbf{w} \mid \mathbf{t}, \alpha, \beta) \mathrm{d} \mathbf{w}$
- 이전 결과를 적용하게 되면 다음과 같은 결과를 얻을 수 있음<br>
  $\begin{aligned} p(t \mid \mathbf{x}, \mathbf{t}, \alpha, \beta) &=\mathcal{N}\left(t \mid \mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A} \Lambda^{-1} \mathbf{A}^{T}\right) \\ &=\mathcal{N}\left(t \mid \phi(\mathbf{x})^{T} \mathbf{m}_{N}, \beta^{-1}+\phi(\mathbf{x})^{T} \mathbf{S}_{N} \phi(\mathbf{x})\right) \end{aligned}$
  
---

### 느낀점
어려웠다. 대략적인 개념을 알고 코드로 구현한 것에 비해 수학적으로 깊이 파고 들었을 때 많이 곤란했다.<br>
복잡한 수식이 반복되지만 그 속에서도 반복되는 수식, 자주 등장하는 수식이 있기 때문에 그 수식부터 외우고,<br>
차근차근 다른 수식유도까지 적용해나간다면 충분히 잘 해낼 수 있을 것 같다.<br>
반드시 복습이 필요한 부분이라고 생각하기 때문에 시간이 날 때 복습을 해야할 것 같다.
