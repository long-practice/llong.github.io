---
title: í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ ì¸ê³µì§€ëŠ¥ ë°ë¸Œì½”ìŠ¤ 56ì¼ì°¨(ë³´ì¶©)
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 12ì£¼ì°¨(22.2.28.(ì›”) ~ 22.3.4.(ê¸ˆ)) 
  ë°ì´í„° íŒ€ì— ê´€í•œ ì»¤ë¦¬ì–´ ì´ì•¼ê¸°
  ë¹…ë°ì´í„°ì™€ ë¶„ì‚°ì²˜ë¦¬ ì‹œìŠ¤í…œ
  Hadoopê³¼ Sparkì— ëŒ€í•œ ì´í•´
  Sparkë¥¼ ì´ìš©í•œ ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì„¤ê³„
tags:
- Spark ì‹¤ìŠµ

use_math: True
---
### Spark ì‹¤ìŠµ

**ì‹¤ìŠµí™˜ê²½: êµ¬ê¸€ Colab**

pyspark íŒ¨í‚¤ì§€: pythonìœ¼ë¡œ Sparkê°ì²´ë¥¼ ë§Œë“¤ê³  í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ë¥¼ ì‘ë™ì‹œí‚¤ëŠ” í•¨ìˆ˜ë“¤ì„ í¬í•¨<br>
Py4j íŒ¨í‚¤ì§€: íŒŒì´ì¬ í”„ë¡œê·¸ë¨ì´ ìë°” ê°€ìƒë¨¸ì‹  ìƒì˜ ì˜¤ë¸Œì íŠ¸ë“¤ì„ ì ‘ê·¼í•  ìˆ˜ ìˆê²Œ í•´ì¤Œ


```python
!pip install pyspark==3.0.1 py4j==0.10.9
```

    Collecting pyspark==3.0.1
      Downloading pyspark-3.0.1.tar.gz (204.2 MB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204.2 MB 34 kB/s 
    [?25hCollecting py4j==0.10.9
      Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198 kB 56.6 MB/s 
    [?25hBuilding wheels for collected packages: pyspark
      Building wheel for pyspark (setup.py) ... [?25l[?25hdone
      Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=9b451b6cc2f85b3addccf4afa557f3f80277fe2086f5896a7eb21ca7b7a362b8
      Stored in directory: /root/.cache/pip/wheels/5e/34/fa/b37b5cef503fc5148b478b2495043ba61b079120b7ff379f9b
    Successfully built pyspark
    Installing collected packages: py4j, pyspark
    Successfully installed py4j-0.10.9 pyspark-3.0.1



```python
!ls -tl
```

    total 4
    drwxr-xr-x 1 root root 4096 Feb 18 14:33 sample_data



```python
!ls -tl sample_data
```

    total 55504
    -rw-r--r-- 1 root root 18289443 Feb 18 14:33 mnist_test.csv
    -rw-r--r-- 1 root root   301141 Feb 18 14:33 california_housing_test.csv
    -rw-r--r-- 1 root root  1706430 Feb 18 14:33 california_housing_train.csv
    -rw-r--r-- 1 root root 36523880 Feb 18 14:33 mnist_train_small.csv
    -rwxr-xr-x 1 root root     1697 Jan  1  2000 anscombe.json
    -rwxr-xr-x 1 root root      930 Jan  1  2000 README.md


#### Spark ì„¸ì…˜ ë§Œë“¤ê¸°


```python
from pyspark.sql import SparkSession

spark = SparkSession.builder\
        .master("local[*]")\
        .appName("PySpark_Tutorial")\
        .getOrCreate()
```


```python
spark
```





    <div>
        <p><b>SparkSession - in-memory</b></p>

<div>
    <p><b>SparkContext</b></p>

    <p><a href="http://d061b9830ae4:4040">Spark UI</a></p>

    <dl>
      <dt>Version</dt>
        <dd><code>v3.0.1</code></dd>
      <dt>Master</dt>
        <dd><code>local[*]</code></dd>
      <dt>AppName</dt>
        <dd><code>PySpark_Tutorial</code></dd>
    </dl>
</div>

    </div>




#### python ê°ì²´ë¥¼ RDDë¡œ ë³€í™˜


```python
num_list_json = ['{"number": "1"}', 
                  '{"number": "2"}', 
                  '{"number": "3"}', 
                  '{"number": "4"}',
                  '{"number": "5"}']
```


```python
# ë°ì´í„° í™•ì¸
for n in num_list_json:
    print(n)
```

    {"number": "1"}
    {"number": "2"}
    {"number": "3"}
    {"number": "4"}
    {"number": "5"}



```python
import json

for n in num_list_json:
    jn = json.loads(n)
    print(jn['number'])
```

    1
    2
    3
    4
    5


#### íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ë¥¼ RDDë¡œ ë³€í™˜
- RDDë¡œ ë³€í™˜ë˜ëŠ” ìˆœê°„ Spark í´ëŸ¬ìŠ¤í„°ë“¤ì˜ ì„œë²„ë“¤ì— ë°ì´í„°ê°€ ë‚˜ëˆ  ì €ì¥(íŒŒí‹°ì…˜)
- Lazy Execution: RDDê°€ ì–´ë–¤ ì˜ë¯¸ìˆëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•  ë•Œ ë°ì´í„°ê°€ RDDë¡œ ë³€í™˜
- ë§Œì•½ ì²œë§Œ ê°œì˜ ë°ì´í„°ë¥¼ ë½‘ëŠ”ë‹¤ê³  ìƒê°í•˜ë©´<br>
  ìœ„ì˜ ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë½‘ì„ ê²½ìš° êµ‰ì¥íˆ ë§ì€ ì‹œê°„ì´ ê±¸ë¦¬ì§€ë§Œ<br>
  ë¶„ì‚° ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œëŠ” êµ‰ì¥íˆ ë¹¨ë¦¬ ëë‚¼ ìˆ˜ ìˆìŒ


```python
rdd = spark.sparkContext.parallelize(num_list_json)
rdd
```




    ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262




```python
rdd.count()
```




    5




```python
parsed_rdd = rdd.map(lambda el:json.loads(el))
parsed_rdd
```




    PythonRDD[2] at RDD at PythonRDD.scala:53




```python
parsed_rdd.collect()
```




    [{'number': '1'},
     {'number': '2'},
     {'number': '3'},
     {'number': '4'},
     {'number': '5'}]




```python
parsed_num_rdd = rdd.map(lambda el: json.loads(el)['number'])
parsed_num_rdd.collect()
```




    ['1', '2', '3', '4', '5']



#### íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜


```python
from pyspark.sql.types import StringType

df = spark.createDataFrame(num_list_json, StringType())
```


```python
df.count()
```




    5




```python
df.printSchema()
```

    root
     |-- value: string (nullable = true)
    



```python
df.select('*').collect()
```




    [Row(value='{"number": "1"}'),
     Row(value='{"number": "2"}'),
     Row(value='{"number": "3"}'),
     Row(value='{"number": "4"}'),
     Row(value='{"number": "5"}')]




```python
df.select('value').collect()
```




    [Row(value='{"number": "1"}'),
     Row(value='{"number": "2"}'),
     Row(value='{"number": "3"}'),
     Row(value='{"number": "4"}'),
     Row(value='{"number": "5"}')]




```python
from pyspark.sql import Row

row = Row("number")
df_num = parsed_num_rdd.map(row).toDF()
```


```python
df_num.printSchema()
```

    root
     |-- number: string (nullable = true)
    



```python
df_num.select("number").collect()
```




    [Row(number='1'),
     Row(number='2'),
     Row(number='3'),
     Row(number='4'),
     Row(number='5')]



#### califonia housing ë°ì´í„°ë¥¼ ì´ìš©í•œ ë°ì´í„° ì²˜ë¦¬ 


```python
# Spark Session ë§Œë“¤ê¸°
from pyspark.sql import SparkSession

spark = SparkSession \
        .builder \
        .appName("Python Spark Dataframe basic example") \
        .getOrCreate()
```


```python
# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
import pandas as pd

housing_pandas_df = pd.read_csv("./sample_data/california_housing_test.csv")
housing_spark_df = spark.createDataFrame(housing_pandas_df)
```


```python
# ë°ì´í„° í”„ë ˆì„ì˜ ì¹¼ëŸ¼ë“¤ì„ ëª¨ë‘ ì¶œë ¥
housing_spark_df.columns
```




    ['longitude',
     'latitude',
     'housing_median_age',
     'total_rooms',
     'total_bedrooms',
     'population',
     'households',
     'median_income',
     'median_house_value']




```python
# ìŠ¤í‚¤ë§ˆ ì¶œë ¥
housing_spark_df.printSchema()
```

    root
     |-- longitude: double (nullable = true)
     |-- latitude: double (nullable = true)
     |-- housing_median_age: double (nullable = true)
     |-- total_rooms: double (nullable = true)
     |-- total_bedrooms: double (nullable = true)
     |-- population: double (nullable = true)
     |-- households: double (nullable = true)
     |-- median_income: double (nullable = true)
     |-- median_house_value: double (nullable = true)
    



```python
# ì²˜ìŒ 5ê°œ ë ˆì½”ë“œ ì¶œë ¥
housing_spark_df.show(n=5)
```

    +---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+
    |longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|
    +---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+
    |  -122.05|   37.37|              27.0|     3885.0|         661.0|    1537.0|     606.0|       6.6085|          344700.0|
    |   -118.3|   34.26|              43.0|     1510.0|         310.0|     809.0|     277.0|        3.599|          176500.0|
    |  -117.81|   33.78|              27.0|     3589.0|         507.0|    1484.0|     495.0|       5.7934|          270500.0|
    |  -118.36|   33.82|              28.0|       67.0|          15.0|      49.0|      11.0|       6.1359|          330000.0|
    |  -119.67|   36.33|              19.0|     1241.0|         244.0|     850.0|     237.0|       2.9375|           81700.0|
    +---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+
    only showing top 5 rows
    



```python
# ì¹¼ëŸ¼ë³„ ê°ì¢… í†µê³„ì¹˜ ì¶”ì¶œ
housing_spark_df.describe().show()
```

    +-------+-------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+
    |summary|          longitude|         latitude|housing_median_age|       total_rooms|   total_bedrooms|        population|       households|     median_income|median_house_value|
    +-------+-------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+
    |  count|               3000|             3000|              3000|              3000|             3000|              3000|             3000|              3000|              3000|
    |   mean|-119.58920000000013|35.63539000000004|28.845333333333333| 2599.578666666667|529.9506666666666|1402.7986666666666|          489.912|3.8072717999999988|        205846.275|
    | stddev| 1.9949362939550175|2.129669523343834|12.555395554955757|2155.5933316255814| 415.654368136323|1030.5430124122422|365.4227098055264|1.8545117296914786|113119.68746964629|
    |    min|            -124.18|            32.56|               1.0|               6.0|              2.0|               5.0|              2.0|            0.4999|           22500.0|
    |    max|            -114.49|            41.92|              52.0|           30450.0|           5419.0|           11935.0|           4930.0|           15.0001|          500001.0|
    +-------+-------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+
    



```python
# "total_rooms" ì¹¼ëŸ¼ì˜ í‰ê· ì„ êµ¬í•˜ê³  ìœ„ì˜ ê²°ê³¼ì™€ ë¹„êµ
from pyspark.sql.functions import mean

housing_spark_df.select(mean("total_rooms")).show()
```

    +-----------------+
    | avg(total_rooms)|
    +-----------------+
    |2599.578666666667|
    +-----------------+
    



```python
# "population"ì˜ ìµœëŒ“ê°’, ìµœì†Ÿê°’ êµ¬í•˜ê¸°
from pyspark.sql.functions import max, min

housing_spark_df.select(max("population"), min("population")).show()
```

    +---------------+---------------+
    |max(population)|min(population)|
    +---------------+---------------+
    |        11935.0|            5.0|
    +---------------+---------------+
    



```python
# "bedroom_ratio"ë¼ëŠ” ìƒˆë¡œìš´ ì¹¼ëŸ¼ ë§Œë“¤ê¸°
# bedroom_ratio = total_bedrooms / total_rooms

new_housing_spark_df = housing_spark_df.withColumn("bedroom_ratio", 
                                                  housing_spark_df.total_bedrooms/housing_spark_df.total_rooms)
new_housing_spark_df.select("total_rooms", "total_bedrooms", "bedroom_ratio").show(5)
```

    +-----------+--------------+-------------------+
    |total_rooms|total_bedrooms|      bedroom_ratio|
    +-----------+--------------+-------------------+
    |     3885.0|         661.0|0.17014157014157014|
    |     1510.0|         310.0| 0.2052980132450331|
    |     3589.0|         507.0| 0.1412649763165227|
    |       67.0|          15.0|0.22388059701492538|
    |     1241.0|         244.0|0.19661563255439163|
    +-----------+--------------+-------------------+
    only showing top 5 rows
    

