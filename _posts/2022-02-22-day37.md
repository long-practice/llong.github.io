---
title: 프로그래머스 인공지능 데브코스 37일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 8주차(22.1.24.(월) ~ 22.1.28.(금)) 
  선형회귀
  선형분류
  Weekly Mission
tags:
- 크로스 엔트로피
- 그레디언트
- 다중클래스 로지스틱 회귀
- 소프트맥스 함수
- 우도함수
- 음의 로그우도
- 배치 경사 하강법
- 확률적 경사 하강법
- 미니 배치 경사 하강법

use_math: True
---
# 37일차(ML_basics - 선형분류)

8주차(22.1.24.(월) ~ 22.1.28.(금)): ML_basic2
* 선형회귀
* 선형분류
* Weekly Mission
<br>
<br>

출처: 프로그래머스 인공지능 데브코스 3기 강의

### 크로스 엔트로피 에러함수의 그레디언트
- 앞에서 언급한 크로스 엔트로피<br>
  $ E_n(\mathbf{w}) = - \{t_n \ln y_n + (1 - t_n) \ln (1 - y_n)\}$<br>
<br>
- $a_n$에 대해 미분해주면<br>
  $\displaystyle \nabla E_{n}(\mathbf{w})=\frac{\partial E_{n}(\mathbf{w})}{\partial y_{n}} \frac{\partial y_{n}}{\partial a_{n}} \nabla a_{n} = \{\frac{1-t_{n}}{1-y_{n}}-\frac{t_{n}}{y_{n}}\} y_{n}(1-y_{n}) \phi_{n} = (y_n - t_n)\phi_n$<br>
  <br>
  따라서 $\displaystyle\nabla E(\mathbf{w}) = \sum_{n=1}^N (y_n - t_n)\phi_n$

---

### 다중클래스 로지스틱 회귀
- 각각의 클래스에 대한 확률을 표현<br>
  $\displaystyle p(C_{k} \mid \phi)=y_{k}(\phi)=\frac{\exp (a_{k})}{\sum_{j} \exp (a_{j})}$<br>
$a_{k}=\mathbf{w}_{k}^{T} \phi$<br>
<br>

#### 우도함수
- 특성벡터 $\phi_n$을 위한 목표벡터 $\mathbf{t}\_n$는 하나의 원소만 1이고 나머지는 0인 1-of-K 인코딩 적용<br>
  $\displaystyle p(\mathbf{T} \mid \mathbf{w}\_{1}, \cdots, \mathbf{w}\_{K})=\prod_{n=1}^{N} \prod_{k=1}^{K} p(C_{k} \mid \phi_{n})^{t_{n k}}=\prod_{n=1}^{N} \prod_{k=1}^{K} y_{n k}^{t_{n k}}$<br>
  <br>
- 양 변에 로그를 취해주면(이 때 $\mathbf{T}$는 $t_{nk}$를 원소로 가지는 $N \times K$ 행렬)<br>
  $\displaystyle E(\mathbf{w}\_{1}, \cdots, \mathbf{w}\_{K})=-\ln p(\mathbf{T} \mid \mathbf{w}\_{1}, \cdots, \mathbf{w}\_{K})=-\sum_{n=1}^{N} \sum_{k=1}^{K} t_{n k} \ln (y_{n k})$<br>
  <br>

#### 그레디언트 계산
- 하나의 샘플 $\phi_n$에 대한 손실함수는<br>
  $\displaystyle E_{n}(\mathbf{w}\_{1}, \cdots, \mathbf{w}\_{K})=-\sum_{k=1}^{K} t_{n k} \ln (y_{n k})$<br>
<br>
- 그레디언트를 계산해보면<br>
  $\displaystyle \nabla_{\mathbf{w}\_{j}} E(\mathbf{w}\_{1}, \ldots, \mathbf{w}\_{K})=\sum_{n=1}^{N} \nabla_{\mathbf{w}\_{j}} E_{n}(\mathbf{w}\_{1}, \cdots, \mathbf{w}\_{K})$<br>
<br>
- $E_n$과 $\mathbf{w}\_j$의 관계는 오직 $a_{nj}$에만 의존
- $E_n$은 $y_{n1}, \cdots, y_{nK}$의 함수, $y_{nk}$는 $a_{n1}, \cdots, a_{nK}$의 함수<br>
<br>

- 따라서 다음과 같이 그레디언트 계산을 할 수 있음($\displaystyle \frac{\partial y_{nk}}{\partial a_{nj}}$는 $k \ne j$와 $k = j$로 나누어서 계산)<br>
  $\begin{aligned*} \nabla_{\mathbf{w}\_{j}} E_{n} &=\frac{\partial E_{n}}{\partial a_{n j}} \frac{\partial a_{n j}}{\partial \mathbf{w}\_{j}} \\ &=\frac{\partial E_{n}}{\partial a_{n j}} \phi_{n} \\ &=\sum_{k=1}^{K}(\frac{\partial E_{n}}{\partial y_{n k}} \frac{\partial y_{n k}}{\partial a_{n j}}) \phi_{n} \\ &=\phi_{n} \sum_{k=1}^{K}\{-\frac{t_{n k}}{y_{n k}} y_{n k}(I_{k j}-y_{n j})\} \\ &=\phi_{n} \sum_{k=1}^{K} t_{n k}(y_{n j}-I_{k j}) \\ &=\phi_{n}(y_{n j} \sum_{k=1}^{K} t_{n k}-\sum_{k=1}^{K} t_{n k} I_{k j}) \\ &=\phi_{n}(y_{n j}-t_{n j}) \end{aligned*}$<br>
<br>

- 그레디언트는 다음과 같음<br>
  $\displaystyle \nabla_{\mathbf{w}\_{j}} E(\mathbf{w}\_{1}, \ldots, \mathbf{w}\_{K})=\sum_{n=1}^{N}(y_{n j}-t_{n j}) \phi_{n}$

---

### 라이브러리 불러오기


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

%matplotlib inline
import seaborn as sns

# 사이킷런에서 제공하는 분류 데이터를 가져올 때 사용
from sklearn.datasets import make_classification

import warnings
warnings.filterwarnings(action = 'ignore')
```

---


### Gradient Descent(batch) 실습


```python
# 분류 데이터 가져오기
# sample이 500개인 이진 분류 데이터셋 생성
X, t = make_classification(n_samples = 500, n_features = 2, n_redundant = 0, n_informative = 1,
                          n_clusters_per_class = 1, random_state = 14)

t = t[:, np.newaxis]

sns.set_style('white')
sns.scatterplot(X[:, 0], X[:, 1], hue = t.reshape(-1))
plt.show()
```


    
<img width = "1200", src = "https://user-images.githubusercontent.com/83870423/155074868-adb1216e-1ce8-4555-afd9-fd87cc2944c7.png">
    
---


```python
# 시그모이드 함수 구현
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```


```python
def compute_cost(X, t, w):
    N = len(X)
    h = sigmoid(X @ w)
    epsilon = 1e-5
    
    # 로지스틱 손실함수를 벡터로 계산시켜 간편하게 구현
    cost = (1/N)*(((-t).T @ np.log(h + epsilon)) - ((1 - t).T @ np.log(1 - h + epsilon)))
    return cost
```

---

```python
# 경사하강법
def gradient_descent(X, t, w, learning_rate, iterations):
    N = len(t)
    # 가중치 업데이트됨에 따라 손실함수가 어떻게 되었는지 확인할 수 있음
    cost_history = np.zeros((iterations, 1))
    
    for i in range(iterations):
        # 가중치 업데이트
        w = w - (learning_rate / N) * (X.T @(sigmoid(X @ w) - t))
        cost_history[i] = compute_cost(X, t, w)
        
    return cost_history, w
```

---

```python
# 예측함수
def predict(X, w):
    return np.round(sigmoid(X @ w))
```

---

```python
# main
N = len(t)

X = np.hstack((np.ones((N, 1)), X))
M = np.size(X, 1)
w = np.zeros((M, 1))

# 1000번 반복
# 1회 반복 시 모든 데이터를 읽음
# 1000번 데이터를 읽은 것과 동일
iterations = 1000
learning_rate = 0.01

# 최초 손실함수 계산
initial_cost = compute_cost(X, t, w)

print(f"Initial Cost is: {initial_cost} \n")

cost_history, w_optimal = gradient_descent(X, t, w, learning_rate, iterations)

print(f"Optimal Parameters are: \n {w_optimal} \n")

plt.figure()
sns.set_style('white')
plt.plot(range(len(cost_history)), cost_history, 'r')
plt.title("Convergence Graph of Cost Function")
plt.xlabel("Number of Iterations")
plt.ylabel("Cost")
plt.show()
```

    Initial Cost is: [[0.69312718]] 
    
    Optimal Parameters are: 
     [[-0.07024012]
     [ 1.9275589 ]
     [ 0.02285894]] 
    


<img width = "1200", src = "https://user-images.githubusercontent.com/83870423/155075004-42fe542d-63e2-4fdd-816f-89e48f4a18c1.png">
    

---


```python
# 정확도 측정
# 우리가 예측한 값과 타깃값이 얼마나 일치하는지 확인

y_pred = predict(X, w_optimal)
score = float(sum(y_pred == t)) / float(len(t))

print(score)
```

    0.954

---

```python
# w 값을 업데이트를 반복하여 결정 경계를 그리기
slope = -(w_optimal[1] / w_optimal[2])
intercept = -(w[0] / w_optimal[2])

sns.set_style('white')
sns.scatterplot(X[:,1], X[:,2], hue = t.reshape(-1))

ax = plt.gca()
ax.autoscale(False)
x_vals = np.array(ax.get_xlim())
y_vals = intercept + (slope * x_vals)
plt.plot(x_vals, y_vals, c = "k")

plt.show()
```



<img width = "1200", src = "https://user-images.githubusercontent.com/83870423/155075059-948ea35f-084d-4a3a-aaef-deacbeaf885e.png">
    
---


### Stochastic Gradient Descent 실습
가중치 업데이트 시 모든 데이터를 이용하는 것이 아닌 데이터 일부만을 이용하여 가중치 업데이트


```python
def sgd(X, t, w, learning_rate, iterations):
    N = len(t)
    cost_history = np.zeros((iterations, 1))
    
    for i in range(iterations):
        i = i % N
        # i 번째 원소만을 이용하여 가중치 업데이트
        # X 데이터 전체를 이용하긴 하지만 i번째 만을 이용하여 1회전 시 가중치 업데이트
        w = w - learning_rate * (X[i, np.newaxis].T * (sigmoid(X[i] @ w) - t[i]))
        cost_history[i] = compute_cost(X[i], t[i], w)
        
    return cost_history, w
```

---

```python
# 데이터 불러오기
X, t = make_classification(n_samples = 500, n_features = 2, n_redundant = 0, n_informative = 1,
                           n_clusters_per_class = 1, random_state = 14)

t = t[:, np.newaxis]

N = len(t)

X = np.hstack((np.ones((N, 1)), X))
M = np.size(X, 1)
w = np.zeros((M, 1))

# 1000번 반복
# 1회 반복 시 데이터 일부만을 읽음
iterations = 2000
learning_rate = 0.01

# 최초 손실함수 계산
initial_cost = compute_cost(X, t, w)

print(f"Initial Cost is: {initial_cost} \n")

# SGD를 실행
cost_history, w_optimal = sgd(X, t, w, learning_rate, iterations)

print(f"Optimal Parameters are: \n {w_optimal} \n")

plt.figure()
sns.set_style('white')
plt.plot(range(len(cost_history)), cost_history, 'r')
plt.title("Convergence Graph of Cost Function")
plt.xlabel("Number of Iterations")
plt.ylabel("Cost")
plt.show()
```

    Initial Cost is: [[0.69312718]] 
    
    Optimal Parameters are: 
     [[-0.19304782]
     [ 2.5431236 ]
     [ 0.01130098]] 
    

---


<img width = "1200", src = "https://user-images.githubusercontent.com/83870423/155075143-562f7ec6-3529-4636-89f5-30cd408ec2cb.png">
    


배치 경사 하강법은 손실함수가 부드럽게 내려오는 반면<br>
확률적 경사 하강법은 데이터 하나하나에 대해 가중치가 업데이트가 되기 때문에 데이터에 따라서 극단적으로 값이 변하기도 함


```python
# 정확도 측정
# 우리가 예측한 값과 타깃값이 얼마나 일치하는지 확인

y_pred = predict(X, w_optimal)
score = float(sum(y_pred == t)) / float(len(t))

print(score)
```

    0.96

---

### 미니 배치 경사 하강법
배치 사이즈를 조절하여 전체 데이터들 중 여러 데이터들을 이용하여 가중치를 업데이트<br>
배치 경사 하강법과 SGD의 중간 정도


```python
def batch_gd(X, t, w, learning_rate, iterations, batch_size):
    N = len(t)
    cost_history = np.zeros((iterations, 1))
    shuffled_indices = np.random.permutation(N)
    X_shuffled = X[shuffled_indices]
    t_shuffled = t[shuffled_indices]
    
    for i in range(iterations):
        i = i % N
        X_batch = X_shuffled[i:i + batch_size]
        t_batch = t_shuffled[i:i + batch_size]
        if X_batch.shape[0] < batch_size:
            X_batch = np.vstack((X_batch, X_shuffled[:(batch_size - X_batch.shape[0])]))
            t_batch = np.vstack((t_batch, t_shuffled[:(batch_size - t_batch.shape[0])]))
        w = w - (learning_rate/batch_size) * (X_batch.T @ (sigmoid(X_batch @ w) - t_batch))
        cost_history[i] = compute_cost(X_batch, t_batch, w)
    
    return cost_history, w
```

---


```python
# 데이터 불러오기
X, t = make_classification(n_samples = 500, n_features = 2, n_redundant = 0, n_informative = 1,
                           n_clusters_per_class = 1, random_state = 14)

t = t[:, np.newaxis]

N = len(t)

X = np.hstack((np.ones((N, 1)), X))
M = np.size(X, 1)
w = np.zeros((M, 1))

# 1000번 반복
# 1회 반복 시 데이터 일부(batch_size만큼)만을 읽음
batch_size = 32
iterations = 1000
learning_rate = 0.01

# 최초 손실함수 계산
initial_cost = compute_cost(X, t, w)

print(f"Initial Cost is: {initial_cost} \n")

# 미니배치 경사하강법 실행
cost_history, w_optimal = batch_gd(X, t, w, learning_rate, iterations, batch_size)

print(f"Optimal Parameters are: \n {w_optimal} \n")

plt.figure()
sns.set_style('white')
plt.plot(range(len(cost_history)), cost_history, 'r')
plt.title("Convergence Graph of Cost Function")
plt.xlabel("Number of Iterations")
plt.ylabel("Cost")
plt.show()
```

    Initial Cost is: [[0.69312718]] 
    
    Optimal Parameters are: 
     [[-0.05319971]
     [ 1.93011946]
     [ 0.02938162]] 
    


<img width = "1200", src = "https://user-images.githubusercontent.com/83870423/155075217-55a3711f-e46b-486f-9005-c78f26559a0f.png">
    
---


```python
# 정확도 측정
# 우리가 예측한 값과 타깃값이 얼마나 일치하는지 확인

y_pred = predict(X, w_optimal)
score = float(sum(y_pred == t)) / float(len(t))

print(score)
```

    0.952

---

### 느낀점
밀렸던 til 작성이 어느정도 마무리 된 것 같다. 복잡한 수식들의 연속이었긴 하지만 실제로 수식이 나타내는 의미는 그리 큰 내용은 아니었으나<br>
수식으로 표현하는 것이 다른 사람을 설득할 때 논리적인 근거가 될 수 있기에 절대 수식을 다루는 것을 게을리하면 안될 것 같다.<br>
더 나아가서 코드로 구현하는 것은 또 다른 영역이기에 많은 노력이 필요할 듯 싶다
