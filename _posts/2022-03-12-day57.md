---
title: 프로그래머스 인공지능 데브코스 57일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 12주차(22.2.28.(월) ~ 22.3.4.(금)) 
  데이터 팀에 관한 커리어 이야기
  빅데이터와 분산처리 시스템
  Hadoop과 Spark에 대한 이해
  Spark를 이용한 데이터 전처리 및 머신러닝 모델 설계
tags:
- SparkSQL
- Spark를 이용하여 Redshift 연결

use_math: True
---

# 57일차(Spark로 Redshift 연결, Spark SQL)

12주차(22.2.28.(월) ~ 22.3.4.(금)): 데이터 팀에 관한 내용, SPARK
* 데이터 팀의 역할
* 커리어 이야기
* SPARK
* SPARK 실습
<br>
<br>

출처: 프로그래머스 인공지능 데브코스 3기 강의

<br>

### SparkSQL
- 구조화된 데이터 처리를 위한 Spark 모듈
- 대화형 Spark shell이 제공
- 하둡 상 데이터를 기반으로 작성된 Hive 쿼리의 경우 변경없이 5 ~ 10배까지 빠른 성능
- 데이터 프레임을 SQL로 처리<br>
  - RDD데이터는 결국 데이터 프레임으로 변환한 후에 처리 가능
  - 외부 데이터(스토리지, 관계형 데이터베이스)는 데이터 프레임으로 변환 후 처리 가능
  - 데이터프레임은 테이블이 되고 그 다음부터 sql함수를 사용가능

### Spark 실습



pyspark 패키지:
Py4j 패키지:

<br>

디렉토리 이동 후 redshift를 읽는데 필요한 JAR파일을 설치

<br>   

Spark Session 만들기(**\뒤에 공백 없는지 꼭 확인**)

<br>

판다스를 이용하여 california_housing_train 데이터 불러오기

<br>

판다스 데이터 프레임을 Spark 데이터 프레임으로 변환

<br>

데이터 프레임을 테이블 뷰로 만들어서 SparkSQL을 처리

<br>

Redshift와 연결해서 테이블들을 데이터프레임으로 로딩<br>
driver: Redshift 연결을 위한 JDBC 드라이버<br>
url: 어디에 엑세스를 할 것이고 그 때 아이디와 패스워드는 뭔지 명시
<br>

Spark sql에서 각 테이블(usc, st)을 데이터 프레임(df_)으로 지정

Spark sql 확인
채널별 서로 다른 userid 개수

<br>

시간이 좀 걸리는 이유 <br>
Lazy Execution: 유의미한 동작을 하기 전까지 세션을 생성하지 않기 때문<br>
위에서 spark 명령들을 show라는 명령이 내려지면서 실행

<br>

show()는 collect()명령을 수행해서 python drive단으로 결과를 끌어와서 출력

<br>

collect() 입력 시

<br>


### 느낀점
Redshift에 Spark를 직접 연동하면서 Spark가 어떤방식으로 동작하는지 감이 잡힌듯하다.<br>
개념으로만 봤을 때는 너무 추상적이어서 당황스러운 부분이 있었으나<br>
실제로 코드를 동작하면서 왜 동작이 느린지, 각각의 명령어, 특히 driver, url을 통해 어떤 방식으로 코드가 작동되고<br>
어떤 방식으로 출력이 이루어지는지 잘 알게되었다.<br>
또한 SQL을 다시 한 번 복습하고, Spark SQL을 사용하게 됐는데 잘 연동되고 SQL을 그대로 사용할 수 있다는 것이 신기했다.<br>
내일부터는 머신러닝 관련 라이브러리를 배우게 될텐데 이전에 배웠던 것들과 잘 연결시켜야할 것 같다
