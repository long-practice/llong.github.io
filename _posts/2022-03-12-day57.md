---
title: 프로그래머스 인공지능 데브코스 57일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 12주차(22.2.28.(월) ~ 22.3.4.(금)) 
  데이터 팀에 관한 커리어 이야기
  빅데이터와 분산처리 시스템
  Hadoop과 Spark에 대한 이해
  Spark를 이용한 데이터 전처리 및 머신러닝 모델 설계
tags:
- SparkSQL
- Spark를 이용하여 Redshift 연결

use_math: True
---
# 57일차(Spark로 Redshift 연결, Spark SQL)

12주차(22.2.28.(월) ~ 22.3.4.(금)): 데이터 팀에 관한 내용, SPARK
* 데이터 팀의 역할
* 커리어 이야기
* SPARK
* SPARK 실습
<br>
<br>

출처: 프로그래머스 인공지능 데브코스 3기 강의

### SparkSQL
- 구조화된 데이터 처리를 위한 Spark 모듈
- 대화형 Spark shell이 제공
- 하둡 상 데이터를 기반으로 작성된 Hive 쿼리의 경우 변경없이 5 ~ 10배까지 빠른 성능
- 데이터 프레임을 SQL로 처리<br>
  - RDD데이터는 결국 데이터 프레임으로 변환한 후에 처리 가능
  - 외부 데이터(스토리지, 관계형 데이터베이스)는 데이터 프레임으로 변환 후 처리 가능
  - 데이터프레임은 테이블이 되고 그 다음부터 sql함수를 사용가능

### Spark 실습



pyspark 패키지:
Py4j 패키지:


```python
!pip install pyspark==3.0.1 py4j==0.10.9
```

    Requirement already satisfied: pyspark==3.0.1 in /usr/local/lib/python3.7/dist-packages (3.0.1)
    Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (0.10.9)

<br>


디렉토리 이동 후 redshift를 읽는데 필요한 JAR파일을 설치


```python
!cd /usr/lib/python3.6/dist-packages/pyspark/jars && wget https://s3.amazonaws.com/redshift-downloads/drivers/jdbc/1.2.20.1043/RedshiftJDBC42-no-awssdk-1.2.20.1043.jar
```

    --2022-03-03 09:39:54--  https://s3.amazonaws.com/redshift-downloads/drivers/jdbc/1.2.20.1043/RedshiftJDBC42-no-awssdk-1.2.20.1043.jar
    Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.234.240
    Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.234.240|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 2413910 (2.3M) [application/java-archive]
    Saving to: ‘RedshiftJDBC42-no-awssdk-1.2.20.1043.jar.1’
    
    RedshiftJDBC42-no-a 100%[===================>]   2.30M  5.12MB/s    in 0.4s    
    
    2022-03-03 09:39:54 (5.12 MB/s) - ‘RedshiftJDBC42-no-awssdk-1.2.20.1043.jar.1’ saved [2413910/2413910]
    

<br>


```python
# Spark Session 만들기
# \뒤에 공백 없는지 꼭 확인
from pyspark.sql import SparkSession

spark = SparkSession \
      .builder \
      .appName("Python Spark SQL basic example") \
      .config("spark.jars", "/usr/lib/python3.6/dist-packages/pyspark/jars/RedshiftJDBC42-no-awssdk-1.2.20.1043.jar") \
      .getOrCreate()
```


```python
spark
```





<div>
    <p><b>SparkSession - in-memory</b></p>

    <div>
        <p><b>SparkContext</b></p>

        <p><a href="http://f993716feab9:4040">Spark UI</a></p>

        <dl>
          <dt>Version</dt>
            <dd><code>v3.0.1</code></dd>
          <dt>Master</dt>
            <dd><code>local[*]</code></dd>
          <dt>AppName</dt>
            <dd><code>Python Spark SQL basic example</code></dd>
        </dl>
    </div>

</div>


<br><br>


```python
# 판다스를 이용하여 california_housing_train 데이터 불러오기
import pandas as pd

housing_pd = pd.read_csv("./sample_data/california_housing_train.csv")
housing_pd = housing_pd[["longitude", "latitude", "housing_median_age"]]
housing_pd.head()
```





  <div id="df-a5e1b6eb-2da5-462e-9a22-d3197db4d59e">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>housing_median_age</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-114.31</td>
      <td>34.19</td>
      <td>15.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-114.47</td>
      <td>34.40</td>
      <td>19.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-114.56</td>
      <td>33.69</td>
      <td>17.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-114.57</td>
      <td>33.64</td>
      <td>14.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-114.57</td>
      <td>33.57</td>
      <td>20.0</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-a5e1b6eb-2da5-462e-9a22-d3197db4d59e')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-a5e1b6eb-2da5-462e-9a22-d3197db4d59e button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-a5e1b6eb-2da5-462e-9a22-d3197db4d59e');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>


<br>


```python
housing_pd[['longitude', 'housing_median_age']].groupby(["housing_median_age"]).count().head()
```





  <div id="df-bf5372a5-3f87-441b-a829-8a01dd3f0a26">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
    </tr>
    <tr>
      <th>housing_median_age</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1.0</th>
      <td>2</td>
    </tr>
    <tr>
      <th>2.0</th>
      <td>49</td>
    </tr>
    <tr>
      <th>3.0</th>
      <td>46</td>
    </tr>
    <tr>
      <th>4.0</th>
      <td>161</td>
    </tr>
    <tr>
      <th>5.0</th>
      <td>199</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-bf5372a5-3f87-441b-a829-8a01dd3f0a26')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-bf5372a5-3f87-441b-a829-8a01dd3f0a26 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-bf5372a5-3f87-441b-a829-8a01dd3f0a26');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>


<br>


```python
# 판다스 데이터 프레임을 Spark 데이터 프레임으로 변환
housing_df = spark.createDataFrame(housing_pd)
housing_df.printSchema()
```

    root
     |-- longitude: double (nullable = true)
     |-- latitude: double (nullable = true)
     |-- housing_median_age: double (nullable = true)
    


<br>

```python
# local에서 일어나는 것이 아닌 spark cluster에서 이루어지는 작업
# show를 하게되면 collect가 된 후 결과를 출력
housing_df.show(5)
```

    +---------+--------+------------------+
    |longitude|latitude|housing_median_age|
    +---------+--------+------------------+
    |  -114.31|   34.19|              15.0|
    |  -114.47|    34.4|              19.0|
    |  -114.56|   33.69|              17.0|
    |  -114.57|   33.64|              14.0|
    |  -114.57|   33.57|              20.0|
    +---------+--------+------------------+
    only showing top 5 rows
    

<br>

```python
# 새로운 데이터 프레임이 생성
housing_df.groupBy(["housing_median_age"]).count()

# local에 있는 python으로 받아와서 결과를 출력(list type)
sorted(housing_df.groupBy(["housing_median_age"]).count().collect())[:5]
```




    [Row(housing_median_age=1.0, count=2),
     Row(housing_median_age=2.0, count=49),
     Row(housing_median_age=3.0, count=46),
     Row(housing_median_age=4.0, count=161),
     Row(housing_median_age=5.0, count=199)]


<br>

```python
# 데이터 프레임을 테이블 뷰로 만들어서 SparkSQL을 처리
housing_df.createOrReplaceTempView("housing")
```


```python
# housing이라는 테이블에 대해 SQL 적용 가능
housing_group_df = spark.sql(
    "SELECT housing_median_age, count(housing_median_age) FROM housing GROUP BY housing_median_age")
sorted(housing_group_df.collect())[:5]
```




    [Row(housing_median_age=1.0, count(housing_median_age)=2),
     Row(housing_median_age=2.0, count(housing_median_age)=49),
     Row(housing_median_age=3.0, count(housing_median_age)=46),
     Row(housing_median_age=4.0, count(housing_median_age)=161),
     Row(housing_median_age=5.0, count(housing_median_age)=199)]

<br>


```python
# Redshift와 연결해서 테이블들을 데이터프레임으로 로딩
# driver: Redshift 연결을 위한 JDBC 드라이버
# url: 어디에 엑세스를 할 것이고 그 때 아이디와 패스워드는 뭔지 명시

df_user_session_channel = spark.read \
    .format("jdbc") \
    .option("driver", "com.amazon.redshift.jdbc42.Driver") \
    .option("url", "jdbc:redshift://...") \
    .option("dbtable", "raw_data.user_session_channel") \
    .load()


df_session_timestamp = spark.read \
    .format("jdbc") \
    .option("driver", "com.amazon.redshift.jdbc42.Driver") \
    .option("url", "jdbc:redshift://...") \
    .option("dbtable", "raw_data.session_timestamp") \
    .load()
```

<br>

```python
# Spark sql에서 각 테이블(usc, st)을 데이터 프레임(df_)으로 지정
df_user_session_channel.createOrReplaceTempView("user_session_channel")
df_session_timestamp.createOrReplaceTempView("session_timestamp")
```

<br>

```python
# Spark sql 확인
# 채널별 서로 다른 userid 개수
channel_count_df = spark.sql("""
    SELECT channel, count(distinct userId) uniqueUsers
    FROM session_timestamp st
    JOIN user_session_channel usc ON st.sessionID = usc.sessionID
    GROUP BY channel
    ORDER BY channel
""")

# channel_count_df # 단순 spark cluster 위에 존재: 칼럼에 대한 정보만 확인 가능

# 시간이 좀 걸리는 이유 
# Lazy Execution: 유의미한 동작을 하기 전까지 세션을 생성하지 않기 때문
# 위에서 spark 명령들을 show라는 명령이 내려지면서 실행

# show()는 collect()명령을 수행해서 python drive단으로 결과를 끌어와서 출력
channel_count_df.show()
```

    +---------+-----------+
    |  channel|uniqueUsers|
    +---------+-----------+
    | Facebook|        889|
    |   Google|        893|
    |Instagram|        895|
    |    Naver|        882|
    |  Organic|        895|
    |  Youtube|        889|
    +---------+-----------+
    

<br>


```python
# collect() 입력 시
channel_with_o_count_df = spark.sql("""
    SELECT COUNT(1)
    FROM user_session_channel
    WHERE channel like '%o%'
""")

channel_with_o_count_df.collect()
```




    [Row(count(1)=50864)]



### 느낀점
Redshift에 Spark를 직접 연동하면서 Spark가 어떤방식으로 동작하는지 감이 잡힌듯하다.<br>
개념으로만 봤을 때는 너무 추상적이어서 당황스러운 부분이 있었으나<br>
실제로 코드를 동작하면서 왜 동작이 느린지, 각각의 명령어, 특히 driver, url을 통해 어떤 방식으로 코드가 작동되고<br>
어떤 방식으로 출력이 이루어지는지 잘 알게되었다.<br>
또한 SQL을 다시 한 번 복습하고, Spark SQL을 사용하게 됐는데 잘 연동되고 SQL을 그대로 사용할 수 있다는 것이 신기했다.<br>
내일부터는 머신러닝 관련 라이브러리를 배우게 될텐데 이전에 배웠던 것들과 잘 연결시켜야할 것 같다
