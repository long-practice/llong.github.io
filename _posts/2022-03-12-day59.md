---
title: 프로그래머스 인공지능 데브코스 59일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 12주차(22.2.28.(월) ~ 22.3.4.(금)) 
  데이터 팀에 관한 커리어 이야기
  빅데이터와 분산처리 시스템
  Hadoop과 Spark에 대한 이해
  Spark를 이용한 데이터 전처리 및 머신러닝 모델 설계
tags:
- Spark.mllib
- MinMaxScaler
- StandardScaler
- LinearRegression
- DecisionTreeRegressor
- GBTRegresssor
- RandomForestRegressor
- monotonically_increasing_id

use_math: True
---

# 59일차(Weekly mission)

12주차(22.2.28.(월) ~ 22.3.4.(금)): 데이터 팀에 관한 내용, SPARK
* 데이터 팀의 역할
* 커리어 이야기
* SPARK
* SPARK 실습
<br>
<br>

출처: 프로그래머스 인공지능 데브코스 3기 강의

### 타이베이 주택가격 예측 모델

![1.png](attachment:1.png)

### 피처벡터 만들기

![2.png](attachment:2.png)

![3.png](attachment:3.png)

4개의 모델(선형 회귀, 결정 트리, 랜덤 포레스트, GBT 회귀)에 대해 각각 예측값을 구하기

![4.png](attachment:4.png)

각각의 예측값을 하나의 테이블로 결합해주기(**autoincrement를 이용하여 테이블 병합하는 것을 주목**)
![5.png](attachment:5.png)

예측값들로 이루어진 피처벡터를 만들고 선형 회귀모델을 적용하여 최종 모델을 만들기
![6.png](attachment:6.png)

테스트 데이터에 대해 적용
![7.png](attachment:7.png)

---

### 느낀점
과제는 어렵지 않았으나 개인적으로 앙상블 기법을 적용하는 등 다양한 시도를 해보고 싶었다.<br>
특히 앙상블 기법을 적용하기 위해 예측값들을 하나의 테이블로 모아주는 즉, 칼럼을 하나의 테이블로 모아주는 과정이 굉장히 까다로웠다.<br>
그 과정에서 `monotonically_increasing_id`라는 것에 대해 알 수 있었고, 또 하나의 문제해결 방식을 얻게 된것 같다.<br>
많은 시도를 하는 만큼 많이 배울 수 있다는 것을 다시 한 번 느꼈다.
