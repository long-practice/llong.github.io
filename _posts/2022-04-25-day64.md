---
title: 프로그래머스 인공지능 데브코스 64일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 13주차(22.3.14.(월) ~ 22.3.18.(금)) 
  Spark ML Pipeline
  NLP 개요, 언어모델, 단어 임베딩, 텍스트 전처리
tags:
- 자연어 처리(NLP)
- 단어 임베딩
- 유사도
- TF-IDF
- Word2Vec
- Skip-Gram

use_math: True
---
# 64일차(단어 임베딩)

13주차(22.3.14.(월) ~ 22.3.18.(금)): 빅데이터 총정리, NLP
* 빅데이터 총정리
* NLP 기초(텍스트 전처리, 언어모델, 문서분류, 단어 임베딩) 강의
<br>
<br>

출처: 프로그래머스 인공지능 데브코스 3기 강의

### 단어 임베딩
- 단어의 의미를 어떻게 표현할지 제일 중요
- 좋은 표현방식은 단어간의 관계를 잘 표현할 수 있어야 함
- 단어의 의미가 비슷한 단어(같은 어근을 공유)(mouse, mice ...)들,<br>
  문맥상 같은 의미를 가지는 단어들(동의어)은 항상 같은 단어로 대체할 수는 없음
- 그러나 유사한 의미를 가진 단어들(vanish, disappear ...) 사이에서 연관 정도를 머신러닝 모델이 판단할 수 있음<br>
<br>

- 단어들은 의미의 유사성 외에도 다양한 방식으로 연관될 수 있음
- Semantic Field: 특정한 주체(topic)이나 영역을 공유하는 단어들<br>
  ex) restaurants: waiter, menu, plate, food, chef ...
- Semantic Frame: 특정 행위에 참여하는 주체들의 역할에 관한 단어들<br>
  ex) buy, sell, pay ...는 같은 semantic frame을 공유<br>
<br>

#### 벡터로 의미 표현
- 단어들든 주변 단어들의 분포에 의해 의미가 결정(두 단어 A, B가 주변 단어들의 분포가 거의 동일하다면 두 단어는 유사어)
- 단어의 의미를 분포적 유사성(distributional similarity)으로써 표현
- 즉, 벡터 공간 내에서는 비슷한 단어들은 가까이 밀집되어 있으며, **벡터로 표현된 단어를 임베딩이라고 함**
- 최근 NLP 방법들은 모두 임베딩을 사용해서 단어의 의미를 표현<br>
<br>

#### 임베딩 사용의 이유
- 임베딩을 사용하지 않는 경우 학습 데이터와 테스트 데이터에 동일한 단어가 나타나지 않으면 예측 결과가 좋지 못함<br>
  각 속성은 한 단어의 존재 유무를 나타내기 때문<br>
- 그러나 임베딩을 사용하는 경우 테스트데이터에 새로운 단어가 나타나도 학습데이터에 존재하는 유사한 단어를 통해<br>
  개략적인 단어들의 분포를 알 수 있고, 학습한 내용이 유효(각 속성은 단어 임베딩 벡터)<br>
<br>

#### 임베딩 종류
- 희소벡터: tf-idf, Vector propagation(검색엔진을 위한 질의어, 문서 표현)<br>
  - 벡터의 길이가 길고($\vert \mathbf{V} \vert = 20000 ~ 50000$), 희소성 특징을 띰(sparse, 대부분의 원소가 0)<br>
<br>
- **밀집벡터(대부분 딥러닝 모델들이 적용)**(Word2vec, Glove)<br>
  - 벡터의 길이가 짧고($\vert \mathbf{V} \vert = 50 ~ 1000$), 밀집성 특징을 띰(dense, 대부분의 원소가 0이 아님)
  - 즉, 더 적은 개수의 학습 파라미터가 사용이 됨
  - 더 나은 일반화 능력을 가지고, 동의어, 유사어를 더 잘 표현함

### 임베딩 벡터들의 유사도
- 각 문서는 단어들의 벡터로 표현(문서에 나타난 단어들의 빈도 수를 계산하여 벡터로 표현)(Term-document행렬)
- 주변 단어들의 빈도를 벡터로 표현(Word-word행렬)
- 각 벡터를 시각화를 하면 서로 연관성이 있는 두 벡터(문서, 단어)가 **비슷한 방향**을 향하고 있는 것을 확인할 수 있음<br>
<br>
- 두 임베딩 벡터에 의해 발생하는 각도가 0에 가까울 수록 두 단어 혹은 문서가 연관성이 높다는 것을 알 수 있음<br>
  $\displaystyle cos(\theta) = \frac{\mathbf{v} \cdot \mathbf{w}}{\vert \mathbf{v} \vert \vert \mathbf{W} \vert} = \frac{\sum_{i=1}^N v_i w_i}{\sqrt{\sum_{i=1}^N {v_i}^2}\sqrt{\sum_{i=1}^N {w_i}^2}}$

### TF-IDF
- 단어의 빈도수를 그대로 사용하게 되면 자주 나타나는 단어들(the, a, it, ...)은 의미를 구별하는데 크게 도움이 되지 않음
- 이를 해결하기 위해 문서 내의 단어에 새로운 가중치 값을 계산(문서 $d$, 단어 $t$)<br>
  $w_{t, d} = \text{tf}_{t, d} \times \text{idf}_t$
- TF-IDF을 적용하게 되면 높은 빈도수의 값은 0에 가까운 값으로 변환<br>
<br>
- 단어의 빈도수(Term frequency, tf)는 스무싱을 하여 적용<br>
  $\text{tf}_{t, d} = count(t, d) \rightarrow \log_{10}(count(t, d) + 1)$<br>
  <br>
- 단어를 포함하는 문서들의 개수(Document frequency, df)로 idf는 df의 역수(마찬가지로 스무싱하여 적용)<br>
  이 때 $N$은 전체 문서의 개수<br>
  $\displaystyle \text{idf}_t = \log_{10}\Big(\frac{N}{\text{df}_t}\Big)$


### Word2vec
- 주어진 단어 w를 인접한 단어들의 빈도수로 나타내는 대신 주변 단어를 예측하는 분류기를 학습<br>
  **단어 w가 주어졌을 때 주변 단어 c가 나타날 확률?**<br>
- 우리의 관심은 예측모델의 최종예측값이 아닌 모델 내 단어 w의 가중치벡터<br>
- 모델을 학습하기 위한 목표값이 이미 데이터 내에 존재하기 때문에 사람이 수동으로 레이블을 생성할 필요가 없음(Self-supervision)<br>
<br>

### Skip-Gram
- Skip-gram 모델은 한 단어가 주어졌을 때 그 주변 단어를 예측할 확률을 최대화하는 것이 목표
- 단어들의 시퀀스 $w_1, \cdots, w_T$가 주어졌을 때 다음 확률을 최대화<br>
  $\displaystyle \prod_{t=1}^{T} \prod_{-m \le j \le m, j \ne 0} p(w_{t+j} \mid w_t)$<br>
<br>

- 파라미터를 명시화해서 우도함수로 표현하면<br>
  $\displaystyle L(\theta) = \prod_{t=1}^{T}\prod_{-m \le j \le m, j \ne 0} p(w_{t+j} \mid w_t; \theta)$<br>
  파라미터 $\theta = \{W, C\}$는 두 개의 임베딩 행렬 $W$와 $C$를 포함<br>
  $W$를 목표(또는 입력) 임베딩 행렬, $C$를 상황(또는 출력) 임베딩 행렬<br>
<br>

- 하나의 목표 단어 $w$와 상황단어 $c$가 주어졌을 때 skip-gram 모델의 확률모델은 다음과 같이 가정(softmax 함수와 매우 유사)<br>
  $\displaystyle p(c \mid w; \theta) = \frac{\exp(u_c^Tv_w)}{\sum_c^{\prime} \exp(u_{c^{\prime}}^Tv_w)}$<br>
<br>
  
- $x_w$를 단어 $w$에 대한 one-hot 벡터라고 하면, $v_w$와 $u_c$는 다음과 같이 정의<br>
  $v_w = (x_w^TW)^T, \quad u_c = (x_c^TC)^T$

- 그러나 위 모델의 문제점은 **분모의 계산량이 너무 많음**($\vert \mathbf{V} \vert d$와 비례)<br>
<br>

#### 해결방안
- Noise-constrastive estimation(NCE)<br>
  Normalization constant(위 식에서 분모)를 하나의 파라미터로 학습<br>
  이진 분류문제에 해당하는 새로운 목표함수를 최적화<br>
- 이를 조금 더 단순화시키면 negative sampling이 되며 **Word2vec은 negative sampling**을 사용  

### 느낀점
여러가지 단어 임베딩을 하는 방식에 대해 배웠다. 수식이 조금 복잡하긴하지만 핵심 아이디어는 크게 어렵지는 않았다.<br>
구현을 잘하는 것만 남았는데 과제를 통해서 skip-gram을 구현해보고 이해해보도록 하겠다.<br>
negative sampling에 관한 내용은 과제를 하면서 조금 더 세부적으로 기록하도록 하겠다.
