---
title: 프로그래머스 인공지능 데브코스 28일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 6주차(22.1.3.(월) ~ 22.1.7.(금)) 
  인공지능과 기계학습 소개
  기계학습과 수학 리뷰
  E2E와 선형대수
  weekly mission을 통한 실습
tags:
- 선형독립
- 선형종속
- 계수(Rank)
- 치역
- 열공간
- 행공간
- 영공간(해공간)
- 직교여공간
- 정사영(투영)
- 이차형식
- 행렬미분
- 대칭행렬
- 최소제곱법
- 고유값
- 최적화문제
- Autoencoder
- 인코더(encoder)
- 인코딩 함수
- 디코더(decoder)
- 디코딩 함수
- 주성분 분석(PCA)

use_math: True
---
# 28일차(기계학습과 수학)

6주차(22.1.10.(월) ~ 22.1.17.(금)): 인공지능과 기계학습 소개
* 인공지능과 기계학습 소개
* 기계학습과 수학 리뷰
* E2E와 선형 대수
* weekly mission을 통한 실습

### 선형독립과 계수(Rank)
- 선형 독립: 어떤 벡터들의 집합 $\{x_1, x_2, \cdots, x_n\} \subset \mathbb{R}^m$에 속해있는 어떤 벡터도 나머지 벡터들의 선형조합으로 나타낼 수 없을 경우
- 선형 종속: 어떤 벡터가 나머지 벡터들의 선형조합으로 나타낼 수 있을 경우
- 행렬의 계수: 행렬 내 일차독립인 행 혹은 열벡터들의 최대 개수이고 $rank(A)$로 나타냄


```python
import numpy as np
import numpy.linalg as LA

A = np.array([
        [1, 2, 7],
        [2, 1, 5],
        [3, -2, -3],
    ])
A[:]
```




    array([[ 1,  2,  7],
           [ 2,  1,  5],
           [ 3, -2, -3]])


---


```python
A[:, 2] == A[:, 0] + 3 * A[:, 1]
```




    array([ True,  True,  True])


---

```python
LA.matrix_rank(A)
```




    2

<br>

위의 식에서 (1열) + 3 x (2열) = (3열)이 성립<br>
따라서 3열은 나머지 열(벡터)들의 선형조합으로 만들어지는 것을 알 수 있음<br>
따라서 행렬 $A$의 계수는 2 $\rightarrow$ 즉, 일차 독립인 행 혹은 열벡터는 2개<br>
<br>

#### 성질
- $A, C \in \mathbb{R}^{m\times n}$, $B \in \mathbb{R}^{n\times p}$에 대해서 다음과 같은 성질이 있음
- $rank(A) \le \min(m, n)$
- $rank(A) = rank(A^T)$<br>
<br>

---

### 치역(Range), 영공간(Nullspace)

<br>

#### 벡터의 집합($\{x_1, x_2, \cdots, x_n\}$)에 대한 생성(span)
<center> $span(\{x_1, x_2, \cdots, x_n\}) = \left\{  v: v = \displaystyle\sum_{i=1}^{n}\alpha_i x_i, \alpha_i \in \mathbb{R} \right\}$</center>

#### 치역(Range)
- 행렬 $A \in \mathbb{R}^{m\times n}$의 치역 $\mathcal{R}(A)$는 $A$의 모든 열들에 대한 생성(span)<br>
<center> $ \mathcal{R}(A) = \{v \in \mathbb{R}^m : v = Ax, x \in \mathbb{R}^n\}$</center>

#### 영공간(Nullspace)
- 행렬 $A \in \mathbb{R}^{m\times n}$의 영공간 $\mathcal{N}(A)$는 $A$와 곱해졌을 때 영벡터가 되는 모든 벡터들의 집합<br>
<center> $ \mathcal{N}(A) = \{x \in \mathbb{R}^n : Ax = 0\}$</center>

#### 직교여공간
- 어떤 벡터 $x$가 $\mathbb{R}^n$의 부분공간 $W$의 모든 벡터와 직교하면, 벡터 $x$는 $W$에 직교
- 이 때 $W$에 직교하는 모든 벡터들의 집합을 $W$의 직교여공간 이라고 함(기호로 $W^{\perp}$로 나타냄)<br>
<br>

### 정사영
- $V \subseteq \mathbb{R}^n$, $V$는 부분공간에서
- $\forall{x} \in \mathbb{R}^n, x = x_1 + x_2$로 유일하게 표현(단, $x_1 \in V, x_2 \in V^\perp$)
- 즉, 어떤 벡터는 해당 공간(혹은 평면)에서 직교하는 서로 다른 두 벡터에 의해 표현이 가능<br>
<br>
- 이를 증명하기위해 $v = \{0\}$일 때와 $v \ne \{0\}$일 때를 증명하면 됨
- 그 중 $v \ne \{0\}$일 때는 $T = (v_1, v_2, \cdots, v_m)$(서로 다른 기저), 즉, $x_1 = a_1v_1 + a_2v_2 + \cdots + a_mv_m = Ta \in V$<br>
- 이 때 $x_2 = x - x_1 = x - Ta \in V^\perp$를 만족하는 $a$가 유일하게 존재하는 것만 밝히면 됨<br>
<br>
$\forall{y} \in \mathbb{R}^m$에서 $y(x-Ta) = 0 \rightarrow Tb(x-Ta) = 0 \Leftrightarrow bT^T(X-Ta) = 0$<br>
<br>
즉, $T^T(X-Ta) = 0$을 만족하는 $a$가 유일한 것을 보이기 위해 식을 전개하면<br>
$ T^TX - T^TTa = 0 \Leftrightarrow T^Tx = T^TTa$<br>
<br>
그런데 $T$는 서로 다른 기저로 이루어져있으므로 $T^T$는 full rank이므로 $T^T$는 가역행렬<br>
따라서 $a = (T^TT)^{-1}T^Tx$<br><br>

- 위의 증명에 따라 $\mathcal{R}(A)$(위에서는 $V$)에 투영시킨 벡터 $y$는 다음과 같이 표현 가능<br>
  $Proj(y; A) = argmin \left\Vert v - y\right\Vert_2 = A(A^TA)^{-1}A^Ty$ (이 식은 위에서 $x_1 = Ta$와 대응)

---

### 이차형식
- 변수 $x, y$에 관한 이차형식은 다음과 같이 표현<br>
  <center>$ ax^2 + 2bxy +cy^2 = \begin{pmatrix} x&y \end{pmatrix} \begin{pmatrix} a&b \\ b&c \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix}$</center><br>
<br>

- 마찬가지로 변수 $x, y, z$에 관한 이차형식은 다음과 같이 표현<br>
  <center>$ ax^2 + by^2 + cz^2 +2dxy + 2exz + 2fyz = \begin{pmatrix}x&y&z\end{pmatrix} \begin{pmatrix} a&d&e\\d&b&f\\e&f&c \end{pmatrix} \begin{pmatrix} x\\y\\z  \end{pmatrix}$</center><br>
  
- 일반화를 해보면 대칭행렬 $A$에 대해 다음과 같이 표현이 가능<br>
  $x^TAx = (x^TAx)T = x^TA^Tx = x^T\left(\frac{1}{2}A + \frac{1}{2}A^T\right)x$

### 행렬미분
- 행렬 $A \in \mathbb{R}^{m\times n}$를 입력으로 받아서 실수값을 돌려주는 함수 $f: \mathbb{R}^{m\times n} \rightarrow \mathbb{R}$이 있을 때 $f$의 그레디언트는 다음과 같음<br>
<center>$$ \nabla_Af(A) \in \mathbb{R}^{m \times n} = \begin{bmatrix} \frac{\partial{f(A)}}{\partial{A_{11}}}&\frac{\partial{f(A)}}{\partial{A_{12}}}& \cdots&\frac{\partial{f(A)}}{\partial{A_{1n}}}\\ \frac{\partial{f(A)}}{\partial{A_{21}}}& \ddots & & \vdots\\ \vdots &&\ddots& \vdots \\ \frac{\partial{f(A)}}{\partial{A_{m1}}}& \cdots & \cdots & \frac{\partial{f(A)}}{\partial{A_{mn}}}\end{bmatrix}$$</center>
<br>
<center>$(\nabla_Af(A))_{ij} = \frac{\partial{f(A)}}{\partial{A_{ij}}}$</center><br>
<br>
- 특별히 $A$가 벡터 $x \in \mathbb{R}^n$인 경우<br>
<center> $$\nabla_xf(x) = \begin{bmatrix} \frac{\partial{f(x)}}{\partial{x_1}}\\ \frac{\partial{f(x)}}{\partial{x_2}}\\ \vdots \\ \frac{\partial{f(x)}}{\partial{x_n}} \end{bmatrix}$$ </center><br>
<br>
- 중요한 공식<br>
  - $\nabla_x b^Tx = b$
  - $\nabla_x x^TAx = 2Ax$
  - $\nabla_x^2 x^TAx =2A$

### 최소제곱법

- 행렬 $A \in \mathbb{R}^{m\times n}$($A$는 full rank)와 벡터 $b \in \mathbb{R}^n$가 주어지고, $b \notin \mathcal{R}(A)$인 경우, $Ax = b$를 만족하는 벡터 $x \in \mathbb{R}^n$을 찾을 수 없음
- 대신 $Ax 가 b$와 최대한 가까워지는 $x$, 즉, $\left\Vert Ax - b \right\Vert_2^2$를 최소화시키는 $x$를 찾는 문제를 고려할 수 있음
- $\left\Vert x \right\Vert_2^2 = x^Tx$ 이므로 위의 식은 다음과 같이 정리가 가능<br>

  $\left\Vert Ax - b \right\Vert_2^2 = (Ax - b)^T(Ax - b) = x^TA^TAx - 2b^TAx + b^Tb$<br>
  <br>
  $\nabla_x(x^TA^TAX - 2b^TAx +b^Tb) = \nabla_xX^TA^TAx - \nabla_x2b^TAx + \nabla_xb^Tb = 2A^TAx -2A^Tb = 0$에서 $x$는 다음과 같음<br>
  <br>
  $\therefore x = (A^TA)^{-1}A^Tb$

### 고유값, 고유벡터와 대칭행렬

- 대칭행렬 $A \in \mathbb{S}^n$가 가지는 놀라운 성질<br>
  - $A$의 모든 고유값들은 실수
  - $A$의 고유벡터들은 직교성을 가짐<br>
<br>
- 따라서 대칭행렬 $A = U\Lambda U^T $($U$는 $A$의 고유벡터들로 이루어진 행렬)로 나타낼 수 있음<br>
<br>
- $y = U^Tx$일 때 $x^TAx = x^TU\Lambda U^Tx = y^T\Lambda y = \displaystyle\sum_{i = 1}^{n}\lambda_iy_i^2$
- $y_i^2$가 양수이므로 위 식의 부호는 $\lambda_i$ 값들에 의해 결정

### 고유값과 최적화문제
- 다음 형태의 최적화문제를 행렬미분을 사용해서 풀면 고유값이 최적해가 되는 것을 보일 수 있음<br>
  $\max x^TAx \quad$ subject to $\left\Vert x \right\Vert_2^2 = 1$<br>
<br>
- 제약조건이 있는 최소화문제는 Lagrangian을 사용해서 해결<br>
  $\mathcal{L}(x, \lambda) = x^TAx - \lambda x^Tx$<br>
  $ \nabla_x\mathcal{L}(x, \lambda) = \nabla_x(x^TAx - \lambda x^Tx) = 2A^Tx - 2\lambda x = 0$<br>
<br>
- 따라서 최적해 $x는 Ax = \lambda x$를 만족해야 하므로 $A$의 고유벡터만이 최적해가 될 수 있음
- 고유벡터 $u_i$는(제약조건 $\left\Vert x \right\Vert_2^2 = 1$ 주목)<br>
  $u_i^TAu_i = \displaystyle\sum_{j = 1}^{n} \lambda_jy_j^2 = \lambda_i, \quad y_j = \begin{cases} 0 \quad (\mbox{if} \ i \ne j) \\ 1 \quad (\mbox{if} \ i = j) \end{cases}$<br>
  을 만족하므로($y = U^Tu_i$), 최적해는 가장 큰 고유값에 해당하는 고유벡터

---

### Autoencoder
<img src = "https://pic2.zhimg.com/v2-86cbd5045efdbebec7961c1cce619fcd_r.jpg">

- 인코딩: 다차원 입력을 점차적으로 차원으로 줄여나가는 과정
- 디코딩: 줄여진 차원의 데이터를 다시 원래 입력 차원으로 늘리는 과정<br>
<br>
- 컴팩트하게 데이터를 많이 보존성있게 저장하기 위해 위의 작업이 필요
- 입력에서 잡음이 있을 때 인코딩과 디코딩의 과정을 거치게 되면서 잡음이 사라지는 경우도 있음
- 여러 개의 layer를 쌓아 사용하기 때문에 주로 딥러닝 모델에서 많이 사용됨<br>
<br>
- 제일 가운데 빨간색에 있는 코드에 데이터가 가장 많이 압축되어 있음
- 가운데에 있는 데이터를 분석하기위해 주성분 분석(PCA)를 적용

### 주성분 분석(PCA)
- PCA를 가장 간단한 형태의 autoencoder로 생각할 수 있음<br>
<br>
- $m$개의 점들 $\{x_1, \cdots, x_m\}$, $x_i \in \mathbb{R}^n$
- 각각의 점들을 $l$차원의 공간으로 투영시키는 함수 $f(x) = c \in \mathbb{R}^l$
- $f(x)$를 다시 $n$차원의 공간으로 회복하는 함수 $g(c)$
- $f(x)$를 인코딩 함수, $g(c)$를 디코딩 함수<br>
<br>
- **인코딩과 디코딩의 목적: $x \approx g(f(x))$**<br>
<br><br>

#### 디코딩 함수
- 함수 $g(c) = Dc, \quad D \in \mathbb{R}^{n \times l}$
- $D$는 열들이 정규화되어 있고 서로 직교하는 경우로 한정<br>
<br><br>

#### 인코딩 함수
- 위의 디코딩 함수에 대해 최적의 인코딩 함수 구하기
- $f(x)^\ast = \mbox{argmin} \int \left\Vert x - g(f(x)) \right\Vert_2 ^2dx$<br>
<br>
- 변분법의 방법(오일러-라그랑주 방정식)으로 해결 가능
- $\nabla_f \left\Vert x - g(f(x))\right\Vert_2^2dx = 0$을 $f$에 관해 풀면 됨<br>
<br>
- $f(x) = c$라고 하면<br>
<br>
$\left\Vert x - g(c) \right\Vert = (x - g(c))^T(x - g(c)) = x^Tx - 2x^TDc + c^Tc$<br>
<br>
$\nabla_c(x^Tx - 2x^TDc +c^Tc) = 0 \quad \Leftrightarrow \quad -2D^Tx + 2c = 0 \quad \Leftrightarrow  \quad c = f(x) = D^Tx$<br>
<br>

#### 최적의 D 찾기
- 입력값 $x$와 출력값 $g(f(x))$ 사이의 거리가 최소화되는 $D$ 찾기
- 에러 행렬은 입력 행렬과 출력 행렬의 차이 $ E = X - R$<br>
<br>
- 최적의 $D$는 다음과 같음<br>
  $ D^\ast = \mbox{argmin}\left\Vert E \right\Vert_F^2$ (이 때, $D^TD = I$)<br>
<br>
- 출력 행렬에서 각각의 열벡터 $g(f(x_m)) = DD^Tx_m$가 됨
- 즉, 행벡터는 $g(f(x_m))^T = x_{m}^TDD^T$이기 때문에 출력행렬 $R = XDD^T$ 성립
- $ D^\ast = \mbox{argmin}\left\Vert E \right\Vert_F^2 = \mbox{argmin}\left\Vert X - XDD^T \right\Vert_F^2 = \mbox{argmin tr}\left((X - XDD^T)^T(X-XDD^T)\right) = \mbox{argmin} -\mbox{tr}(D^TX^TXD)$<br>
<br>
- 따라서 $ \mbox{argmin} -\mbox{tr}(D^TX^TXD) = \mbox{argmax tr}(D^TX^TXD) = \mbox{argmax} \displaystyle\sum_{i = 1}^{l}d_i^TX^TXd_i$<br>
- $d_i^Td_i = 1$이므로 벡터 $d_1, \cdots, d_l$이 $X^TX$의 가장 큰 고유값 $l$개와 대응되는 고유벡터들일 때 최적의 $D$성립

### 느낀점
초반에는 굉장히 쉬웠고 하도 많이 들었던 내용이라 직접 기록을 하지 않은 내용이 굉장히 많아서 쉽게 생각했는데<br>
어려웠다... 갑작스럽게 난이도가 훅 올라간 느낌이 있는데 전반적으로 강의가 기계학습 내용을 어느정도 진행한 후 이루어진 강의인 것 같아서<br>
현재로서는 이런 것이 있구나 하고 넘어가는게 맞는 것 같다. 그러나 나중에 기계학습에 관련된 내용이 나오면서 위의 내용들이<br>
분명 언급될 것이기 때문에 해당 내용과 병행해서 혹은 그 수업 이전에 위 내용을 숙지하고 앞으로의 내용을 들어야할 것 같다
