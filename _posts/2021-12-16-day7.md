---
title: 프로그래머스 인공지능 데브코스 7일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 2주차(21.12.13.(월) ~ 21.12.17.(금) 인공지능 수학
  선형대수
  자료의 정리
  확률과 확률분포
  추정, 검정, 엔트로피
tags:

- 투영
- 정사영
- 내적
- 직교행렬
- 정규직교행렬
- QR 분해
- LR 분해
- 그람-슈미트 과정
- 병렬처리
- 특이값 분해(SVD)
- 주성분 분석(PCA)
- 벡터공간
- 최소제곱법
- 통계학
- 모집단
- 표본
- 모수
- 도수
- 상대도수
- 측도
- 평균
- 중앙값
- 표본분산
- 모분산
- 표본표준편차
- 모표준편차
- 범위
- 사분위수
- z-score

use_math: true
---

# 7일차(직교행렬, QR분해, 특이값 분해, 주성분분석, 최소제곱법, 통계기초)

2주차(21.12.13.(월) ~ 21.12.17.(금)): 인공지능 수학

* 선형대수, 자료의 정리, 확률과 확률분포, 추정, 검정, 엔트로피 강의
* Jupyter Notebook, numpy를 이용한 실습

### 벡터의 투영

* 두 벡터 $u, a$에 대하여 벡터 $u$를 $a$에 투영한 벡터를 $proj_au$라고 표현

<center>$\displaystyle proj_au = \frac{u\cdot{a}}{\lVert{a}\rVert^2}a$ \big_l\( \frac{u\cdot{a}}{\lVert{a}\rVert^2}\big_r\)$는 기저 $a$의 좌표 값)</center>

* 벡터 $u$를 $a$에 투영하고 남은 벡터를 보완벡터라고 하고 $u - proj_au$가 됨
* 직관적으로 이해를 쉽게하기 위해서 $u$는 직사각형의 대각선, $proj_au, a$를 직사각형의 가로와 세로라고 생각하면 됨
* 따라서 $proj_au, a$는 서로 직교

<center>$proj_au \perp a$</center>

### 직교행렬

* 주어진 행렬의 열벡터가 각각 모두 직교하는 행렬
* 다음과 같은 행렬은 모두 직교행렬이라고 볼 수 있다.

<center> $\begin{bmatrix} -1&3\\3&1 \end{bmatrix} \begin{bmatrix} 1&2&-4\\1&1&7\\3&-1&1 \end{bmatrix}$</center>

### 정규직교행렬

* 직교행혈의 모든 열벡터의 크기가 1인 행렬
* 다음과 같은 행렬은 모두 정규직교행렬이라고 볼 수 있다.

<center> $\begin{bmatrix} -\frac{1}{\sqrt{10}}&\frac{3}{\sqrt{10}}\\\frac{3}{\sqrt{10}}&\frac{1}{\sqrt{10}} \end{bmatrix} \begin{bmatrix} \frac{1}{\sqrt{11}}&\frac{2}{\sqrt{6}}&\frac{4}{\sqrt{66}}\\ \frac{1}{\sqrt{11}}&\frac{1}{\sqrt{6}}&\frac{7}{\sqrt{66}}\\ \frac{3}{\sqrt{11}}&-\frac{2}{\sqrt{6}}&\frac{1}{\sqrt{66}} \end{bmatrix}$</center>

### 선형시스템에서의 직교행렬

* 선형시스템 $Ax = b$에서 행렬$A$가 직교행렬이라면 역행렬$A^{-1}$를 구하지 않고 해$x$를 구할 수 있음
* 벡터의 투영을 이용해서 해를 구할 수 있음

<center> $\displaystyle x_i = \frac{b\cdot{a_i}}{\lVert{a_i^2}\rVert}$ </center><br>

* $x_i, x_j$의 계산은 **서로 독립적이기 때문에 병렬처리가 가능하다**
* 위의 식에서 $\lVert{a_i^2}\rVert = 1$이라면, 즉, 행렬$A$가 정규직교행렬이라면 해는 $b$벡터와 $a_i$벡터의 내적으로 쉽게 구할 수 있음

<center> $x_i = b\cdot{a_i}$ </center><br>

### QR 분해

* QR분해는 선형시스템 행렬 $A$를 정규직교행렬$Q$와 상삼각행렬$R$로 행렬분해하는 것을 의미
* 선형스템을 QR분해를 했을 때의 장점은 해$x$를 내적계산과 후방대입법을 통하여 쉽게 계산이 가능하다는 장점이 있음

<center> $Ax = b \Leftrightarrow Q(Rx) = b \Leftrightarrow Qy = b$ </center><br>

* QR분해는 그람-슈미트 직교화 과정을 코드화한 것으로 행렬$Q$는 행렬$A$에서 정규직교성을 추출한 행렬이고 행렬$R$은 정규직교성을 추출하고 남은 상삼각행렬
* LU분해와 마찬가지로 b가 업데이트될 때마다 빠르게 계산이 가능하지만<br>
  QR분해시 $Q$행렬은 LU분해시 $L$보다 0이 아닌 원소가 많기 때문에 메모리의 사용량이 더 많다는 단점이 있음
* 그러나 QR분해는 LU분해와 다르게 병렬처리가 가능하다는 점이 있음($y$ 계산 시에만 병렬처리 가능)

### 그람-슈미트 직교화 과정

* 직교 기저를 찾기위해 적용
* 원리는 임의의 기저벡터 $v_i$에 대해 $v_1, v_2, \cdots, v_{i - 1}$성분을 제거하여 구해냄
* 수식으로 표현하면 다음과 같음<br>

임의의 벡터공간 $w$에 대해 $B = {v_1, v_2, \cdots, v_k}$일 때 직교기저로 변환한 $B' = {w_1, w_2, \cdots, w_k}$<br>
$ w_1 = v_1$<br>
$ w_2 = v_2 - proj_{w_1}v_2$<br>
$ w_3 = v_3 - proj_{w_2}v_3 - proj_{w_1}v_3$<br>
순차적으로 계산하게되면 $w_k$는 다음과 같이 정의가능
$ w_k = v_k - \displaystyle\sum_{i = 1}^{k - 1}proj_{w_i}v_k$


```python
# QR분해
# 분해 시 시간복잡도 O(n**3)
# 계산 시 병렬처리가 가능하다는 장점

# 단위벡터로 변환
# 만약 어떤 벡터의 길이가 0이면 프로그램 종료
def trans_unit_vector(vector):
    l = (sum([v * v for v in vector])) ** 0.5
    if not l:
        print("ZeroDivisionError")
        exit(0)
    return [v / l for v in vector]


# 벡터 내적 구현
def inner_product(vector_1, vector_2):
    return sum([v1 * v2 for v1, v2 in zip(vector_1, vector_2)])


# proj 구현
def proj(w_vector, v_vector):
    weight = inner_product(w_vector, v_vector)
    return [weight * w for w in w_vector]


# 벡터 빼기 구현
def vector_minus(vector_1, vector_2):
    return [v_1 - v_2 for v_1, v_2 in zip(vector_1, vector_2)]


# 행렬 곱셈 구현
def mul_matrix(M1, M2):
    M2 = transpose(M2)
    return [[inner_product(M1_row, M2_row) for M2_row in M2] for M1_row in M1]


# 전치행렬 구현
def transpose(M):
    return [list(col) for col in zip(*M)]

# 그람 슈미트 과정
def Gram_Schumidt_Process(B):
    W = []
    for v in B:
        for w in W:
            # v - proj_w{v}
            v = vector_minus(v, proj(w, v))
        W.append(trans_unit_vector(v))
    return W


# Q행렬 구하기
def extract_orthonormal_matrix(Matrix):
    B = [list(col) for col in zip(*Matrix)]
    B_prime = Gram_Schumidt_Process(B)
    return [list(row) for row in zip(*B_prime)]


# 입력
D = int(input("Enter the Dimension: "))
A = [list(map(float, input().split())) for _ in range(D)]

Q = extract_orthonormal_matrix(A)

print('Matrix Q:')
for q in Q:
    for n in q:
        print(f"{n:.4f}", end = ' ')
    print()
print()

# R 행렬 구하기(A = QR 양변에 Q_transpose 곱해주기)
R = mul_matrix(transpose(Q), A)

print('Matrix R:')
for r in R:
    for n in r:
        print(f"{n:.4f}", end = ' ')
    print()
print()


print('b = ')
b = [[float(input())] for _ in range(D)]
for bb in b:
    print(bb)
print()

# y = QR
# Q^TA = Rx = y
print('y = ')
y = mul_matrix(transpose(Q), b)
for yy in y:
    print(f"{yy[0]:.4f}")
print()

# 후방대입법으로 해구하기
x = []
for r in range(1, D + 1):
    for c in range(1, r):
        y[-r][0] -= R[-r][-c] * x[c - 1]
    x.append(y[-r][0] / R[-r][-r])

print('x = ')
for xx in x[::-1]:
    print(f"{xx:.4f}", end = ' ')
```

```
Enter the Dimension: 3
12 -51 4
6 167 -68
-4 24 -41
Matrix Q:
0.8571 -0.3943 -0.3314 
0.4286 0.9029 0.0343 
-0.2857 0.1714 -0.9429 

Matrix R:
14.0000 21.0000 -14.0000 
-0.0000 175.0000 -70.0000 
-0.0000 -0.0000 35.0000 

b = 
38
-296
13
[38.0]
[-296.0]
[13.0]

y = 
-98.0000
-280.0000
-35.0000

x = 
-5.0000 -2.0000 -1.0000 
```

### 특이값 분해(SVD)

* 임의의 행렬 $m x n$에 대하여 적용가능한 행렬분해
* 행렬 분해 시 $A_{m x n} = U_{m x m}D_{m x n}V_{n x n}^T$와 같이 표현이 가능
* $U$: m차원 회전행렬(정규직교행렬) / $D$: n차원 확대축소 / $V$: n차원 회전행렬(정규직교행렬)
* $U$, $D$, $V$는 **행렬 $A$의 열벡터가 어느 방향으로 응집성이 강한지 분석가능**
* 주로 영상처리, 음성처리하는데 적용되며, 데이터의 중요성이 높은 부분만 골라내는데 적용할 수 있음

### 주성분 분석(PCA)

* 데이터의 공분산행렬에 대한 고유값 분해에 기반을 둔 직교행렬
* 예를 들어 2차원 평면에 수많은 데이터가 놓여져있을 때 응집성이 강한 하나의 직선으로 데이터들을 표기할 때 주로사용
* $K$개의 $n$차원 데이터$\{x_i\}_{i=1}^K$가 있을 때, 데이터 중심$m$과 공분산행렬$C$는 다음과 같음

<center>$\displaystyle m = \frac{1}{K}\sum_{i = 1}^K{x_i}$<br></center> <br>
<center>$\displaystyle C =\frac{1}{K}\sum_{i = 1}^K{(x_i - m)(x_i - m)^T}$</center><br>

* 공분산 행렬 $C$는 다음과 같음
* $W$: n차원 회전행렬(정규직교행렬), $D$: n차원 확대축소

<center> $ C = WDW^T$</center><br>

* 다음의 공분산 행렬에 대한 주성분 분석(PCA)를 해보면

<center>$\displaystyle C = \frac{1}{6} \begin{bmatrix} 28&18\\18&12 \end{bmatrix} \approx \begin{bmatrix} -0.84&0.55\\-0.55&-0.84 \end{bmatrix} \begin{bmatrix} 6.3&0\\0&0.555 \end{bmatrix} \begin{bmatrix} -0.84&-0.55\\0.55&-0.84 \end{bmatrix}$</center>

<br>

* 데이터들이 $W$의 열벡터 $w_1 = (-0.84, 0.55)$방향으로는 $6.3$만큼 응집되어 있고, $w_2 = (0.55, -0.84)$방향으로는 $0.55$만큼 응집되어 있다는 것을 알 수 있음
* 따라서 $6.3w_1 + 0.55w_2$로 데이터를 표현을 할 수 있음(차원 축소 가능)

### 벡터공간

* 모든 n차원 벡터집합 $\mathbb{R}^n$은 n차원 벡터공간이라고 부를 수 있음
* **행렬 $A$의 열벡터들의 선형조합으로 구성된 집합을 열공간($col(A)$)**이라고 함
* 따라서 어떤 선형시스템($Ax = b$)의 해가 존재한다면 $b \in col(A)$(Consistent), 해가 존재하지 않다면 $b \notin col(A)$(Inconsistent)

### 최소제곱법

* 선형시스템($Ax = b$)이 Inconsistent할 때, 즉 해가 없을 때 할 수 있는 최선은 무엇인가??
* 행렬 $A$의 열벡터들로 구성된 벡터공간$w$로 $b$를 투영($proj_wb$)시키면 달성가능한 최선의 목표
* 즉, 최소제곱법은 선형시스템이 해를 구할 수 없을 때 최선의 대안을 내놓는 기법으로<br>
  **목표 $b$와 최선의 목표 $\bar{b}$의 차이의 제곱$(b - \bar{b})^2$을 최소화**하는 의미를 갖기 때문에 최소제곱법이라고 함
* 최소제곱법으로 구한 해는 선형시스템을 만족하는 해가 아닌 **근사해**로 구하는 방법은 다음과 같음

<center>$\bar{x} = (A^TA)^{-1}b$</center>

* 선형회귀 문제에서 적용할 수 있음

### 통계학

* 통계학은 데이터의 수집, 구성, 분석, 해석, 표현에 관한 학문
* 데이터를 수집, 구성, 표현을 하는 분야는 기술통계학
* 표현된 데이터에 대한 분석, 해석, 추론, 예측을 하는 것이 추측통계학

### 모집단

* 어떤 질문이나 실험을 위해 관심의 대상이 되는 개체나 사건의 집합
* 예를 들면 전교생의 몸무게, 키 혹은 특정 집단에 속해있는 사람들의 성별 등등
* 이처럼 모집단의 수치적인 특성(평균, 분산, 표준편차)을 **모수**라고 지칭
* 모집단에서 선택된 개체나 사건의 집합을 **표본**이라고 함

### 도수

* 어떤 사건이 실험이나 관찰로부터 발생한 횟수
* 표현 방법으로는 도수분포표, 막대그래프, 히스토그램 등이 있음
* 도수를 전체 원소의 수로 나눈 것을 상대도수라고 하며, **상대도수의 합은 1**


```python
# 여러가지 통계값(측도)
import numpy as np
import statistics as statis
import scipy.stats

a = [58, 50, 65, 97, 72, 86, 56, 58, 66, 64, 90, 56, 85, 70, 72]
print('a = ', a)
print('원소의 수: ', len(a))

# 평균
# 모든 데이터의 합을 전체 원소의 수로 나눈 값
print('평균(mean): ', statis.mean(a))
print()

# 중앙값
# 평균의 경우 극단 값의 영향을 많이 받음
# 중앙값은 수들을 정렬시켰을 때 가운데에 위치하는 값
print('a를 정렬: ', sorted(a))
print('중앙값(median): ', statis.median(a))
print()

# 분산
# 편차의 제곱의 합을 자료의 수로 나눈 값
# 표본분산의 경우 (자료의 수 - 1)로 나누어줘야 함
# 표본분산, 표준편차가 모분산, 모표준편차보다 값이 더 큰 것을 알 수 있음(N - 1로 나누었기 때문)
print('분산(variance): ', statis.variance(a))
print('분산(variance): ', np.var(a, ddof = 1))
print()

print('모분산(population variance): ', statis.pvariance(a))
print('모분산(population variance): ', np.var(a))
print()

# 표준편차
# 분산의 제곱근
print('표준편차(standard deviation): ', statis.stdev(a))
print('표준편차(standard deviation): ', np.std(a, ddof = 1))
print()

print('모표준편차(population standard deviation)', statis.pstdev(a))
print('모표준편차(population standard deviation)', np.std(a))
print()

# 범위
# 최대값과 최소값의 차이
print('범위(range): ', max(a) - min(a))
print('범위(range): ', np.max(a) - np.min(a))
print()

# 사분위수
# 전체 자료를 정렬했을 때 1/4, 2/4, 3/4 위치에 있는 수
print('사분위수(quartile)')
print('제 1사분위수(Q1): ', np.quantile(a, .25))
print('제 2사분위수(Q2): ', np.quantile(a, .5))
print('제 3사분위수(Q3): ', np.quantile(a, .75))

# 꼭 0.25, 0.5, 0.75 값만 들어갈 필요는 없음
print('60% 위치에 있는 수: ', np.quantile(a, .6))
print()

# z-score
# 어떤 값이 평균으로부터 몇 표준편차 떨어져있는지 의미하는 값
print('모집단에서 z-score: ', scipy.stats.zscore(a))
print('표본집단에서 z-score: ', scipy.stats.zscore(a))
```

```
a =  [58, 50, 65, 97, 72, 86, 56, 58, 66, 64, 90, 56, 85, 70, 72]
원소의 수:  15
평균(mean):  69.66666666666667

a를 정렬:  [50, 56, 56, 58, 58, 64, 65, 66, 70, 72, 72, 85, 86, 90, 97]
중앙값(median):  66

분산(variance):  198.0952380952381
분산(variance):  198.09523809523807

모분산(population variance):  184.88888888888889
모분산(population variance):  184.88888888888886

표준편차(standard deviation):  14.074631010979937
표준편차(standard deviation):  14.074631010979935

모표준편차(population standard deviation) 13.59738536958076
모표준편차(population standard deviation) 13.597385369580758

범위(range):  47
범위(range):  47

사분위수(quartile)
제 1사분위수(Q1):  58.0
제 2사분위수(Q2):  66.0
제 3사분위수(Q3):  78.5
60% 위치에 있는 수:  70.8

모집단에서 z-score:  [-0.85800809 -1.4463565  -0.34320324  2.01019039  0.17160162  1.20121133
 -1.00509519 -0.85800809 -0.26965969 -0.41674679  1.49538553 -1.00509519
  1.12766778  0.02451452  0.17160162]
표본집단에서 z-score:  [-0.85800809 -1.4463565  -0.34320324  2.01019039  0.17160162  1.20121133
 -1.00509519 -0.85800809 -0.26965969 -0.41674679  1.49538553 -1.00509519
  1.12766778  0.02451452  0.17160162]
```


### 느낀점

통계학부분은 기초적인 부분이라 가볍게 들을 수 있었지만 QR분해과정을 직접 찾아보고 SVD의 기하학적 의미를 생각해보는 과정이 쉽지 않았다. <br>
직관적인 부분이 아니라 유튜브를 찾아가며 공부하고 또 직접 QR분해를 구현하면서 행렬 계산과정을 알게 되었다. <br>
분명 강의로 전달하고자 하는 것은 인공지능 수학에서 정말 최소한의 요구치일 것이라고 판단되기에 <br>
더 많이 찾아보고 공부를 해야겠다고 생각했다. <br>
