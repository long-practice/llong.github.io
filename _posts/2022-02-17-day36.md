---
title: 프로그래머스 인공지능 데브코스 36일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 8주차(22.1.24.(월) ~ 22.1.28.(금)) 
  선형회귀
  선형분류
  Weekly Mission
tags:
- 선형분류
- 판별함수
- 분류를 위한 최소제곱법
- 제곱합 에러함수
- 퍼셉트론 알고리즘
- 확률적 생성 모델
- 로지스틱 시그모이드 함수
- 최대우도해
- 로지스틱 회귀
- 크로스 엔트로피

use_math: True
---
# 36일차(ML_basics - 선형분류)

8주차(22.1.24.(월) ~ 22.1.28.(토)): ML_basic2
* 선형회귀
* 선형분류
* Weekly Mission

출처: 프로그래머스 인공지능 데브코스 3기 강의

### 선형분류
- 분류의 목표: 입력벡터 $\mathbf{x}$를 $K$개의 가능한 클래스 중 하나의 클래스로 할당
- 분류를 위한 결정이론<br>
  - 생성모델: $P(\mathbf{x} \mid C_k)$와 $P(C_k)$를 모델링한 다음 베이즈 정리를 사용해서 클래스의 사우확률 $P(C_k \mid \mathbf{x})$를 구함 혹은 결합확률 $P(\mathbf{x}, C_k)$를 직접 모델링
  - 식별모델: $P(C_k \mid \mathbf{x})$를 직접적으로 모델링
- 판별함수: 입력 $\mathbf{x}$을 클래스로 할당하는 판별함수를 찾기, 확률값은 계산하지 않음

### 판별함수
#### 두 개의 클래스의 경우
- 선형판별함수는 $y(\mathbf{x}) = \mathbf{w}^T\mathbf{x} + w_0$이고, $\mathbf{w}^T$는 가중치 벡터, $w_0$는 바이어스
- $y(\mathbf{x}) \ge 0$인 경우 이를 $C_1$으로 판별하고 아닌 경우 $C_2$로 판별<br>
<br>
- 이에 따라 결정 경계는 $y(\mathbf{x}) = 0$으로 결정
- $D - 1$차원의 초평면이 결정($\mathbf{x}$가 $D$차원의 입력벡터일 때)<br>
<br>

- 결정 경계면 위의 임의의 두 점 $\mathbf{x}\_{A}$와 $\mathbf{x}\_{B}$
- 즉, $y(\mathbf{x}\_A) = y(\mathbf{x}\_B) = 0$이고, $\mathbf{w}^T(\mathbf{x}\_A - \mathbf{x}\_B) = 0$이라면 $\mathbf{w}$는 결정 경계면에 수직<br>

![1.png](attachment:1.png)

<br>

- 벡터 $w\_{\perp}$를 원점에서 결정 경계면에 대한 사영일 때 즉, $\displaystyle r \frac{\mathbf{w}}{\|\mathbf{w}\|}=\mathbf{w}\_{\perp}$일 때<br>
  $y\left(\mathbf{w}\_{\perp}\right)=0$, &emsp; $\mathbf{w}^{T} \mathbf{w}\_{\perp}+w_{0}=0$, &emsp; $\frac{\mathbf{w}^{T} \mathbf{w}}{\|\mathbf{w}\|} r+w_{0}=0$, &emsp; $\|\mathbf{w}\| r+w_{0}=0$<br>
  따라서 $r$은 다음과 같이 구해짐 $\displaystyle r=-\frac{w_{0}}{\|\mathbf{w}\|}$<br>
<br>
- $w_0$는 결정 경계면의 위치를 결정<br>
  - $w_0 < 0$이면 결정 경계면은 원점으로부터 $\mathbf{w}$가 향하는 방향
  - $w_0 > 0$이면 결정 경계면은 원점으로부터 $\mathbf{w}$의 반대 방향<br>
<br>

- $y(\mathbf{x})$값은 $\mathbf{x}$와 결정 경계면 사이의 부호화된 거리와 비례
- 임의의 한 점 $\mathbf{x}$의 결정 경계면에 대한 사영을 $\mathbf{x}\_{\perp}$<br>
  $\mathbf{x} = \mathbf{x}\_{\perp} + r\frac{w_{0}}{\|\mathbf{w}\|}$ 양변에 $\mathbf{W}^T$를 곱하고 $w_0$를 더하면<br>
  $\displaystyle r = \frac{y(\mathbf{x})}{\|\mathbf{w}\|}$로 유도 가능<br>
<br>
- $y(\mathbf{x}) > 0$이면 $\mathbf{x}$는 결정 경계면 기준 $\mathbf{w}$가 향하는 방향
- $y(\mathbf{x}) < 0$이면 $\mathbf{x}$는 결정 경계면 기준 $-\mathbf{w}$가 향하는 방향
- $y(\mathbf{x})$의 절대값이 클 수록 더 멀리 떨어져 있음<br>
<br>

#### 다수의 클래스의 경우
  판별함수: $y_{k}(\mathbf{x})=\mathbf{w}\_{k}^{T} \mathbf{x}+w_{k 0}, \quad (k = 1, \cdots, K)$<br>
- 위와 같은 판별함수는 $j \ne k$일 때 $y_k(\mathbf{x}) > y_j(\mathbf{x})$를 만족하면 $\mathbf{x}$를 클래스 $C_k$로 판별

### 분류를 위한 최소제곱법
- 위의 판별함수를 다음과 같이 간편하게 표현<br>
  $y(\mathbf{x})=\widetilde{\mathbf{W}}^{T} \widetilde{\mathbf{x}}$<br> 이 때, $\widetilde{\mathbf{W}}$의 $k$번째 열은 $\widetilde{\mathbf{w}}\_{k}=\left(w_{k 0}, \mathbf{w}\_{k}^{T}\right)^{T}$<br>
<br>

#### 제곱합 에러 함수
- 학습 데이터 $\left\\{\mathbf{x}\_{n}, \mathbf{t}\_{n}\right\\}, n=1, \ldots, N$<br>
  $n$번째 행이 $\mathbf{t}\_n^T$인 행렬 $\mathbf{T}$, $n$번째 행이 $\tilde{\mathbf{x}}\_n^T$인 행렬 $\widetilde{\mathbf{X}}$<br>
  $\widetilde{\mathbf{X}}=\left[\begin{array}{c}\vdots \\\\\\ -\widetilde{\mathbf{x}}\_{n}^{T}- \\\\\\ \vdots\end{array}\right], \widetilde{\mathbf{W}}=\left[\begin{array}{c}\mid \\\\\\ \cdots \widetilde{\mathbf{w}}\_{k} \cdots \\\\\\ \mid\end{array}\right], \quad \mathbf{T}=\left[\begin{array}{c}\vdots \\\\\\ -\mathbf{t}\_{n}^{T}- \\\\\\ \vdots\end{array}\right]$<br>
<br>
- 이 때 제곱합 에러함수는 다음과 같이 표현<br>
  $\displaystyle E_{D}(\widetilde{\mathbf{W}})=\frac{1}{2} \operatorname{tr}\left\\{(\widetilde{\mathbf{X} \mathbf{W}}-\mathbf{T})^{T}(\widetilde{\mathbf{X} \mathbf{W}}-\mathbf{T})\right\\}$<br>
<br>

- $\widetilde{\mathbf{W}}$에 대한 $E_{D}(\widetilde{\mathbf{W}})$의 최솟값을 구하면<br>
  $\widetilde{\mathbf{W}}=\left(\widetilde{\mathbf{X}}^{T} \widetilde{\mathbf{X}}\right)^{-1} \widetilde{\mathbf{X}}^{T} \mathbf{T}=\widetilde{\mathbf{X}}^{\dagger} \mathbf{T}$<br>
  따라서 판별함수는 $y(\mathbf{x})=\widetilde{\mathbf{W}}^{T} \widetilde{\mathbf{x}}=\mathbf{T}^{T}\left(\widetilde{\mathbf{X}}^{\dagger}\right)^{T} \widetilde{\mathbf{x}}$<br>
<br>

- 분류를 위한 최소제곱법의 문제들<br>
  - 극단치(outlier)에 민감
  - 목표값의 확률분포에 대한 잘못된 가정에 기초<br>
    목표값의 확률분포가 가우시안 분포를 따를 때 최소제곱법 적용가능

### 퍼셉트론 알고리즘
- $y(\mathbf{x})=f\left(\mathbf{w}^{T} \phi(\mathbf{x})\right)$
- 이 때 $f$는 활성 함수(activation function)로 퍼셉트론은 아래와 같은 계단형 함수를 사용<br>
  $f(a)=\left\\{\begin{array}{cc}+1, & a \geq 0 \\ -1, & a<0\end{array}\right\\}$<br>
<br>
- 이 때, $\phi_0(\mathbf{x}) = 1$이고 에러함수는 다음과 같음<br>
  $\displaystyle E_{P}(\mathbf{w})=-\sum_{n \in \mathcal{M}} \mathbf{w}^{T} \phi_{n} t_{n} > 0$<br>
  $\mathcal{M}$은 잘못 분류된 데이터들의 집합<br>
<br>

- 확률적 경사하강법을 적용할 때<br>
  $\mathbf{w}^{(\tau+1)}=\mathbf{w}^{(\tau)}-\eta \nabla E_{p}(\mathbf{w})=\mathbf{w}^{(\tau)}+\eta \phi_{n} t_{n}$<br>
<br>

- 업데이트가 실행될 때 오분류된 샘플에 대하여 다음과 같음<br>
  $-\mathbf{w}^{(\tau+1) T} \phi_{n} t_{n}=-\mathbf{w}^{(\tau) T} \phi_{n} t_{n}-\left(\phi_{n} t_{n}\right)^{T} \phi_{n} t_{n}<-\mathbf{w}^{(\tau) T} \phi_{n} t_{n} \quad (\because \left(\phi_{n} t_{n}\right)^{T} \phi_{n} t_{n} > 0)$<br>
  수식 때문에 복잡해 보이겠지만 오분류된 샘플에 대해서는 결정 경계와의 오차가 점점 줄어든다는 것을 의미<br>
<br>

- 한계: 클래스를 구분해주긴 하지만 각각의 클래스에 대한 확률을 계산해주지는 않음

### 확률적 생성 모델
- 분류문제를 확률적 관점에서 살펴보면, 선형회귀와 마찬가지로 확률적 모델은 통합적인 관점을 제공
- 데이터의 분포에 관해 어떤 가정을 두게 되면 선형적인 결정경계가 그 결과로 유도됨<br>
<br>
- $p(\mathbf{x} \mid \mathcal{C}_k)$와 $p(\mathcal{C}_k)$를 모델링한 다음 베이즈 정리를 적용하여 클래스의 사후 확률 $p(\mathcal{C}_k \mid \mathbf{x})$를 구해냄
- 이전 판별함수 방법에서는 어떤 에러함수를 최소화시키기 위한 최적의 파라미터를 찾는 것이 목적
- 확률적 모델은 데이터의 분포(클래스를 포함)를 모델링하면서 분류문제를 결과적으로 풀게 됨<br>
  $\displaystyle p\left(C_{1} \mid \mathbf{x}\right)=\frac{p\left(\mathbf{x} \mid C_{1}\right) p\left(C_{1}\right)}{p\left(\mathbf{x} \mid C_{1}\right) p\left(C_{1}\right)+p\left(\mathbf{x} \mid C_{2}\right) p\left(C_{2}\right)}=\frac{1}{1+\exp (-a)}=\sigma(a)$<br>
  <br>
  $\displaystyle a=\ln \frac{p\left(\mathbf{x} \mid C_{1}\right) p\left(C_{1}\right)}{p\left(\mathbf{x} \mid C_{2}\right) p\left(C_{2}\right)}$<br>
  <br>
  $\displaystyle \sigma(a)=\frac{1}{1+\exp (-a)}$<br>
<br>

#### 로지스틱 시그모이드의 성질 및 역함수
- 우선 시그모이드 함수가 도입이된 이유는 함수값이 0~1로 표현이 가능하고 이를 확률 값으로 표현 가능하기 때문
- $\sigma(-a) = 1 - \sigma(a)$
- $\displaystyle a = \ln(\frac{\sigma}{1 - \sigma})$ 이 때 $a$를 로짓(logit)이라고 부름
- 또한 위에서 언급한 $\displaystyle a=\ln \frac{p(\mathbf{x} \mid C_{1}) p(C_{1})}{p(\mathbf{x} \mid C_{2}) p(C_{2})}$은 로그 오즈(log odds)라고 함<br>

<br>


- $K > 2$인 경우<br>
  $\displaystyle p\left(C_{k} \mid \mathbf{x}\right)=\frac{p\left(\mathbf{x} \mid \mathcal{C}\_{k}\right) p\left(C_{k}\right)}{\sum_{j} p\left(\mathbf{x} \mid \mathcal{C}\_{j}\right) p\left(\mathcal{C}\_{j}\right)}=\frac{\exp \left(a_{k}\right)}{\sum_{j} \exp \left(a_{j}\right)}$ 이 때 이 함수를 소프트맥스(softmax) 함수라고 부름<br>

  $a_{k}= \ln\left(p\left(\mathbf{x} \mid \mathcal{C}\_{k}\right) p\left(\mathcal{C}\_{k}\right)\right)$<br>
<br>

---

### 입력이 연속적인 값일 경우
- 우선 $p(\mathbf{x} \mid C_{k})$가 가우시안 분포를 따르고 모든 클래스에 대해 공분산이 동일하다고 가정<br>
  $\displaystyle p(\mathbf{x} \mid \mathcal{C}\_{k})=\frac{1}{(2 \pi)^{D / 2}|\Sigma|^{1 / 2}} \exp \{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu}\_{k})^{T} \Sigma^{-1}(\mathbf{x}-\boldsymbol{\mu}\_{k})\}$<br>
<br>
- 두 개의 클래스인 경우로 가정 $\rightarrow p(C_1 \mid \mathbf{x}) = \sigma(a)$<br>
  $a$에 관하여 표현하고 위의 식을 대입, 전개하면<br>
  $\begin{aligned} a &=\ln \frac{p\left(\mathbf{x} \mid \mathcal{C}\_{1}\right) p\left(\mathcal{C}\_{1}\right)}{p\left(\mathbf{x} \mid \mathcal{C}\_{2}\right) p\left(\mathcal{C}\_{2}\right)} \\\\\\ &=-\frac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}\_{1}\right)^{T} \Sigma^{-1}\left(\mathbf{x}-\boldsymbol{\mu}\_{1}\right)+\frac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}\_{2}\right)^{T} \Sigma^{-1}\left(\mathbf{x}-\boldsymbol{\mu}\_{2}\right)+\ln \frac{p\left(C_{1}\right)}{p\left(C_{2}\right)} \\\\\\ &=\left\{\left(\boldsymbol{\mu}\_{1}^{T}-\boldsymbol{\mu}\_{2}^{T}\right) \Sigma^{-1}\right\} \mathbf{x}-\frac{1}{2} \boldsymbol{\mu}\_{1}^{T} \Sigma^{-1} \boldsymbol{\mu}\_{1}+\frac{1}{2} \boldsymbol{\mu}\_{2}^{T} \Sigma^{-1} \boldsymbol{\mu}\_{2}+\ln \frac{p\left(C_{1}\right)}{p\left(C_{2}\right)} \end{aligned}$<br>
<br>
- 위의 식에서 $a$는 $\mathbf{x}$의 선형식($a = \mathbf{w}^T\mathbf{x} + w_0$)으로 표현이 가능<br>
  $\begin{aligned} \mathbf{w} &=\Sigma^{-1}\left(\boldsymbol{\mu}\_{1}-\boldsymbol{\mu}\_{2}\right) \\ w_{0} &=-\frac{1}{2} \boldsymbol{\mu}\_{1}^{T} \Sigma^{-1} \boldsymbol{\mu}\_{1}+\frac{1}{2} \boldsymbol{\mu}\_{2}^{T} \Sigma^{-1} \boldsymbol{\mu}\_{2}+\ln \frac{p\left(C\_{1}\right)}{p\left(C_{2}\right)} \end{aligned}$<br>
  
- 정리하면 같은 공분산을 가지는 두 개의 가우시안 분포가 만나는 지점은 직선을 형성할 것임을 알 수 있음<br>
- 따라서 두 개의 가우시안 분포의 분류는 아래와 같이 될 수 있음(오른쪽은 시그모이드 함수로 분류가 될 수 있음을 표현)<br>
  ![2.png](attachment:2.png)

### 최대우도해
- MLE를 통해 모델 파라미터들을 구하기
- 두 개의 클래스인 경우 파라미터들은 $p(C_1) = \pi$일 때 구해야할 파라미터들은 $\mu_1, \mu_2, \Sigma, \pi$<br>
<br>
- 결합확률분포는 다음과 같음<br>
  $t_n = 1$일 때 $p(\mathbf{x}\_n, C_1) = p(C_1)p(\mathbf{x}\_n \mid C_1) = \pi \mathcal{N}(\mathbf{x}\_n \mid \mu_1, \Sigma)$<br>
  $t_n = 0$일 때 $p(\mathbf{x}\_n, C_2) = p(C_2)p(\mathbf{x}\_n \mid C_2) = (1 - \pi) \mathcal{N}(\mathbf{x}\_n \mid \mu_2, \Sigma)$<br>
<br>

- 따라서 우도함수는 다음과 같음<br>
  $\displaystyle p(\mathbf{t} \mid \pi, \boldsymbol{\mu}_{1}, \boldsymbol{\mu}_{2}, \Sigma)=\prod_{n=1}^{N}[\pi \mathcal{N}(\mathbf{x}\_{n} \mid \boldsymbol{\mu}\_{1}, \Sigma)]^{t_{n}}[(1-\pi) \mathcal{N}(\mathbf{x}\_{n} \mid \boldsymbol{\mu}\_{2}, \Sigma)]^{1-t_{n}}$<br>
<br>

#### 매개변수 구하기 - $\pi$
- 로그우도함수에서 $\pi$관련항들만 모아보면<br>
  $\displaystyle \sum_{n=1}^{N}\{t_{n} \ln \pi+(1-t_{n}) \ln (1-\pi)\}$<br>
- 위의 식을 $\pi$에 관하여 미분하고 그 식을 0으로 만족시키는 $\pi$를 구하면<br>
  $\displaystyle \pi=\frac{1}{N} \sum_{n=1}^{N} t_{n}=\frac{N_{1}}{N}=\frac{N_{1}}{N_{1}+N_{2}}$<br>
- 이 때 $N_1$은 $C_1$에 속하는 샘플 수 $N_2$는 $C_2$에 속하는 샘플 수<br>
<br>

#### 매개변수 구하기 - $\mu_1, \mu_2$
- 위에서와 마찬가지로 $\mu_1$ 혹은 $\mu_2$관련항들만 모아서 각각의 식을 해당 매개변수로 미분하여 그 식의 값이 0을 만족시키는 값을 구하기<br>
  $\displaystyle \sum_{n=1}^{N} t_{n} \ln N\left(\mathbf{x}\_{n} \mid \mu_{1}, \Sigma\right)=-\frac{1}{2} \sum_{n=1}^{N} t_{n}\left(\mathbf{x}\_{n}-\mu_{1}\right)^{T} \Sigma^{-1}\left(\mathbf{x}\_{n}-\mu_{1}\right)+$ const 을 $\mu_1$으로 식의 값이 0을 만족시키는 매개변수 값 구하기<br>
<br>
- 따라서 $\mu_1$은 다음과 같이 나타낼 수 있고<br>
  $\displaystyle \mu_{1}=\frac{1}{N_{1}} \sum_{n=1}^{N} t_{n} \mathbf{x}\_{n}$
- 마찬가지 방법으로 $\mu_2$는 다음과 같이 나타낼 수 있음<br>
  $\displaystyle \mu_{2}=\frac{1}{N_{2}} \sum_{n=1}^{N}\left(1-t_{n}\right) \mathbf{x}\_{n}$<br>
<br>
- 직관적으로 알 수 있듯이 평균 $\mu$는 각 클래스에 속하는 관찰값들의 평균인 것을 알 수 있음<br>
<br>

#### 매개변수 구하기 - $\Sigma$
- 마찬가지 방법으로 $\Sigma$에 관련된 항들만 모아보면<br>
  $\displaystyle -\frac{1}{2} \sum_{n=1}^{N} t_{n} \ln |\Sigma|-\frac{1}{2} \sum_{n=1}^{N} t_{n}\left(\mathbf{x}_{n}-\mu_{1}\right)^{T} \Sigma^{-1}\left(\mathbf{x}\_{n}-\mu_{1}\right)-\frac{1}{2} \sum_{n=1}^{N}\left(1-t_{n}\right) \ln |\Sigma|-\frac{1}{2} \sum_{n=1}^{N}\left(1-t_{n}\right)\left(\mathbf{x}\_{n}-\mu_{1}\right)^{T} \Sigma^{-1}\left(\mathbf{x}\_{n}-\mu_{1}\right)$<br> 
  $\displaystyle =-\frac{N}{2} \ln |\Sigma|-\frac{N}{2} \operatorname{tr}\left\{\Sigma^{-1} \mathbf{S}\right\}$<br>
<br>
- 위의 식을 미분하여 그 값을 0이되는 $\Sigma$의 값은 $\mathbf{S}$가 됨<br>
<br>

- 이 때 $\mathbf{S}$는 다음과 같음<br>
  $\displaystyle \mathbf{S}=\frac{N_{1}}{N} \mathbf{S}\_{1}+\frac{N_{2}}{N} \mathbf{S}\_{2}$<br>
  $\displaystyle \mathbf{S}\_{1}=\frac{1}{N_{1}} \sum_{n \in C_{1}}\left(\mathbf{x}\_{n}-\mu_{1}\right)\left(\mathbf{x}\_{n}-\mu_{1}\right)^{T}$<br>
  $\displaystyle \mathbf{S}\_{2}=\frac{1}{N_{2}} \sum_{n \in C_2}\left(\mathbf{x}\_{n}-\mu_{2}\right)\left(\mathbf{x}\_{n}-\mu_{2}\right)^{T}$
  
---

### 입력이 이산적인 값일 경우
- 각 특성 $x_i$가 0 혹은 1중 하나의 값만 가질 경우
- 클래스가 주어졌을 때 각 특성들이 조건부 독립이라는 가정을 할 경우 단순하게 표현이 가능(naive Bayes 가정)<br>
  $\displaystyle p(\mathbf{x} \mid C_k) = \prod_{i=1}^{D} \mu_{ki}^{x_i}(1 - \mu_{ki})^{1-x_i}$<br>
  <br>
  마찬가지로
  $a_k(\mathbf{x}) = \ln(p(\mathbf{x}\mid C_k)p(C_k)$<br>
  <br>
  $\displaystyle a_k(\mathbf{x}) = \sum_{i=1}^{D}\{x_i\ln \mu_{ki} + (1 - x_i)\ln (1 - \mu_{ki})\} + \ln p(C_k)$<br>
<br>
- 앞의 생성모델에서 $p(C_k \mid \mathbf{x})$의 선형함수가 로지스틱 시그모이드(이중 분류) 혹은 소프트맥스(다중 분류)를 통과하는 식으로 표현
- 이전에는 파라미터를 MLE로 구했으나 대안적인 방법을 살펴본다면 **오히려 MLE를 이용하여 직접적으로 파라미터들을 구하는 것**
- 비선형 기저함수 $\phi(\mathbf{x})$를 입력벡터 $\mathbf{x}$대신 사용

---

### 로지스틱 회귀
- 클래스 C_1의 사후확률은 특성벡터 $\phi$의 선형함수가 logistic sigmoid를 통과하는 함수<br>
  $P(C_1 \mid \phi) = y(\phi) = \sigma(\mathbf{w}^T\phi)$<br>
  $P(C_2 \mid \phi) = 1- P(C_1 \mid \phi)$<br>
  <br>

#### 최대우도해
- 우도함수와 음의 로그 우도는<br>
  $\displaystyle p(\mathbf{t} \mid \mathbf{w})=\prod_{n=1}^{N} y_{n}^{t_{n}}(1-y_{n})^{1-t_{n}}$<br>
  $\displaystyle E(\mathbf{w})=-\ln p(\mathbf{t} \mid \mathbf{w})=-\sum_{n=1}^{N}\{t_{n} \ln y_{n}+(1-t_{n}) \ln (1-y_{n})\}$ 이 때 $y_{n} = \sigma(a_n), a_n = \mathbf{w}^T\phi_n$<br>
<br>
- 여기서 $E(\mathbf{w})$(음의 로그 우도 형태)를 크로스 엔트로피 에러함수라고 부름<br>
<br>
- 또한 크로스 엔트로피를 일반적인 정의로 나타내보면 다음과 같음<br>
  $H(p, q)=-\mathbb{E}\_{p}[\ln q]$<br>
  $\displaystyle H(p, q)=-\sum_{x} p(x) \ln q(x)$<br>
<br>

- 따라서 크로스 엔트로피를 최소화될 때 두 확률분포($p, q$의 분포)의 차이가 최소화 되어있고, 에러함수를 최소화 시키는 것은 두 가지 관점에서 접근이 가능<br>
  1. 우도함수를 최대화
  2. 모델의 예측값과 목표변수의 차이를 최소화

### 느낀점
선형분류에 대해 알아보았다. 전반적인 내용은 잘 이해할 수 있었고, 이전에 알고있던 내용들을 수식으로 풀어서 정리한 것이라<br>
생각보다 이해도 잘됐던 것 같다. 기본적인 내용들이지만 이 내용을 나중에 코드로 잘 구현할수 있어야하기 때문에<br>
완벽한 이해가 따라줘야 완벽한 구현이 될 수 있을 것 같고, 또 상황에 따라 응용도 할 수 있을 것 같다
