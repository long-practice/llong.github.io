---
title: 프로그래머스 인공지능 데브코스 61일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 13주차(22.3.7.(월) ~ 22.3.11.(금)) 
  Spark ML Pipeline
  NLP 개요, 언어모델, 단어 임베딩, 텍스트 전처리
tags:
- 자연어 처리(NLP)
- 텍스트 전처리
- 단어
- 말뭉치
- 텍스트 정규화
- 토큰화
- Byte-Pair Encoding(BPE)
- WordPiece
- Unigram
- 단어 정규화

use_math: True
---

# 61일차(NLP-텍스트 전처리)

13주차(22.3.14.(월) ~ 22.3.8.(금)): 빅데이터 총정리, NLP
* 빅데이터 총정리
* NLP 기초(텍스트 전처리, 언어모델, 문서분류, 단어 임베딩) 강의
<br>
<br>

출처: 프로그래머스 인공지능 데브코스 3기 강의

<br><br>

### 자연어 처리(NLP)
- 자연어의 의미를 컴퓨터로 분석해서 특정 작업을 위해 사용할 수 있도록 하는 것
- 기계번역, 감정분석, 문서분류, 챗봇, 음성인식, 문장생성, 추천시스템 등에 적용할 수 있음
- 기존 컴퓨터 비전에 적용했던 CNN과 같은 알고리즘들이 자연어 처리 분야에 적용함에 따라서 좋은 성능을 보이기 시작했음
- 최근에는 딥러닝을 적용하여 NLP분야에서 좋은 성능을 보이는 모델들이 등장(Transformer, BERT, GPT, ...)<br>
  또한 위의 모델들이 컴퓨터 비전에 적용이되기 시작하면서 이전 모델들보다 더 좋은 성능을 보이기도 함<br>
<br>
<br>

---

### 텍스트 전처리
- 자연어 처리를 하기 위해 제일 먼저 수행해야하는 작업<br>
<br>

#### 단어(word)
- 문장에서 **몇 개의 단어**를 가지고 있는지?, **문장부호를 단어로 포함**을 시켜야할 지 판단해야 함
- 구어체 문장의 경우 **특정 말투(uh, um...), 깨어진 단어(버벅거리는 경우)**를 포함시킬지도 포함시켜야할 지 판단해야 함
- **표제어**: 여러 단어들이 공유하는 뿌리 단어
- **단어형태**: 같은 표제어를 공유하지만 다양한 형태를 가질 수 있음<br>
<br>

- Vocabulary: 단어의 집합
- Type: Vocabulary의 한 원소
- Token: 문장 내에 나타나는 한 단어<br>
<br><br>

#### 말뭉치(Corpus)
- 하나의 말뭉치는 일반적으로 대용량 문서들의 집합
- 언어, 방언, 장르(소설, 특정분야 문서, 뉴스 등), 글쓴이의 속성(나이, 성별, 인종 등)에 따라 말뭉치의 특성이 달라짐
- 다양한 말뭉치에 적용할 수 있는 NLP알고리즘이 필요<br>
<br><br>

#### 텍스트 정규화
- 텍스트 전처리시 가장 중요한 과정
- 모든 자연어 처리는 텍스트 정규화를 필요로 함
- 토큰화(tokenizing words), 단어정규화(normalizing word formats), 문장분절화(segmenting sentences) 등<br>
<br><br>

#### 토큰화
- 텍스트에서 각각의 단어를 토큰화를 시키고 vocabulary를 생성
- 그러나 문장부호(punctuation)들을 항상 무시할 수는 없음(문장부호, 화폐단위, 이메일 주소, URL 등)
- 다른 단어에 붙어서 존재하는 형태의 단어인 접어에 대해서 취약할 수 있음(we're == we are 등)
- 여러 개의 단어가 붙어야 의미가 있는 경우에서도 취약할 수 있음(New York, rock'n'roll 등)
- 언어에 따라서 띄어쓰기를 하지 않는 경우 토큰화시키는 것(몇 개의 단어로 구분할 지)이 어려울 수 있음<br>
<br><br>

#### 한국어의 토큰화
- 한국어의 경우 토큰화가 굉장히 복잡함
- 띄어쓰기가 잘 지켜지지 않고 띄어쓰기가 제대로 되었더라도 **한 어절이 하나 이상의 의미를 가질 수 있음**
- 형태소(morpheme): 뜻을 가진 가장 작은 말의 단위<br>
  - 자립 형태소: 형태소 자체적으로 의미를 가진 형태소
  - 의존 형태소: 다른 형태소와 결합하여 사용되는 형태소
- **한국어의 경우 단어보다 작은 단위(subword)로 토큰화가 필요하다는 것을 알 수 있음**
- 예를 들면 `제 친구인 철수입니다.`는 `['저', '의', '친구', '이', '-ㄴ', '철수', '이', '-ㅂ니다', '.']`로 토큰화가 됨<br>
<br><br>

#### 작은 단위로의 토큰화(subword tokenization)
- 학습데이터: `['low', 'new', 'newer']`<br>
  테스트데이터: `lower`의 경우를 살펴보면<br>
  `-er, -est`등과 같은 형태소를 분리할 수 있으면 좋음
- Byte-Pair Encoding(BPE), WordPiece, Unigram language modeling 등의 알고리즘들이 있음<br>
<br>
- 두 가지 구성요소<br>
  - Token learner: 말뭉치에서 vocabulary(token들의 집합)를 만들어 냄
  - Token segmenter: 새로운 문장을 토큰화<br>
<br><br>

#### Byte-Pair Encoding(BPE)
- Vocabulary를 단일 문자들의 집합으로 초기화
- 다음 과정을 반복<br>
  1. 말뭉치에서 연속적으로 **가장 많이 발생하는 두 개의 기호**들을 찾기
  2. 두 기호들을 하나로 병합하고 새로운 기호로 vocabulary에 추가
  3. 말뭉치에서 그 두 기호들을 병합된 기호로 모두 교체
- 병합은 단어안에서만 이루어지고, 단어 끝을 나타내는 특수기호 `_`를 단어 뒤에 추가, 각 단어를 문자단위로 쪼갬<br>
<br>

- 예를들어 어떤 말뭉치에 `['low', 'lowest', 'newer', 'wider', 'new']`라는 단어가 있을 때
- 각각의 단어들을 문자단위로 쪼개면 `['l o w _', 'l o w e s t _', ... , 'n e w _']`로 나타낼 수 있음<br>
  - 이 때 vocabulary는 `[_, d, e, i, l, n, o, r, s, t, w]`
- 가장 많이 등장한 3번째 단어 `newer`와 4번째 단어 `wider`에서 공통부분<br>
  - `n e w e r _`, `w i d e r _`에서 `e`, `r`이 병합되어 `er`이 생기고, `er`, `_`이 병합되어 `er_`이 생성
  - 이 때 vocabulary는 `[_, d, e, i, l, n, o, r, s, t, w, er, er_]`가 됨
  - 각각의 단어는 `['n e w er_', 'w i d er_']`이 됨
- 계속적으로 병합이 이루어지면서((ne, w), (l, o), (lo, w), ...) vocabulary의 크기는 점점 커짐
- vocabulary의 크기를 제한함으로써 token생성을 제어할 수 있음<br>
<br>

- 새로운 단어가 주어졌을 때 토큰화를 진행하는 방법은 그리디하게 병합을 진행하며, 학습한 순서대로 적용
- **자주 등장하는 단어는 하나의 토큰으로 병합될 가능성이 높고 드물게 등장하는 단어는 subword 토큰들로 분할**
- 최종적으로 vocabulary가 `[_, d, e, i, l, n, o, r, s, t, w, er, er_, ne, new, lo, low, newer_, low_]`일 때,<br>
  새로운 단어 `newer`는 `n e w er _` > `n e w er_` > `ne w er_` > `new er_` > `newer_`로 토큰화<br>
  새로운 단어 `lower`은 `l o w er _` > `l o w er_` > `lo w er_` > `low er_`로 토큰화<br>
<br>

- subword 토큰 또한 어떤 의미를 가지고 있을 것이기 때문에 하나의 단어를 분할하게 되면 NLP모델에서 더 좋은 성능을 기대할 수 있음<br>
<br><br>

#### WordPiece
- 기호들의 쌍을 찾을 때 빈도수 대신 **likelihood를 최대화시키는 쌍**을 찾음
- Corpus1에서 Corpus2로 병합될 확률 $P(Corpus2)$가 Corpus3으로 병합될 확률 $P(Corpus3)$보다 크다면<br>
  즉, $P(Corpus2) > P(Corpus3)$라면 Corpus2방식으로 병합
- 위에서 확률은 **언어모델**을 이용하여 구할 수 있음
- 제한한 vocabulary 크기에 도달하거나 threshold likelihood를 넘어서지 않을 때까지 word unit을 찾아냄 <br>
<br>

#### Unigram
- **언어모델(확률모델)을 이용**하고, 학습데이터 내의 문장을 관측하여 확률변수로 정의
- 즉, vocabulary를 구축할 때 확률값을 같이 저장
- Tokenization을 잠재(latent) 확률변수로 정의(연속적인(sequential) 변수)
- 데이터의 주변 우도(marginal likelihood)를 최대화시키는 tokenization을 구함<br>
<br><br>

---

### 단어정규화
- 단어들을 정규화된 형식으로 표현
- 같은 뜻을 나타내지만 다르게 표현되는 단어들이 존재<br>
  예를 들면 `U.S.A, USA, US` 혹은 `am, is, be, are`
- 모든 문자들을 소문자로 변경해주는 것이 유용할 때가 있는데 학습 데이터와 테스트 데이터 사이에서 불일치 문제에 도움이 될 수 있음
- 정보검색, 음성인식 등에서 유용할 수 있으나 감정분석 등의 문서분류 문제에서는 오히려 대소문자 구분이 유용할 수 있음(국가이름 `US` vs 대명사 `us`)<br>
<br>
- 최근 경향에 따르면 단어 정규화를 복잡하게 하지는 않음
- 물론 단어들 사이의 유사성을 이해해야하기 때문에 정규화를 해야함
- 그러나 단어를 vocabulary로 정의된 공간, 저차원 밀집 벡터로 대응시킬 때를 생각해보면<br>
  **비슷한 뜻을 가지는 단어들끼리는 서로 밀집되어 분포**<br>
- 따라서 **단어 임베딩을 통해 단어를 표현하게되면 단어 정규화의 필요성이 줄어들게 됨**<br>
<br><br>


### 느낀점
어느덧 NLP까지 오게되었다. 현재 자연어 처리 특히 한국어의 자연어 처리가 굉장히 어렵다고 하는데,<br>
왜 그런지 이유를 상세하게 알게되었고, 또한 다른 언어들에 대해서도 어떤 문제들이 발생할 수 있는지 알 수 있었다.<br>
최종 프로젝트를 자연어 처리와 관련된 주제로 진행을 하게될텐데 다른 강의보다 더 중요하게 생각하고<br>
세부적으로 이해해야할 것 같다는 생각이든다. 첫 시간이어서 그런지 어려운 내용은 없었으나<br>
희미하게 이해하고 있는 부분들에 대해서는 바로바로 해결해나가야할 것 같다.
