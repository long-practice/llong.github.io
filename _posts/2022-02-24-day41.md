---
title: 프로그래머스 인공지능 데브코스 41일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 9주차(22.2.7.(월) ~ 22.1.12.(토)) 
  신경망, 딥러닝 기초
  다층 퍼셉트론
  Weekly Mission
tags:
- 다층 퍼셉트론
- 특징 공간
- 계단함수
- 시그모이드 함수
- 오류 역전파
- 활성함수
- 다층 퍼셉트론의 구조
- 범용적 근사 이론
- 은닉층
- 다층 퍼셉트론의 동작
- 아키텍처
- 초기값
- 학습률
- 목적함수
- 훈련집합
- 평균 제곱 오차
- 연쇄 법칙(chain rule)
- 미니배치 방식
- 자동미분연산
- pytorch

use_math: True
---

# 41일차(신경망 기초 - 다층 퍼셉트론)

9주차(22.2.7.(월) ~ 22.2.12.(토)): 신경망 기초
* 신경망 기초
* 다층 퍼셉트론
* 딥러닝 기초
* monthly project

### 다층 퍼셉트론
- 퍼셉트론(선형 분류기)의 한계는 선형 분리가 불가능한 상황(XOR 분류 문제)에서는 적용하기 어려움
- 그런데 XOR 연산을 예로 들으면 XOR연산은 AND, OR, NOT 연산의 조합으로 구현이 가능
- 따라서 여러개의 퍼셉트론으로 XOR연산을 구현가능<br>
<br>
- 퍼셉트론 2개를 병렬 결합할 때를 생각해보면
- 원래 공간 $\mathbf{x} = (x_1, x_2)^T$를 새로운 특징 공간 $\mathbf{z} = (z_1, z_2)^T$로 변환
- 즉, 새로운 특징 공간 $\mathbf{z}$에서 선형 분리 가능
- 이러한 학습 과정은 사람이 수작업 특징 학습을 수행한 것과 유사 $\rightarrow$ 표현 학습<br>
<br>
- 변형된 새로운 특징 공간$\mathbf{z}$을 선형 분리 수행하는 새로운 퍼셉트론을 순차결합하면 다층 퍼셉트론이 됨
- 즉, 원래 공간에서 선형 분리가 안될 때 데이터들을 새로운 특징 공간으로 변환시킴으로써 선형 분리가 가능하게 만듦<br>
  - 행렬 곱: 회전
  - 편향: 이동
  - 비선형 함수: 왜곡

#### 다층 퍼셉트론의 핵심 아이디어
- 은닉층을 두고 특징 공간을 분류하는데 훨씬 유리한 새로운 특징 공간으로 변환
- 시그모이드 활성함수를 도입하여 연성 의사결정이 가능 (cf. 계단함수는 경성 의사결정(1 또는 0))
- 연성에서는 출력이 연속값인데 출력을 신뢰도로 간주함으로써 더 융통성 있게 의사결정 가능
- 오류 역전파 알고리즘 사용<br>
  - 다층 퍼셉트론은 여러 층이 순차적으로 이어진 구조
  - 역방향으로 진행하면서 한 번에 한 층씩 그레디언트를 계산하고 가중치를 갱신

#### 다층 퍼셉트론의 용량(capacity)
- 3개의 퍼셉트론을 결합한 경우를 생각하면
- 2차원 공간을 7개의 영역으로 나누고 각 영역을 3차원 점으로 변환
- 계단함수를 활성함수 $\tau$로 사용을 가정했으므로 **7개의 영역을 7개의 점으로 변환(3차원 공간으로 변환)**
- 따라서 일반화를 하면 **$p$개의 퍼셉트론이 결합되면 $p$차원 공간으로 변환**

---

### 활성함수
- 딱딱한 공간 분할과 부드러운 공간 분할(경성 의사결정 vs 연성 의사결정)<br>
  - 계단함수는 딱딱한(경성) 의사결정 $\rightarrow$ 영역을 점으로 변환
  - 그 외 활성함수(시그모이드, 하이퍼볼릭 탄젠트 시그모이드, rectifier, softplus, ...)는 부드러운(연성) 의사결정 $\rightarrow$ 영역을 영역으로 변환<br>
    부드러운 의사결정을 하는 활성함수는 **모든 구역에서 연속인 그래프 형태로 나타나짐**<br>
<br>

#### 시그모이드 함수
- 대표적인 비선형 함수로 S자 모양의 활성함수<br>
- 이진 시그모이드 함수: $\displaystyle \tau_1(x) = \frac{1}{1 + e^{\alpha x}}$
- 양극 시그모이드 함수: $\displaystyle\tau_2(x) = \frac{2}{1 + e^{\alpha x}} - 1$ 
- $\alpha$($e$의 지수부의 입력변수의 계수)가 커질수록 계단함수에 가까워짐
<br>
<br>

#### 활용 예
- 퍼셉트론은 계단함수
- 다층 퍼셉트론은 로지스틱 시그모이드와 하이퍼볼릭 탄젠트 함수<br>
  - 양 극단에 데이터를 배치하기 때문
- 심층학습은 ReLU(rectified linear activation)<br>
  - S자 모양의 넒은 포화곡선은 경사도 기반한 학습(오류 역전파)을 어렵게 함
  - 오류 역전파 알고리즘을 적용할 때 어떤 데이터를 입력 값이 크게 되면 그레디언트가 없어지는 경우가 발생(시그모이드 함수의 경우)
  - 따라서 손실이 발생했음에도 불구 중간에 그레디언트가 없어지게 됨(gradient vanish) 이에 따라 수 많은 층을 거쳐야하는 깊은 신경망 학습이 불가<br>
<br>

---

### 다층 퍼셉트론의 구조
- $d+1$개의 입력 노드($d$는 특징 개수), $c$개의 출력 노드($c$는 부류 개수)
- $p$개의 은닉 노드: $p$는 하이퍼 매개변수(사용자가 정해주는 매개변수)
- $p$가 너무 크면 과대적합, 너무 작으면 과소적합
- **다층 퍼셉트론은 입력층 - $p$개의 은닉층 - 출력의 $p$층 구조**
- 주로 3 ~ 4층을 사용하고 그 이상의 구조를 갖는 경우 깊은 신경망 학습<br>
![2.png](attachment:2.png)<br>
<br>

#### 범용적 근사 이론
- 하나의 은닉층은 함수의 근사
- 다층 퍼셉트론도 공간을 변환하는 근사 함수<br>
<br>

#### 얕은 은닉층의 구조
- 지수적으로 더 넓은 폭이 필요할 수 있음(더 많은 가중치를 필요로할 수 있음)
- 과대적합이 되기 쉬움
- 일반적으로 깊은 은닉층의 구조가 좋은 성능<br>
<br>

#### 은닉층 깊이에 따른 이점
- 지수의 표현<br>
  - 각 은닉층은 입력 공간을 어디서 접을지 지정 $\rightarrow$지수적으로 많은 선형적인 영역 <br>
<br>
- 즉, 은닉층을 통과했더니 데이터들이 간단하게 혹은 선형적으로 분리가 가능
- 하지만 은닉층에서의 결정 경계는 원래 공간에서 굉장히 복잡한 형태의 결정경계로 그려짐

---

### 다층 퍼셉트론의 동작
- 특징벡터 $\mathbf{x}$를 출력 벡터 $\mathbf{o}$로 사상하는 함수로 간주<br>
  - 2층 퍼셉트론: $\mathbf{o} = f(\mathbf{x}) = f_2(f_1(x))$
  - 3층 퍼셉트론: $\mathbf{o} = f(\mathbf{x}) = f_3(f_2(f_1(x)))$
  - 깊은 신경망: $\mathbf{o} = f_L( \cdots f_2(f_1(x))), L \ge 4$<br>
<br>
- 행렬로 표기<br>
  $\mathbf{o = \tau(U^2 \tau_h(U^1x))}$<br>
<br>
- 은닉층은 새로운 특징 공간으로 변환하는 역할을 하는데 다시 표현하면 특징 벡터를 분류에 더 유리한 새로운 특징 공간으로 변환하는 것을 뜻함
- 따라서 은닉층을 통과하는 과정은 데이터의 특징들을 계층화, 추상화하는 과정
- 현대 기계학습에서는 특징학습이라고 부름
- 심층학습은 더 많은 층을 거쳐 계층화된 특징학습을 함<br>
<br>

#### 다층 퍼셉트론 학습 과정
- 순방향 전파(입력층 $\rightarrow$ 은닉층 $\rightarrow$ 오차계산) $\rightarrow$ 전방 계산
- 역방향 전파(그레디언트 계산 $\rightarrow$ 출력층 $\rightarrow$ 입력층) $\rightarrow$ 오류 역전파
- 오류 역전파 계산 시 전방 계산 대비 약 1.5배 ~ 2배의 시간 소요 $\rightarrow$ 비교적 빠름
- 학습 알고리즘은 오류 역전파 반복, 시간복잡도는 $\Theta((cp + dp)nq)$<br>
  ($c$: 분류 수, $d$: 특징 차원, $p$: 은닉층 차원, $n$: 훈련집합의 크기, $q$: 세대 수)<br>
  학습 알고리즘의 시간복잡도는 주로 훈련집합 크기와 훈련을 얼마나 많이 반복하는지가 주로 결정

### 신경망의 경험적 개발에서 중요 쟁점
- 아키텍처: 은닉층과 은닉 노드 개수 선정<br>
  - 은닉층과 은닉 노드를 늘리면 신경망의 용량이 커지는 대신 추정할 매개변수가 많아지고 과대적합의 가능성
- 초기값: 최초 가중치를 초기할 때 값의 범위와 분포를 어떻게 설정해야 하는지
- 학습률: 처음부터 끝까지 같은 학습률을 사용할 것인지, 아니면 처음에는 큰 값으로 나중에는 점점 줄일 것인지
- 활성함수: 어떤 활성함수를 사용할 것인지, 특히 은닉층의 개수가 늘어남에 따라 그레디언트 소멸과 같은 문제를 어떤 활성함수를 사용함으로써 해결할 것인지

---

### 목적함수

#### 훈련집합
- 특징 벡터 집합 $\mathbb{X} = \{x_1, x_2, \cdots, x_n\}$과 부류 벡터 집합 $\mathbb{Y} = \{y_1, y_2, \cdots, y_n\}$
- 부류 벡터는 단발성(one-hot) 코드로 표현, 즉 $y_i = (0, 0, \cdots, 1, \cdots, 0)^T$
- 설계 행렬로 표현하면 다음과 같음<br>
  $\mathbf{X}=\left(\begin{array}{c}\mathbf{x}\_{1}^{\mathrm{T}} \\\\\\ \mathbf{x}\_{2}^{\mathrm{T}} \\\\\\ \vdots \\\\\\ \mathbf{x}\_{n}^{\mathrm{T}}\end{array}\right)$ $, \quad \mathbf{Y}=\left(\begin{array}{c}\mathbf{y}\_{1}^{\mathrm{T}} \\\\\\ \mathbf{y}\_{2}^{\mathrm{T}} \\\\\\ \vdots \\\\\\ \mathbf{y}\_{n}^{\mathrm{T}}\end{array}\right)$
- $\mathbf{Y} = f(\mathbf{X})$에서 모든 샘플을 올바르게 분류하는 함수 $f$를 찾는 일이 기계학습의 목표<br>
<br>

#### 평균 제곱 오차
- L2-norm을 사용하여 두 데이터 값의 차이를 구함
- 온라인 모드는 순차적으로 들어오는 데이터들에 대해 오차를 구함 $\displaystyle e=\frac{1}{2}\|\mathbf{y}-\mathbf{o}\|\_{2}^{2}$
- 배치 모드는 일정 사이즈의 데이터 셋에 대한 각 오차들의 평균을 구함 $\displaystyle e=\frac{1}{2 n} \sum_{i=1}^{n}\left\vert\mathbf{y}\_{i}-\mathbf{o}\_{i}\right\vert\_{2}^{2}$

---

### 오류 역전파 알고리즘의 설계
- 연쇄 법칙으로 구현
- 반복되는 부분식들을 저장하거나 재연산을 최소화(동적 프로그래밍)
- 경사도 계산을 할 때를 살펴보면 입력$w$와 은닉층$x, y$, 출력$z$에서 그레디언트는 $\displaystyle \frac{\partial z}{\partial w}$
- 위의 그레디언트를 은닉층에서의 미분을 포함시켜 연쇄법칙으로 표현을 한다면 다음과 같음<br>
  $\displaystyle \frac{\partial z}{\partial w} = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x} \frac{\partial x}{\partial w} = f^{\prime}(y) f^{\prime}(x) f^{\prime}(w) = f^{\prime}(f(f(w))) f^{\prime}(f(w)) f^{\prime}(w)$<br>
- 위의 식에서 보면 $f(w), f(f(w)), f(\cdots(f(w))$는 연산이 반복되는 부분<br>
<br>
- 아래 그림을 보면 $\displaystyle \frac{d z}{d w}$를 구하는 과정이 오류 역전파 알고리즘<br>
![1.png](attachment:1.png)<br>
<br>

- 위에서 언급한 목적함수를 다시 쓰면<br>
  $\displaystyle J(\Theta)=\frac{1}{2}\|\mathbf{y}-\mathbf{o}(\Theta)\|\_{2}^{2}$
- 2층 퍼셉트론의 경우 $\Theta = \{\mathbf{U}^1, \mathbf{U}^2\}$로 표현이 가능하고, $J(\Theta) = J(\{\mathbf{U}^1, \mathbf{U}^2\})$의 최저점을 찾아주는 경사 하강법은 다음과 같음<br>
  $\left.\begin{array}{l}\mathbf{U}^{1}=\mathbf{U}^{1}-\rho \frac{\partial J}{\partial \mathbf{U}^{1}} \\\\\\ \mathbf{U}^{2}=\mathbf{U}^{2}-\rho \frac{\partial J}{\partial \mathbf{U}^{2}}\end{array}\right\\}$<br>
<br>
- 따라서 오류 역전파 알고리즘은 출력의 오류를 역방향으로 전파하여 그레디언트(경사도)를 계산하는 알고리즘
- 반복되는 부분식들의 경사도의 지수적 폭발 혹은 사라짐을 피해야 함<br>

<br>

#### 여러가지 오류 역전파
- 단일 노드($\text{out} = f(\text{in})$)의 역전파<br>
  $\displaystyle\frac{\partial \mathcal{E}}{\partial \text {in}}=\frac{\partial \mathcal{E}}{\partial \text {out}} \cdot \frac{\partial \text {out}}{\partial \text {in}} =\frac{\partial \mathcal{E}}{\partial \mathrm{out}} \cdot f^{\prime}(\text{in})$<br>
<br>
- 곱셈($\text{out} = \text{in}_1 \cdot \text{in}_2$)의 역전파<br>
  $\displaystyle\frac{\partial \mathcal{E}}{\partial \text {in}_1}=\frac{\partial \mathcal{E}}{\partial \text {out}} \cdot \frac{\partial \text {out}}{\partial \text {in}_1} =\frac{\partial \mathcal{E}}{\partial \mathrm{out}} \cdot \text{in}_2$<br>
    $\displaystyle\frac{\partial \mathcal{E}}{\partial \text {in}_2}=\frac{\partial \mathcal{E}}{\partial \text {out}} \cdot \frac{\partial \text {out}}{\partial \text {in}_2} =\frac{\partial \mathcal{E}}{\partial \mathrm{out}} \cdot \text{in}_1$<br>
<br>
- 덧셈($\text{out} = \sum_i \text{in}_i$)의 역전파<br>
  $\displaystyle\frac{\partial \mathcal{E}}{\partial \text {in}}=\frac{\partial \mathcal{E}}{\partial \text {out}} \cdot \frac{\partial \text {out}}{\partial \text {in}} =\frac{\partial \mathcal{E}}{\partial \mathrm{out}} \cdot 1 = \frac{\partial \mathcal{E}}{\partial \mathrm{out}}$<br>
<br>
- S자 모양 활성함수($\text{out} = \sigma(\text{in})$)<br>
  $\displaystyle\frac{\partial \mathcal{E}}{\partial \text {in}}=\frac{\partial \mathcal{E}}{\partial \text {out}} \cdot \frac{\partial \text {out}}{\partial \text {in}} =\frac{\partial \mathcal{E}}{\partial \mathrm{out}} \cdot \sigma^{\prime}(\text {in}) =\frac{\partial \mathcal{E}}{\partial \text { out }} \cdot[\sigma(\text{in})(1-\sigma(\text{in}))]$<br>
<br>
- 최대화($\displaystyle \text{out} = \max\_{i}\left\{\text{in}\_{i}\right\}$)<br>
  $\displaystyle\frac{\partial \mathcal{E}}{\partial \text {in}}=\frac{\partial \mathcal{E}}{\partial \text {out}} \cdot \frac{\partial \text {out}}{\partial \text {in}} = \begin{cases}\frac{\partial \mathcal{E}}{\partial \text { out }} & \text { if in }\_{i} \text { is max } \\\\\\ 0 & \text { otherwise }\end{cases}$<br>
<br>

#### 벡터의 경우 오류 역전파
- 입력이 벡터고 출력이 스칼라일 경우 출력을 입력에 대해서 미분할 때는 다음과 같음<br>
  $\displaystyle \nabla_{\mathbf{x}} z=\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)^{T} \nabla_{\mathbf{y}} z$<br>
<br>
- 즉, 출력($z$)에 대해 은닉층에서의 결과($\mathbf{y}$)로 미분을 하게 되면 그 결과도 벡터형태<br>
  $\nabla_{y} z=\left[\begin{array}{lll}\frac{\partial z}{\partial y_{1}} & \cdots & \frac{\partial z}{\partial y_{n}}\end{array}\right]^{T} \in \mathbb{R}^{n \times 1}$<br>
<br>
- 따라서 $\displaystyle \frac{\partial \mathbf{y}}{\partial \mathbf{x}}$는 야코비언 형태<br>
  $y=g(x): x \in \mathbb{R}^{m} \mapsto \mathbb{R}^{n} \ni y$에서<br>
  $\begin{aligned} \frac{\partial y}{\partial x} &=\left[\begin{array}{llc}\frac{\partial y}{\partial x_{1}} & \cdots & \frac{\partial y}{\partial x_{m}}\end{array}\right] \\\\\\ &=\left[\begin{array}{ccc}\frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{1}}{\partial x_{m}} \\\\\\ \vdots & \ddots & \vdots \\\\\\ \frac{\partial y_{n}}{\partial x_{1}} & \cdots & \frac{\partial y_{n}}{\partial x_{m}}\end{array}\right] \in \mathbb{R}^{n \times m} \end{aligned}$<br>
<br>
- 정리하면 다음과 같음<br>
  $\begin{aligned}\left(\frac{\partial y}{\partial x}\right)^{T} \nabla_{y} z &=\left[\begin{array}{ccc}\frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{n}}{\partial x_{1}} \\\\\\ \vdots & \ddots & \vdots \\\\\\ \frac{\partial y_{1}}{\partial x_{m}} & \cdots & \frac{\partial y_{n}}{\partial x_{m}}\end{array}\right]\left[\begin{array}{c}\frac{\partial z}{\partial y_{1}} \\\\\\ \vdots \\\\\\ \frac{\partial z}{\partial y_{n}}\end{array}\right] \\\\\\ &=\left[\begin{array}{c}\frac{\partial z}{\partial y_{1}} \frac{\partial y_{1}}{\partial x_{1}}+\cdots+\frac{\partial z}{\partial y_{n}} \frac{\partial y_{n}}{\partial x_{1}} \\\\\\ \vdots \\\\\\ \frac{\partial z}{\partial y_{1}} \frac{\partial y_{1}}{\partial x_{m}}+\cdots+\frac{\partial z}{\partial y_{n}} \frac{\partial y_{n}}{\partial x_{m}}\end{array}\right]=\left[\begin{array}{c}\frac{\partial z}{\partial x_{1}} \\\\\\ \vdots \\\\\\ \frac{\partial z}{\partial x_{m}}\end{array}\right]=\nabla_{x} z \end{aligned}$<br>
<br>
- 결론: 야코비언 행렬과 그레디언트를 곱한 연쇄 법칙으로 얻어서 구함(원리는 동일)

---

### 미니배치 방식
- 한 번에 $t$개의 샘플을 처리($t$는 미니배치 크기)<br>
  - $t = 1$: 확률론적 경사 하강법
  - $t = n$: 배치 경사 하강법
- 미니배치 방식은 보통 $t = 10n ~ 100n$<br>
  - 경사도의 잡음을 줄여주는 효과 때문에 수렴이 빠름
  - GPU를 사용한 병렬처리에도 유리
- **현대 기계 학습은 미니배치 기반의 확률론적 경사 하강법을 표준**

---

### 자동미분연산 실습

#### Autograd
- ```autograd``` 패키지는 텐서의 모든 연산에 대한 자동 미분 제공
- 실행-기반-정의 프레임워크, 코드로 어떻게 작성하여 실행하느냐에 따라 역전파가 정의
- 역전파는 학습 과정의 매 단계마다 달라짐


```python
import torch

print(torch.__version__)
```

    1.10.2


---


```python
# requires_grad = True로 설정하여 x의 연산 과정을 추적
x = torch.ones(2, 2, requires_grad = True)
print(x)

# grad_fn이 추적된 연산을 확인가능
print(x.grad_fn)
```

    tensor([[1., 1.],
            [1., 1.]], requires_grad=True)
    None



```python
# y를 x값의 연산으로 생성
# grad function을 소유하고 있음
y = x + 2
print(y)
```

    tensor([[3., 3.],
            [3., 3.]], grad_fn=<AddBackward0>)



```python
# grad function확인
print(y.grad_fn)
```

    <AddBackward0 object at 0x7fafd99243a0>


---


```python
# 마찬가지로 z와 out에 grad function생성 확인
z = y * y * 3
out = z.mean()

print(z)
print(out)
```

    tensor([[27., 27.],
            [27., 27.]], grad_fn=<MulBackward0>)
    tensor(27., grad_fn=<MeanBackward0>)


---


```python
# requires_grad_()를 사용하면 기존 Tensor의 requires_grad 값을 바꿀 수 있음, 디폴트 값은 False
# requires_grad 속성을 True로 설정하면 해당 텐서에서 이루어진 모든 연산을 추적

# 텐서가 기록을 추적하는 것을 중단하게 하려면 detach()를 호출하여 연산이 추적되는 것을 방지할 수 있음
# 또한 with torch.no_grad(): 코드 블럭으로 기록이 추적되는 것을 방지할 수 잇음
a = torch.randn(2, 2)
print(a)
```

    tensor([[-0.5190, -0.1290],
            [ 2.5750, -0.5917]])



```python
# requires_grad 값을 수정
a = ((a * 3) / (a - 1))
print(a)
print(a.requires_grad)
```

    tensor([[1.0250, 0.3428],
            [4.9048, 1.1152]])
    False



```python
a.requires_grad_(True)
```




    tensor([[1.0250, 0.3428],
            [4.9048, 1.1152]], requires_grad=True)




```python
# 수정된 requires_grad 확인
print(a.requires_grad)
```

    True



```python
b = (a * a).sum()

print(b)

# 마찬가지로 a로부터 생성된 b에도 requires_grad가 True인 것을 확인 가능
print(b.requires_grad)
```

    tensor(26.4685, grad_fn=<SumBackward0>)
    True


---

### 변화도(gradient) 확인


```python
x, y, z
```




    (tensor([[1., 1.],
             [1., 1.]], requires_grad=True),
     tensor([[3., 3.],
             [3., 3.]], grad_fn=<AddBackward0>),
     tensor([[27., 27.],
             [27., 27.]], grad_fn=<MulBackward0>))




```python
x = torch.ones(2, 2, requires_grad = True)
y = x + 2
z = y * y * 3
out = z.mean()
print('out:', out)

# 중간 값에 대한 미분 값 참조시 retain_grad를 호출
y.retain_grad()

# 여러 번 미분을 진행하기 위해서는 retain_graph = True로 설정
out.backward(retain_graph = True)

print('x grad:', x.grad)
print('y grad:', y.grad)
print('z grad:', z.grad)
print('z is leaf?', z.is_leaf)
# print()
# print()

# out.backward()
# print('x grad:', x.grad)
# print('y grad:', y.grad)
```

    out: tensor(27., grad_fn=<MeanBackward0>)
    x grad: tensor([[4.5000, 4.5000],
            [4.5000, 4.5000]])
    y grad: tensor([[4.5000, 4.5000],
            [4.5000, 4.5000]])
    z grad: None
    z is leaf? False


- 위의 계산대로라면 다음과 같음
- $\displaystyle \text{out} = \frac{1}{4} \sum_i z_i$
- $z_i = 3y_i^2 = 3(x_i + 2)^2$
- out을 $x$에 대해 미분하면 $\frac{1}{4}\cdot6(x + 2) = \frac{3}{2}(x + 2) = \frac{3}{2}y$
- 위의 $x, y$에 각각의 값을 대입하면 ```x_grad, y_grad``` 구할 수 있음
---


```python
# torch.autograd는 벡터-야코비언 곱을 계산하는 엔진
# torch.autograd를 사용하면 전체 야코비언을 직접 계산할 수는 없지만
# 벡터-야코비언 곱은 backward에 해당 벡터를 인자로 제공하여 얻을 수 있음
x = torch.randn(3, requires_grad = True)
y = x * 2

while y.data.norm() < 1000:
    y = y * 2

print(y)
```

    tensor([ 618.5011, -771.5411,  494.3506], grad_fn=<MulBackward0>)



```python
# y의 벡터-야코비언 곱을 구하는 과정
v = torch.tensor([0.1, 1.0, 0.0001], dtype = torch.float)
y.backward(v)

# y = (2**n) * x
# x.grad = 1024 * v
print(x.grad)
```

    tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])



```python
# with wotrch.no_grad()로 코드 블록을 감싸서 텐서 연산 기록 추적을 멈추기
print(x.requires_grad)
print((x ** 2 ).requires_grad)
print()

with torch.no_grad():
    print('"with torch.no_grad():"')
    print((x ** 2).requires_grad)
```

    True
    True
    
    "with torch.no_grad():"
    False



```python
# detach()를 호출하여 requires_grad가 다른 새로운 텐서를 가져올 수 있음
print(x.requires_grad)
print()
# 연산 기록 추적이 다른지 확인
y = x.detach()
print(y.requires_grad)
print()
print()
# 내부 원소들은 동일한 것을 확인 가능
print(x.eq(y).all())
```

    True
    
    False
    
    
    tensor(True)


---

### ANN(Artificial Neural Networks)
- 신경망은 ```torch.nn```패키지를 사용하여 생성할 수 있음
- ```nn```은 모델을 저으이하고 미분하기 위해서 위에서 살펴본 ```autograd```를 사용
- ```nn.Module```은 계층(layer)과 ```output```을 반환하는 ```forward(input)``` 메소드를 포함<br>
<br>
- 신경망의 일반적인 학습 과정<br>
  1. 학습 가능한 매개변수(가중치)를 갖는 신경망 정의
  2. 데이터 셋 입력을 반복
  3. 입력을 신경망에서 전파(process)
  4. 손실 게산
  5. 그레디언트를 신경망의 매개변수들에 역으로 전파(역전파 과정)
  6. 신경망의 가중치 갱신(갱신된 가중치 = 가중치 - 학습률 * 그레디언트)


```python
import numpy as np
import pandas as pd

from sklearn.datasets import load_iris

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

import warnings
warnings.filterwarnings(action = 'ignore')
```


```python
class Net(nn.Module):
    
    def __init__(self):
        super(Net, self).__init__()
        
        # 선형함수 5개를 통과
        self.layer0 = nn.Linear(4, 128)
        self.layer1 = nn.Linear(128, 64)
        self.layer2 = nn.Linear(64, 32)
        self.layer3 = nn.Linear(32, 16)
        self.layer4 = nn.Linear(16, 3)
        
        # 배치단위에 있는 값들을 정규화
        # 양극단에 값이 위치하지 않도록 조정
        self.bn0 = nn.BatchNorm1d(128)
        self.bn1 = nn.BatchNorm1d(64)
        self.bn2 = nn.BatchNorm1d(32)
        
        # 비선형함수 ReLU통과
        self.act = nn.ReLU()
        
        
    def forward(self, x):
        # 입력이 layer0을 통과하고 값을 정규화한 후 비선형함수 통과
        x = self.act(self.bn0(self.layer0(x)))
        x = self.act(self.bn1(self.layer1(x)))
        x = self.act(self.bn2(self.layer2(x)))
        x = self.act(self.layer3(x))
        x = self.layer4(x)
    
        return x
```


```python
# 손실함수
# 출력이 정답으로부터 얼마나 떨어져있는지 추정
# 위에서 forward함수를 정의했으므로 backward함수는 autograd를 사용하여 자동으로 정의
# 모델의 학습 가능한 매개변수는 net.parameters()에 의해 변환

# 랜덤 값 생성
# 다중 분류 시 크로스 엔트로피를 손실함수로 사용
criterion = nn.CrossEntropyLoss()

# 입력 값과 타깃 값 정의
ex_X, ex_y = torch.randn([4, 4]), torch.tensor([1, 0, 2, 0])
net = Net()
output = net(ex_X)

# 예측 값과 타깃 값의 오차를 계산
loss = criterion(output, ex_y)
print('loss: ', loss.item())

# 그레디언트를 초기화 > None 출력
net.zero_grad()

print('layer0.bias.grad before backward')
# 그레디언트가 초기화 되었으므로 None 출력
print(net.layer4.bias.grad)

print(net.layer4.bias.is_leaf)
loss.backward()

print('layer0.bias.grad after backward')
print(net.layer4.bias.grad)
```

    loss:  1.22153639793396
    layer0.bias.grad before backward
    None
    True
    layer0.bias.grad after backward
    tensor([-0.2178,  0.1727,  0.0451])


---


```python
params = list(net.parameters())
print(len(params))

# layer0의 weight
print(params[0].size())

# layer0은 4개의 입력을 128개의 출력으로 변환
# 최초 4x4 행렬이 입력으로 들어와서 128개의 병렬적인 계산을 통해 128개의 출력이 생성
# layer0에서 파라미터 행렬의 크기는 128x4
```

    16
    torch.Size([128, 4])


---


```python
# 가중치 갱신
# 가장 단순한 갱신 규칙: SGD(확률적 경사하강법)
# 새로운 가중치 = 가중치 - 학습률 * 변화도

import torch.optim as optim

optimizer = optim.SGD(net.parameters(), lr = 0.001)

optimizer.zero_grad()
output = net(ex_X)
loss = criterion(output, ex_y)
loss.backward()

# 업데이트 진행
optimizer.step()
```

---


```python
# iris 데이터 불러오기

dataset = load_iris()

data = dataset.data
label = dataset.target

# 속성들을 출력
print(dataset.DESCR)
```

    .. _iris_dataset:
    
    Iris plants dataset
    --------------------
    
    **Data Set Characteristics:**
    
        :Number of Instances: 150 (50 in each of three classes)
        :Number of Attributes: 4 numeric, predictive attributes and the class
        :Attribute Information:
            - sepal length in cm
            - sepal width in cm
            - petal length in cm
            - petal width in cm
            - class:
                    - Iris-Setosa
                    - Iris-Versicolour
                    - Iris-Virginica
                    
        :Summary Statistics:
    
        ============== ==== ==== ======= ===== ====================
                        Min  Max   Mean    SD   Class Correlation
        ============== ==== ==== ======= ===== ====================
        sepal length:   4.3  7.9   5.84   0.83    0.7826
        sepal width:    2.0  4.4   3.05   0.43   -0.4194
        petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)
        petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)
        ============== ==== ==== ======= ===== ====================
    
        :Missing Attribute Values: None
        :Class Distribution: 33.3% for each of 3 classes.
        :Creator: R.A. Fisher
        :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)
        :Date: July, 1988
    
    The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken
    from Fisher's paper. Note that it's the same as in R, but not as in the UCI
    Machine Learning Repository, which has two wrong data points.
    
    This is perhaps the best known database to be found in the
    pattern recognition literature.  Fisher's paper is a classic in the field and
    is referenced frequently to this day.  (See Duda & Hart, for example.)  The
    data set contains 3 classes of 50 instances each, where each class refers to a
    type of iris plant.  One class is linearly separable from the other 2; the
    latter are NOT linearly separable from each other.
    
    .. topic:: References
    
       - Fisher, R.A. "The use of multiple measurements in taxonomic problems"
         Annual Eugenics, 7, Part II, 179-188 (1936); also in "Contributions to
         Mathematical Statistics" (John Wiley, NY, 1950).
       - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.
         (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.
       - Dasarathy, B.V. (1980) "Nosing Around the Neighborhood: A New System
         Structure and Classification Rule for Recognition in Partially Exposed
         Environments".  IEEE Transactions on Pattern Analysis and Machine
         Intelligence, Vol. PAMI-2, No. 1, 67-71.
       - Gates, G.W. (1972) "The Reduced Nearest Neighbor Rule".  IEEE Transactions
         on Information Theory, May 1972, 431-433.
       - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al"s AUTOCLASS II
         conceptual clustering system finds 3 classes in the data.
       - Many, many more ...


---


```python
print('shape of data: ', data.shape)
print('shape of label: ', label.shape)
```

    shape of data:  (150, 4)
    shape of label:  (150,)



```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(data, label, test_size = 0.25)
print(len(X_train))
print(len(X_test))
```

    112
    38



```python
# DataLoader 생성
# 배치 크기, 셔플을 지정
X_train = torch.from_numpy(np.array(X_train)).float()
y_train = torch.from_numpy(np.array(y_train)).long()

X_test = torch.from_numpy(np.array(X_test)).float()
y_test = torch.from_numpy(np.array(y_test)).long()

train_set = TensorDataset(X_train, y_train)

train_loader = DataLoader(train_set, batch_size = 4, shuffle=True)
```


```python
net = Net()
print(net)
```

    Net(
      (layer0): Linear(in_features=4, out_features=128, bias=True)
      (layer1): Linear(in_features=128, out_features=64, bias=True)
      (layer2): Linear(in_features=64, out_features=32, bias=True)
      (layer3): Linear(in_features=32, out_features=16, bias=True)
      (layer4): Linear(in_features=16, out_features=3, bias=True)
      (bn0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act): ReLU()
    )



```python
# SGD로 학습
# 크로스 엔트로피를 손실함수로 사용
# 200번 반복
optimizer = torch.optim.SGD(net.parameters(), lr = 0.001)
criterion = nn.CrossEntropyLoss()
epochs = 200
```


```python
losses = list()
accuracies = list()

for epoch in range(epochs):
    epoch_loss = 0
    epoch_accuracy = 0
    for X, y in train_loader:
        optimizer.zero_grad()
        output = net(X)
        
        loss = criterion(output, y)
        loss.backward()
        
        optimizer.step()
        # output = [0.11, 0.5, 0.8] > 예측 클래스 값은 2
        _, predicted = torch.max(output, dim = 1)
        accuracy = (predicted == y).sum().item()
        epoch_loss += loss.item()
        epoch_accuracy += accuracy
        

    epoch_loss /= len(train_loader)
    epoch_accuracy /= len(X_train)
    if not (epoch + 1) % 20 or epoch == 0:
        print(f"epoch :{str(epoch + 1).zfill(3)}")
        print(f"loss :{round(epoch_loss, 4)}, \t accuracy :{round(epoch_accuracy, 4)}")
        print()
    losses.append(epoch_loss)
    accuracies.append(epoch_accuracy)
```

    epoch :001
    loss :0.2911, 	 accuracy :0.8839
    
    epoch :020
    loss :0.2357, 	 accuracy :0.875
    
    epoch :040
    loss :0.4454, 	 accuracy :0.8482
    
    epoch :060
    loss :0.3199, 	 accuracy :0.8839
    
    epoch :080
    loss :0.1863, 	 accuracy :0.9375
    
    epoch :100
    loss :0.2827, 	 accuracy :0.8839
    
    epoch :120
    loss :0.285, 	 accuracy :0.8929
    
    epoch :140
    loss :0.1951, 	 accuracy :0.9107
    
    epoch :160
    loss :0.2331, 	 accuracy :0.9196
    
    epoch :180
    loss :0.206, 	 accuracy :0.9286
    
    epoch :200
    loss :0.3854, 	 accuracy :0.8571
    


정확도와 손실함수가 에포크가 진행됨에따라 진동하는 모습을 확인할 수 있음

---


```python
# 테스트
output = net(X_test)

# 38개의 텐서에서 가장 높은 값으로 얻어진 클래스 값
print(torch.max(output, dim = 1))

# 가장 높은 값에 해당하는 클래스 매칭
_, predicted = torch.max(output, dim=1)
accuracy = round((predicted == y_test).sum().item() / len(y_test), 4)

# 정확도 출력
print("test_set accuracy :", round(accuracy, 4))
```

    torch.return_types.max(
    values=tensor([3.0076, 1.7995, 2.2529, 3.9314, 4.7372, 3.5602, 3.3414, 2.7488, 2.8679,
            3.4966, 3.9686, 3.3054, 4.6465, 2.1990, 3.2504, 4.6892, 2.3049, 2.3095,
            3.4408, 2.2425, 3.3068, 3.0878, 1.7934, 3.0567, 2.7407, 1.9649, 3.0846,
            3.5001, 3.3602, 2.7217, 1.4416, 3.3578, 4.3132, 2.7993, 2.9893, 4.0877,
            5.2600, 4.3767], grad_fn=<MaxBackward0>),
    indices=tensor([1, 1, 0, 2, 2, 0, 0, 1, 1, 1, 2, 0, 2, 0, 0, 2, 1, 2, 0, 0, 0, 0, 2, 0,
            1, 0, 2, 2, 2, 2, 0, 0, 1, 0, 0, 1, 2, 0]))
    test_set accuracy : 0.9737


---

### 느낀점
다층 퍼셉트론에 대해서 알아보았는데 다양한 예제와 시각화 자료를 통해서 쉽게 이해할 수 있었다.<br>
또한 역전파 알고리즘이 어떻게 이루어지는지 잘 알게 되었으나<br>
실습과 이론은 서로 다른 영역이라는 것을 알 수 있었다.<br>
반복되는 실습을 통해서 코드작성에 익숙해져야할 것 같고 많은 경험을 쌓아야할 것 같다
