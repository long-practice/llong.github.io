---
title: 프로그래머스 인공지능 데브코스 48일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 10주차(22.2.14.(월) ~ 22.2.18.(토)) 
  선형회귀
  선형분류
  Weekly Mission
tags:
- 순차 데이터
- 단어 가방
- one-hot 코드
- 단어 임베딩
- 순환 신경망(RNN)
- 시간성
- 가변 길이
- 문맥 의존성
- RNN 구조
- RNN 동작
- 그레디언트 계산
- BPTT
- LSTM
- 장기문맥 의존성
- 개폐구(Gate)
- LSTM 동작
- 언어 모델
- n-그램
- 기계 번역
- 영상 주석 생성

use_math: True
---
# 48일차(신경망 기초 - RNN)

10주차(22.2.14.(월) ~ 22.2.18.(금)): CNN & RNN
* Deep Learning 기초
* CNN Models
* Deep Learning 최적화
* RNN
* weekly mission
<br>
<br>

출처: 프로그래머스 인공지능 데브코스 3기 강의

### 순차 데이터
- 시간성 데이터(Time Series)라고도 부름
- 특징이 순서를 가짐(현재까지는 어느 한 시점에 취득한 정적인 데이터, 고정 길이 데이터)
- 동적이며 가변적인 특성이 있음
- 온라인 숫자, 심전도 신호 등등


#### 응용사례

- 심전도 신호 분석하여 심장 이상 유무 판정
- 주식 시세를 분석하여 사고 파는 시점 결정
- 음성 인식을 통한 지능적인 인터페이스 구축
- 기계 번역기 또는 자동 응답 장치 제작
- 유전자 열 분석을 통한 치료계획 수립 등<br>
<br>

---

### 순차 데이터의 표현
- 벡터의 벡터(벡터의 요소가 벡터)<br>
  $\mathbf{x} = \bigl(\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \cdots, \mathbf{x}^{(T)} \bigr)$<br>
<br>
- 온라인 숫자의 요소는 1차원 벡터, 심전도 요소는 3차원 벡터<br>
  만약 심전도 신호를 초당 80번 샘플링하고 3분간 측정한다면 그 길이는 80 * 60 * 3 = 14,400<br>
<br>

- 훈련집합 $\mathbb{X} = \\{\mathbf{x}\_1, \mathbf{x}\_2, \cdots, \mathbf{x}\_n\\}, \mathbb{Y} = \\{\mathbf{y}\_1, \mathbf{y}\_2, \cdots, \mathbf{y}\_n\\}$이 되겠고,<br>
  각 샘플은 $\mathbf{x} = \bigl(\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \cdots, \mathbf{x}^{(T)} \bigr)^T, \mathbf{y} = \bigl(\mathbf{y}^{(1)}, \mathbf{y}^{(2)}, \cdots, \mathbf{y}^{(L)} \bigr)^T$<br>
<br>

- 대표적인 순차 데이터인 문자열을 표현할 때는 사전을 사용하여 표현
- 사전 구축 방법: 사람이 사용하는 단어를 모아 구축 또는 주어진 말뭉치를 분석하여 단어를 자동 추출, 구축
- 사전을 사용한 텍스트 순차 데이터의 표현 방법: **단어가방, one-hot 코드(one-hot 벡터), 단어 임베딩(가장 많이 사용)**<br>
<br>

#### 단어 가방
- 단어 각각의 빈도수를 세어 $m$차원 벡터로 표현($m$은 사전 크기)
- 한계: 정보 검색에 주로 사용되지만, 기계 학습에는 부적절(단어의 순서가 바뀌는 것에 취약)<br>
<br>

#### one-hot 코드
- 해당 단어의 위치만 1로 표시
- 한계: 한 단어를 표현하는데 $m$차원 벡터를 사용하는 비효율적이고, **단어 간의 유사성을 측정할 수 없음**<br>
<br>

#### 단어 임베딩
- 순차 데이터 표현 시 주로 많이 사용하는 방식
- 단어 사이의 상호작용을 분석, 새로운 공간으로 변환($m$차원보다 훨씬 낮은 차원으로 변환)
- 즉, 단어의 빈도수, 유사성을 이용하여 낮은 차원으로 투영시킴
- 예를 들면 worde2vec은 $m$ =30000차원을 620차원으로 변환
- 변환 과정은 학습이 말뭉치를 훈련집합으로 사용하여 알아냄<br>
<br>

---

### 순차 데이터의 특성
- 특징이 나타나는 순서가 중요("아버지가 방에 들어가신다." vs "아버지 가방에 들어가신다.")
- 비순차 데이터에서는 순서를 바꿔도 무방<br>
<br>

- 샘플마다 길이가 다름, 순환 연결을 통해 가변 길이를 수용하는 특성

- 문맥 의존성
- 비순차 데이터는 공분산이 특징 사이의 의존성을 나타냄
- 순차 데이터에서는 공부산은 의미가 없고, 대신 문맥 의존성이 중요
- 장기 의존성은 LSTM으로 처리하는 것이 더 효율적

### 순환 신경망(Recurrent Neural Network)
- 순환 신경망은 시간성 정보를 활용하여 순차 데이터를 처리하는 효과적인 학습 모델
- 매우 긴 순차 데이터(예를 들어 30단어 이상 긴 문장) 처리에는 장기 의존성(데이터들의 의존성)을 잘 다루는 LSTM을 주로 사용
- LSTM은 선별 기억 능력을 가짐
- 최근에는 순환 신경망도 생성 모델로 사용(CNN과 LSTM이 협력하여 자연 영상에 주석을 생성)<br>
<br>

#### 순환 신경망이 갖춰야할 세 가지 필수 기능
1. 시간성: 특징을 순서대로 한 번에 하나씩 입력
2. 가변 길이: 길이가 $T$인 샘플을 처리하려면 은닉층이 $T$번 등장, $T$는 가변적
3. 문맥 의존성: 이전 특징 내용을 기억하고 있다가 적절한 순간에 활용<br>
<br>

#### 구조
- 깊은 신경망과 유사(입력층, 은닉층, 출력층)
- 그러나 은닉층에서 순환 연결을 가짐($t - 1$순간에 발생한 정보를 $t$순간으로 전달)
- 시간성 가변 길이, 문맥 의존성 모두 처리 가능
![1.png](attachment:1.png)<br>
<br>


![2.png](attachment:2.png)

- 수식으로 표현($\Theta$는 순환 신경망의 매개변수)<br>
  $\mathbf{h}^{(t)} = f(\mathbf{h}^{(t-1)}, \mathbf{x}^{(t)}; \Theta)$
- $t = 1$순간에 계산, 그 결과를 가지고 $t =2$순간에 계산, 그 결과를 가지고 $t =3$순간에 계산 ...<br>
  다시 말해서 $t$순간에 $t - 1$순간의 은닉층 값(상태) $\mathbf{h}^{(t-1)}$과 $t$순간의 입력 $\mathbf{x}^{(t)}$를 받아 $\mathbf{h}^{(t)}$로 전환<br>
- $t$시점에서의 출력은 $t$시점의 입력에도 영향을 받지만 이전 층의 출력과의 연결 연산을 통해 입력이 됨<br>
  즉, 이전 층들 모두의 영향을 받기 때문에 문맥 의존성을 갖출 수 있음<br>
<br>
- 이 과정을 조금 더 세부적으로 수식으로 표현하면<br>
  $\begin{aligned} \mathbf{h}^{(T)} &=f\left(\mathbf{h}^{(T-1)}, \mathbf{x}^{(T)} ; \Theta\right) \\\\\\ &=f\left(f\left(\mathbf{h}^{(T-2)}, \mathbf{x}^{(T-1)} ; \Theta\right), \mathbf{x}^{(T)} ; \Theta\right) \\\\\\ &=f\left(f\left(\cdots f\left(\mathbf{h}^{(1)}, \mathbf{x}^{(2)} ; \Theta\right), \cdots, \mathbf{x}^{(T-1)} ; \Theta\right), \mathbf{x}^{(T)} ; \Theta\right) \\\\\\ &=f\left(f\left(\cdots f\left(f\left(\mathbf{h}^{(0)}, \mathbf{x}^{(1)} ; \Theta\right), \mathbf{x}^{(2)} ; \Theta\right), \cdots, \mathbf{x}^{(T-1)} ; \Theta\right), \mathbf{x}^{(T)} ; \Theta\right) \end{aligned}$<br>
  이전 층들의 입력이 영향을 미치는 것을 확인할 수 있음<br>
<br>

- 순환 신경망의 매개변수(가중치 집합) $\Theta = \{\mathbf{U, W, V, b, c}\}$<br>
  - $\mathbf{U}$는 입력층과 은닉층을 연결하는 $p \times d$행렬
  - $\mathbf{W}$는 은닉층과 은닉층을 연결하는 $p \times p$행렬
  - $\mathbf{V}$는 은닉층과 출력층을 연결하는 $q \times p$행렬
  - $\mathbf{b, c}$는 바이어스 $p \times 1, q \times 1$행렬<br>
  $\rightarrow$ **RNN학습의 목적: 훈련집합을 최적의 성능으로 예측하는 $\Theta$를 찾는 것**<br>
<br>

- 각 층에서 같은 매개변수를 공유하는 특징이 있음<br>
  - 추정할 매개변수 수가 크게 줄어듦
  - 매개변수 수는 특징 벡터 길이 $T$에 무관
  - 특징이 나타나는 순간 순서가 바뀌어도 유사한 출력<br>
    ("아버지가 방에 들어가신다" vs "방에 아버지가 들어가신다")<br>
<br>
    
- 입력과 출력의 개수에 따라 여러가지 RNN 구조가 발생<br>
  one to one, one to many, many to one, many to many, ...<br>
  다대일과 일대다의 조합(encoder, decoder) 등등<br>
![3.png](attachment:3.png)<br>
<br>

- 그러나 한방향으로 정보가 흐르는 단방향 RNN은 한계가 있음(글씨체가 비슷한 단어(거지 vs 지지))
- 이를 해결하기 위해 양방향 RNN을 사용하고 앞쪽 단어와 뒤쪽 단어 정보를 모두 보고 처리
- 기계 번역세더도 BRNN을 주로 사용<br>
<br>

#### 동작
- RNN동작도 MLP의 동작과 유사하게 입력층 - 은닉층 - 출력층 계산으로 진행<br>
<br>

- 입력층과 은닉층을 연결하는 가중치 행렬$\mathbf{U}$에서 $\mathbf{u}\_j$는 $\mathbf{U}$행렬의 $j$번째 행<br>
  즉, $j$번째 은닉층 $h_j$에 연결된 선들의 가중치들을 의미
- 마찬가지로 은닉층과 은닉층을 연결하는 가충치 행렬$\mathbf{W}$에서도 동일<br>
<br>
- $t$번째 은닉층의 계산(이전 층의 은닉층의 연산 결과 + 현재 층에서의 입력)은 다음과 같음<br>
  $a_j^{(t)} = \mathbf{w}\_j\mathbf{h}^{(t-1)} + \mathbf{u}\_j\mathbf{x}^(t) + b_j, \quad j = 1, 2, \cdots, p$<br>
  $h_j^{(t)} = \tau\bigl(a_j^{(t)}\bigr)$<br>
<br>
- 행렬 표기로 쓰면<br>
  $\mathbf{a}^{(t)}=\mathbf{W h}^{(t-1)}+\mathbf{U} \mathbf{x}^{(t)}+\mathbf{b}$<br>
  $\mathbf{h}^{(t)}=\tau\left(\mathbf{a}^{(t)}\right)$<br>
<br>

- 은닉층 계산 후 출력층에서 계산<br>
  $\mathbf{o}^{(t)}=\mathbf{V h}^{(t)}+\mathbf{c}$<br>
  $\mathbf{y}^{\prime(t)}=\operatorname{softmax}\left(\mathbf{o}^{(t)}\right)$<br>
<br>

- 만약 $\mathbf{x}^{(1)}$이 변하면 $\mathbf{h}^{(1)},\mathbf{h}^{(2)}\mathbf{h}^{(3)} ...$이 바뀌고, 이에 따라 출력 $\mathbf{y}^{\prime(1)},\mathbf{y}^{\prime(2)}, \mathbf{y}^{\prime(3)}, ...$도 바뀜<br>
  이것은 RNN이 각 층에서의 입력을 기억한다고 볼 수 있음
- 그리고 각각의 입력이 상호작용한다고 볼 수 있기 때문에 문맥 의존성도 있다고 볼 수 있음<br>
<br>

#### RNN vs DMLP
- RNN은 샘플마다 은닉층의 수가 다름
- DMLP는 왼쪽에 입력, 오른쪽에 출력(입력 후 출력) vs RNN은 매 순간 입력과 출력이 있음
- RNN은 가중치를 공유

---

<br>

### BPTT(BackPropagation Through Time)
- 출력 값: $\mathbf{y}^{\prime} = \mathbf{y}^{\prime(1)},\mathbf{y}^{\prime(2)}, \cdots, \mathbf{y}^{\prime(T)}$, 목표값: $\mathbf{y} = \mathbf{y}^{(1)},\mathbf{y}^{(2)}, \cdots, \mathbf{y}^{(T)}$
- 목적함수: $\displaystyle J(\boldsymbol{\Theta})=\sum_{t=1}^{T} J^{(t)}(\boldsymbol{\Theta})$
- 오차<br>
  - MSE: $\displaystyle J^{(t)}(\boldsymbol{\Theta})=\sum_{j=1}^{q}(y_{j}^{(t)}-y_{j}^{\prime(t)})^{2}$
  - 크로스 엔트로피: $\displaystyle J^{(t)}(\boldsymbol{\Theta})=-\mathbf{y}^{(t)} \log \mathbf{y}^{\prime(t)}=-\sum_{j=1}^{q} y_{j}^{(t)} \log y_{j}^{\prime(t)}$
  - 로그 우도: $J^{(t)}(\boldsymbol{\Theta})=-\log y^{\prime(t)}$<br>
<br>
- 학습의 목표: $\displaystyle \widehat{\boldsymbol{\Theta}}=\underset{\boldsymbol{\Theta}}{\operatorname{argmin}} J(\boldsymbol{\Theta})=\underset{\boldsymbol{\Theta}}{\operatorname{argmin}} \sum_{t=1}^{T} J^{(t)}(\boldsymbol{\Theta})$<br>
<br>

#### 그레디언트 계산
- 시간적인 요소도 반드시 고려해야 함(단순히 시점 $t$만을 고려하면 안됨)
- 목표값 $\mathbf{y} = (1, 0)^T$ 또는 $(0, 1)^T$일 때
- $\displaystyle \frac{\partial J}{\partial \Theta}$ 를 구하려면, $\left.\Theta=\\{\mathbf{U}, \mathbf{W}, \mathbf{V}, \mathbf{b}, \mathbf{c}\right\\}$ 이므로 $\displaystyle \frac{\partial J}{\partial \mathbf{U}}, \frac{\partial J}{\partial \mathbf{W}}, \frac{\partial J}{\partial \mathbf{V}}, \frac{\partial J}{\partial \mathbf{b}}, \frac{\partial J}{\partial \mathbf{c}}$ 를 계산<br>
<br><br>

#### 그레디언트 계산 - 출력층
- 우선 $\mathbf{V}$는 $q \times p$ 행렬 $\rightarrow$ $\displaystyle \frac{\partial J}{\partial \mathbf{V}}$ 또한 동일<br>
  $\displaystyle \frac{\partial J}{\partial \mathbf{V}}=\left(\begin{array}{cccc}\frac{\partial J}{\partial v_{11}} & \frac{\partial J}{\partial v_{12}} & \cdots & \frac{\partial J}{\partial v_{1 p}} \\\\\\ \frac{\partial J}{\partial v_{21}} & \frac{\partial J}{\partial v_{22}} & \cdots & \frac{\partial J}{\partial v_{2 p}} \\\\\\ \vdots & \vdots & \ddots & \vdots \\\\\\ \frac{\partial J}{\partial v_{q 1}} & \frac{\partial J}{\partial v_{q 2}} & \cdots & \frac{\partial J}{\partial v_{q p}}\end{array}\right)$<br>
<br>
- 오차를 표현하기 위해 로그우도를 선택하고, $v_{12}$를 미분(연쇄법칙 적용)<br>
  $\displaystyle \frac{\partial J^{(t)}}{\partial v_{12}}=\frac{\partial J^{(t)}}{\partial y^{\prime(t)}} \frac{\partial y^{\prime(t)}}{\partial o_{1}^{(t)}} \frac{\partial o_{1}^{(t)}}{\partial v_{12}}$<br>
- $o_{1}^{(t)}=\mathbf{v}\_{1} \mathbf{h}^{(t)}=v_{11} h_{1}^{(t)}+v_{12} h_{2}^{(t)}+v_{13} h_{3}^{(t)}$ 이므로 $\displaystyle \frac{\partial o_{1}^{(t)}}{\partial v_{12}}=h_{2}^{(t)}$<br>
<br>
- $\mathbf{y}^{(t)} = (1, 0)^T$인 경우 $J^{(t)}=-\log y_{1}^{\prime(t)}$에서 <br>
  $\begin{aligned} \frac{\partial J^{(t)}}{\partial y^{\prime(t)}} & \frac{\partial y^{\prime(t)}}{\partial o_{1}^{(t)}}=\frac{\partial J^{(t)}}{\partial o_{1}^{(t)}}=\frac{\partial\left(-\log \frac{\exp \left(o_{1}^{(t)}\right)}{\exp \left(o_{1}^{(t)}\right)+\exp \left(o_{2}^{(t)}\right)}\right)}{\partial o_{1}^{(t)}} \\\\\\ &=\frac{\partial\left(-o_{1}^{(t)}+\log \left(\exp \left(o_{1}^{(t)}\right)+\exp \left(o_{2}^{(t)}\right)\right)\right)}{\partial o_{1}^{(t)}} \\\\\\ &=-1+\frac{\exp \left(o_{1}^{(t)}\right)}{\exp \left(o_{1}^{(t)}\right)+\exp \left(o_{2}^{(t)}\right)} \\\\\\ &=-1+y_{1}^{\prime(t)} \end{aligned}$<br>
<br>
- 마찬가지 방법으로 $\mathbf{y}^{(t)} = (0, 1)^T$인 경우 <br>
  $\begin{aligned} \frac{\partial J^{(t)}}{\partial y^{\prime(t)}} & \frac{\partial y^{\prime(t)}}{\partial o_{1}^{(t)}}=\frac{\partial J^{(t)}}{\partial o_{1}^{(t)}}=\frac{\partial\left(-\log \frac{\exp \left(o_{2}^{(t)}\right)}{\exp \left(o_{1}^{(t)}\right)+\exp \left(o_{2}^{(t)}\right)}\right)}{\partial o_{1}^{(t)}} \\\\\\ &=\frac{\partial\left(\log \left(\exp \left(o_{1}^{(t)}\right)+\exp \left(o_{2}^{(t)}\right)\right)\right)}{\partial o_{1}^{(t)}} \\\\\\ &=\frac{\exp \left(o_{1}^{(t)}\right)}{\exp \left(o_{1}^{(t)}\right)+\exp \left(o_{2}^{(t)}\right)} \\\\\\ &=y_{1}^{\prime(t)} \end{aligned}$<br>
<br>

- 따라서 $\displaystyle \frac{\partial J^{(t)}}{\partial v_{12}} $는 다음의 두 경우로 표현 가능<br>
  $\left.\begin{array}{l} \left(y_{1}^{\prime(t)}-1\right) h_{2}^{(t)}, & \mathbf{y}^{(t)}=(1,0)^{\mathrm{T}}\\\\\\ y_{1}^{\prime(t)} h_{2}^{(t)}, 
 & \mathbf{y}^{(t)}=(0,1)^{\mathrm{T}}\end{array}\right\\}$<br>
<br>

- 위에서 구한 식을 간결하게, 일반화하여 표현하면 다음과 같음<br>
  $\displaystyle \frac{\partial J^{(t)}}{\partial v_{j i}}=(y_{j}^{\prime(t)}-y_{j}^{(t)}) h_{i}^{(t)}$<br>
  $\displaystyle \frac{\partial J}{\partial v_{j i}}=\sum_{t=1}^{T}(y_{j}^{\prime(t)}-y_{j}^{(t)}) h_{i}^{(t)}$<br>
<br>
<br>


#### 그레디언트 계산 - 은닉층
- 마지막 순간 $T$에서의 그레디언트<br>
  $\displaystyle \frac{\partial J^{(T)}}{\partial \mathbf{h}^{(T)}}=\frac{\partial J^{(T)}}{\partial \mathbf{o}^{(T)}} \frac{\partial \mathbf{o}^{(T)}}{\partial \mathbf{h}^{(T)}}= \mathbf{V}^{\mathrm{T}} \frac{\partial J^{(T)}}{\partial \mathbf{o}^{(T)}}$ 이 때 $\mathbf{o}^{(t)}=\mathbf{V h}^{(t)}+\mathbf{c}$<br>
<br>
- $T-1$에서의 그레디언트<br>
  $\displaystyle \frac{\partial(J^{(T-1)}+J^{(T)})}{\partial \mathbf{h}^{(T-1)}} =\frac{\partial J^{(T-1)}}{\partial \mathbf{o}^{(T-1)}} \frac{\partial \mathbf{o}^{(T-1)}}{\partial \mathbf{h}^{(T-1)}}+\frac{\partial \mathbf{h}^{(T)}}{\partial \mathbf{h}^{(T-1)}} \frac{\partial J^{(T)}}{\partial \mathbf{h}^{(T)}} =\mathbf{V}^{\mathrm{T}} \frac{\partial J^{(T-1)}}{\partial \mathbf{o}^{(T-1)}}+\mathbf{W}^{\mathrm{T}} \frac{\partial J^{(T)}}{\partial \mathbf{h}^{(T)}} \mathbf{D}\bigl(1-(\mathbf{h}^{(T)})^{2}\bigr) $<br>
<br>

- 위의 두 식을 일반화<br>
  이 때, $J^{(\tilde{t})}$는 $t$를 포함하여 이후 목적함수의 값을 모두 더한 값, $J^{(\tilde{t})}=J^{(t)}+J^{(t+1)}+\cdots+J^{(T)}$<br>
  $\displaystyle \frac{\partial J^{(t)}}{\partial \mathbf{h}^{(t)}}=\mathbf{V}^{\mathrm{T}} \frac{\partial J^{(t)}}{\partial \mathbf{o}^{(t)}}+\mathbf{W}^{\mathrm{T}} \frac{\partial J^{(t+1)}}{\partial \mathbf{h}^{(t+1)}} \mathbf{D}\bigl(1-(\mathbf{h}^{(t+1)})^{2}\bigr)$<br>

![4.png](attachment:4.png)

#### 그레디언트 계산 결과
- $v_{ji}$를 미분하는 식에서 $\displaystyle \frac{\partial J}{\partial \mathbf{V}}$를 구하고 위 식을 확장하여 $\displaystyle \frac{\partial J}{\partial \mathbf{U}}, \frac{\partial J}{\partial \mathbf{W}}, \frac{\partial J}{\partial \mathbf{b}}, \frac{\partial J}{\partial \mathbf{c}}$을 계산<br>
<br>

- $\displaystyle \frac{\partial J}{\partial \mathbf{V}}=\sum_{t=1}^{T} \frac{\partial J^{(t)}}{\partial \mathbf{o}^{(t)}} \mathbf{h}^{(t)^{\mathrm{T}}}$<br>
<br>

- $\displaystyle  \frac{\partial J}{\partial \mathbf{W}}=\sum_{t=1}^{T} \mathbf{D}\bigl(1-(\mathbf{h}^{(t)})^{2}\bigr) \frac{\partial J^{(\tilde{t})}}{\partial \mathbf{h}^{(t)}} \mathbf{h}^{(t-1)^{\mathrm{T}}}$<br>
<br>

- $\displaystyle \frac{\partial J}{\partial \mathbf{U}}=\sum_{t=1}^{T} \mathbf{D}\bigl(1-(\mathbf{h}^{(t)})^{2}\bigr) \frac{\partial J^{(\tilde{t})}}{\partial \mathbf{h}^{(t)}} \mathbf{x}^{(t)^{\mathrm{T}}}$<br>
<br>

- $\displaystyle \frac{\partial J}{\partial \mathbf{c}}=\sum_{t=1}^{T} \frac{\partial J^{(t)}}{\partial \mathbf{o}^{(t)}}$<br>
<br>

- $\displaystyle  \frac{\partial J}{\partial \mathbf{b}}=\sum_{t=1}^{T} \mathbf{D}\bigl(1-(\mathbf{h}^{(t)})^{2}\bigr) \frac{\partial J^{(\tilde{t})}}{\partial \mathbf{h}^{(t)}}$

---

### LSTM

#### 장기 문맥 의존성
- 관련된 요소가 멀리 떨어져있을 때, 즉, 한 문장에서 어떤 단어와 연관있는 단어가 멀리 떨어져 있는 상황
- 경사 소멸 문제($\mathbf{W}$ 요소가 1보다 작을 때) 혹은 경사 폭발 문제($\mathbf{W}$ 요소가 1보다 클 때) 발생
- 긴 입력 샘플, 가중치 공유라는 특성 때문에 DMLP나 CNN보다 RNN 더욱 심각하게 경사 소멸, 폭발문제가 발생<br>
<br>

#### 개폐구(gate)
- 개폐구를 열면 신호가 흐르고 닫으면 차단
- 선택적으로 입력(입력 개폐구)받고 출력(출력 개폐구)하는 역할
- 실제로 $[0, 1]$ 사이의 실수 값으로 개폐 정도를 조절 $\rightarrow$ 학습으로 이 값을 알아냄<br>
<br>

#### LSTM의 핵심 요소
- 메모리 블록(셀): 은닉 상태 장기 기억
- 망각 개폐구(1: 유지, 0: 제거): 기억 유지 혹은 제거 $\rightarrow$ 아래 그림에서 가장 왼쪽 로지스틱 시그모이드 함수와 연결되어 있는 게이트
- 입력 개폐구, 출력 개폐구: 입력, 출력 연산 $\rightarrow$ 아래 그림에서 각각 가운데, 오른쪽 로지스틱 시그모이드 함수와 연결되어 있는 게이트

![5.png](attachment:5.png)<br>
<br>


#### 동작(그림 참고)
- $f_t$의 값은 이전 은닉층과 현재 입력층에 의해 결정(0 ~ 1사이의 값)
- **이 값과 현재 메모리에 저장되어 있는 값과 유지시킬지 제거할지 결정**<br>
<br>
- $\tilde{c}_t$는 과거의 출력 값들과 현재 입력의 비선형 연산 값을 $f_t$와 마찬가지로 이전 은닉층과 현재 입력층에 의해 얼마나 입력값을 선택적 입력시킬지 결정
- 따라서 현재 중요하다고 생각되는 값이라면 이전에 저장되었던 값 $c_{t-1}$과 더하여 저장($c_t$)
- 출력 또한 과거의 출력과 현재 입력에 의해 얼마나 내보낼지 결정
- 위와 같은 이유로 장기 문맥 의존성이 보존<br>
<br>
- 결론적으로 출력 게이트와 입력 게이트의 값이 1.0으로 고정되면 RNN 동작과 동일
- **그러나 이들 값은 가중치와 신호 값에따라 결정, 개폐정도를 조절(RNN과 차이)**

![6.png](attachment:6.png)

- 입력단, 입력 개폐구, 출력 개폐구에서 계산<br>
  - 입력단: $g=\tau_{g}\left(\mathbf{u}\_{j}^{g} \mathbf{x}^{(t)}+\mathbf{w}\_{j}^{g} \mathbf{h}^{(t-1)}+b_{j}^{g}\right) \rightarrow$ 이 때 활성함수는 $\tanh$
  - 입력 게이트: $i=\tau_{f}\left(\mathbf{u}\_{j}^{i} \mathbf{x}^{(t)}+\mathbf{w}\_{j}^{i} \mathbf{h}^{(t-1)}+b_{j}^{i}\right) \rightarrow$ 이 때 활성함수는 logistic sigmoid
  - 출력 게이트: $o=\tau_{f}\left(\mathbf{u}\_{j}^{o} \mathbf{x}^{(t)}+\mathbf{w}\_{j}^{o} \mathbf{h}^{(t-1)}+b_{j}^{o}\right) \rightarrow$ 이 때 활성함수는 logistic sigmoid<br>
<br>
- 아래쪽 기호 $\circledast$는 입력 개폐를 조절하는 역할<br>
- 기호 /가 붙어있는 원은 메모리 블록 상태로 메모리 블록 상태는 시간에 따라 변함<br>
  $\mathbf{s}^{(t)} = \mathbf{s}^{(t - 1)} + \mathbf{g}\cdot \mathbf{i} $<br>
  입력 개폐구의 값($i$)가 0.0이면 이전 상태와 동일(이전 내용을 그대로 기억) $\rightarrow$ 이전 입력의 영향력을 더 멀리 확장
- 위쪽 기호 $\circledast$는 출력 개폐를 조절하는 역할<br>
  $h_j^{(t)} = \tau_h\bigl(\mathbf{s}^{(t)}\bigr) \cdot \mathbf{o}$<br>
<br>

- 예측 값<br>
  $y^{\prime(t)} = \operatorname{softmax}(\mathbf{Vh}^{(t)} + \mathbf{c})$

![7.png](attachment:7.png)

- 왼쪽에 있는 $\circledast$는 망각 개폐를 조절하는 역할
- 현재 메모리에 유지되고 있는 데이터를 유지하지 않을 때 망각 게이트에서 0의 값을 통과, 값을 리셋
- 망각 게이트: $f = \tau_f\bigl(\mathbf{U}^f\mathbf{x}^{(t)} + \mathbf{W}^f\mathbf{h}^{(t - 1)} + \mathbf{b}^f\bigr)$
- 이에 따라 메모리 블록 상태는 망각 게이트의 영향을 받으므로 다음과 같음<br>
    $\mathbf{s}^{(t)} = \mathbf{f} \cdot \mathbf{s}^{(t - 1)} + \mathbf{g}\cdot \mathbf{i} $<br>
<br>
- 작은 구멍 기능은 블록의 내부 상태를 3개의 개폐구에 알려주는 역할
- 순차 데이터를 처리하다가 어떤 조건에 따라 특별한 조치를 취할 때 효과적<br>
  예를 들면 음성 인식을 수행하다가 특정 단어가 발견되면 지정된 행위를 수행<br>
<br>

![8.png](attachment:8.png)

### LSTM 응용 사례
- 순환 신경망은 분별 모델뿐 아니라 생성 모델로도 활용
- LSTM은 장기 문맥을 처리하는데 적용<br>
<br>

#### 언어 모델
- 문장, 즉, 단어 열의 확률분포를 모형화<br>
  P(자세히, 보아야, 예쁘다) > P(예쁘다, 보아야, 자세히) $\rightarrow$ 여러가지 단어의 나열 중 확률이 높은 단어의 나열을 선택<br>
- 음성 인식기 또는 언어 번역기에 사용<br>
<br>
- 확률분포를 추정하는 방법에는 $n$-그램, 다층 퍼셉트론, 순환 신경망이 있음<br>
<br>

#### $n$-그램을 이용한 언어 모델
- 고전적인 방법으로 문장을 $\mathbf{x} = (z_1, z_2, \cdots, z_T)^T$라고 하면, $\mathbf{x}$가 발생할 확률을 추정<br>
  $\displaystyle P(z_1, z_2, \cdots, z_T) = \prod_{t=1}^T P(z_t \mid z_1, z_2, \cdots, z_{t - 1})$
- $n$-그램은 $n -1$개의 단어만 고려<br>
  $\displaystyle P(z_1, z_2, \cdots, z_T) \approx \prod_{t=1}^T P(z_t \mid z_{t-(n-1)}, \cdots, z_{t - 1})$
- 차원의 저주를 피하기 위해서는 $n$을 1~3 정도로 작게 해야함<br>
  예를 들어 1단어의 말뭉치의 등장확률 vs 7단어 말뭉치의 등장확률 비교<br>
<br>

- 단순한 문장 처리할 때 사용
- 확률 추정은 말뭉치를 사용
- 단어가 one-hot 코드로 표현되기 때문에 단어 간 의존성을 반영하지 못함<br>
<br>

#### 순환 신경망을 이용한 언어 모델
- 현재까지 본 단어 열을 기반으로 다음 단어 예측
- 확률분포 추정뿐만 아니라 문장 생성 기능까지 갖춤
- 비지도 학습, 말뭉치로부터 쉽게 훈련집합 구축 가능<br>
<br>

- 훈련 집합에서 입력 $\mathbf{x} = (<\text{시작}>, z_1, z_2, \cdots, z_T)^T$, 출력 $y = (z_1, z_2, \cdots, z_T, <\text{끝}>)^T$
- 순환 신경망의 학습은 훈련집합의 데이터를 BPTT학습 알고리즘 적용

![9.png](attachment:9.png)<br>

<br>

- 기계 번역기나 음성 인식기의 성능을 향상하는 데 활용
- 일반적으로 사전학습을 수행한 언어 모델을 개별 과제에 맞게 미세 조정<br>
<br>


#### 기계 번역
- 훈련 샘플 예: $\mathbf{x}=(<\text { 시작 }>\text {, 자세히, 보아야, 예브다 })^{\mathrm{T}}, \quad \mathbf{y}=(\text { It, is, beautiful, to, see, more, closely, }<\text { 끝 }>)^{\mathrm{T}}$<br>
<br>
- 언어 모델은 입력 문작과 출력 문장의 길이가 같지만 기계 번역은 길이가 서로 다름(열 대 열 문제 발생)
- 각 언어마다 고유한 문법이 있기에 어순이 다른 문제가 발생
- 고전 적인 통계적 기계 번역 방법의 한계가 있으며, 현재 심층학습 기반 기계 번역 방법이 주류를 형성<br>
<br>
- LSTM을 사용하여 번역 과정 전체를 통째로 학습
- 2개의 LSTM을 사용(앞쪽은 부호기(encoder), 뒤쪽은 복호기(decoder))
- 부호기는 원시 언어 문장 $\mathbf{x}$를 $\mathbf{h}\_{Ts}$라는 특징 벡터로 변환
- 복호기는 $\mathbf{h}\_{Ts}$를 가지고 언어 문장(출력 $\mathbf{y}$)을 생성
![10.png](attachment:10.png)<br>
<br>
- 가변길이 문장을 고정 길이 특징 벡터로 변환 후 다시 가변 길이 문장을 생성하지만 **문장의 길이가 서로 크게 다를 때 성능의 저하가 발생**<br>
<br>
- 모든 순간 상태 변수를 사용(부호기에서의 계산 결과 $h$를 복호기에 넘겨줌)
- 양방향 구조를 채택하여 어순이 다른 문제 해결<br>
<br>


#### 영상 주석 생성
- 영상 속 물체를 인식 검출, 물체의 속성과 행위, 물체간 상호 작용을 알아냄 $\rightarrow$ CNN의 역할
- 의미를 요약하는 문장 생성 $\rightarrow$ LSTM의 역할
- 현재는 딥러닝 기술을 사용하여 통째로 학습
- 학습의 목적은 CNN, LSTM, 단어 임베딩 매개변수의 최적화<br>
<br>


### 느낀점
내용이 굉장히 많아서 당황스러웠고 내용자체도 가볍게 여길 내용은 아니었다. 특히 논문 세미나와 관련된 내용들이 많이 등장해<br>
복습이 필수적인 부분인 것 같다. 중간에 수식이 나오는 부분은 미처 모두 확인하지 않고 넘어갔는데<br>
나중에 시간이 여유로울 때 한 번씩 증명을 해봐야겠다. 아직 구현 실습은 진행하지 않았지만 다른 노트북에 추가로 블로그 포스팅 할 예정이다.
