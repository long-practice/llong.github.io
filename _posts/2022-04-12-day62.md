---
title: 프로그래머스 인공지능 데브코스 62일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 13주차(22.3.7.(월) ~ 22.3.11.(금)) 
  Spark ML Pipeline
  NLP 개요, 언어모델, 단어 임베딩, 텍스트 전처리
tags:
- 자연어 처리(NLP)
- 언어모델
- n-gram
- Unigram
- Bigram

use_math: True
---

# 62일차(NLP-언어모델)

13주차(22.3.14.(월) ~ 22.3.8.(금)): 빅데이터 총정리, NLP
* 빅데이터 총정리
* NLP 기초(텍스트 전처리, 언어모델, 문서분류, 단어 임베딩) 강의
<br>
<br>

출처: 프로그래머스 인공지능 데브코스 3기 강의

### 언어모델
- 언어모델: 연속적인 단어들(sequence of words)에 확률으 부여하는 모델
- 어떤 문장 혹은 연속적인 단어들이 주어졌을 때 이어질 단어를 구하는 것
- `I ate it`과 `I it ate` 중 나타날 확률을 구해보면 전자 문장이 더 높음
- **문장이 등장할 확률을 구하는 것이 언어모델의 목표**
- 기계번역, 맞춤법검사, 음성인식 등 다양한 분야에 적용할 수 있음<br>
<br>

#### $P(W)$구하기
- 연쇄 법칙(Chain rule)을 이용하여 결합확률(joint probability) 구하기<br>
  $P(\text{its}, \text{water}, \text{is}, \text{so}, \text{transparent}, \text{that}) = 
P(\text {its}) \times P(\text {water} \mid \text {its}) \times P(\text {is} \mid \text {its water})
\times P(\text{so} \mid \text{its water is}) \times P(\text{transparent} \mid \text{its water is so})$<br>
<br>
- 위 식에서 다음의 경우를 생각해보면<br>
  $\displaystyle P(\text{the} \mid \text{its water is so transparent that}) =
\frac{Count(\text{its water is so transparent that the})}{Count(\text{its water is so transparent that})}$에서<br>
  위 식을 계산할 수 있는 데이터가 충분하지 않을 가능성이 굉장히 높음 - **희소성 문제**<br>
<br>

---

### n-gram 모델
- 위와 같이 복잡한 문제를 해결하기 위해서는 가정을 취해줌을써 해결
- Markov Assumption을 적용: **"한 단어의 확률은 그 단어 앞에 나타나는 몇 개의 단어들에만 의존한다"**<br>
  $P(\text {the} \mid \text {its water is so transparent that}) \approx P(\text {the} \mid \text {that})$<br>
  $P(\text {the} \mid \text {its water is so transparent that}) \approx P(\text {the} \mid \text {transparent that})$<br>
<br>
- 위의 식을 일반화하면 "어떤 연속적인 단어가 주어졌을 때 $i$번째 단어 $w_i$가 등장할 확률은 직전 $k$개의 단어에만 영향을 받는다"로 생각할 수 있음<br>
<br>
  $P(w_{i} \mid w_{1} w_{2} \ldots w_{i-1}) \approx P(w_{i} \mid w_{i-k} \cdots w_{i-1})$<br>
  <br>
  $\displaystyle P(w_{1} w_{2} \cdots w_{n}) \approx \prod_{i} P(w_{i} \mid w_{i-k} \cdots w_{i-1})$<br>
<br>

- 결론적으로 아주 긴 문장이더라도 몇 개의 단어들의 조합만 살펴보기 때문에 다음 단어의 등장 확률을 구하는데 큰 문제가 없음<br>
<br>

#### Unigram 모델
- Markov Assumption을 극단적으로 생각을 해보면<br>
  즉, 현재 단어가 등장할 확률은 이전 단어와는 아무 관계가 없을 경우($k = 0$일 경우)<br>
  $\displaystyle P(w_{1} w_{2} \cdots w_{n}) \approx \prod_{i} P(w_{i})$<br>
- Unigram으로 생성된 문장을 살펴보면 일반적으로 학습 시에 많이 등장했던 단어들이 등장<br>
<br>

#### Bigram 모델
- 어떤 단어의 등장확률은 바로 직전 단어에 영향을 받음<br>
  $\displaystyle P(w_{1} w_{2} \cdots w_{n}) \approx \prod_{i} P(w_{i} \mid w_{i-1})$<br>
- Bigram을 생성된 문장을 살펴보면 Unigram 모델보다 문맥적으로 자연스러운 형태를 띄고 있음<br>
<br>

#### 위의 모델들을 확장하여 일반화한 형태: n-gram 모델
- 위의 Unigram과 Bigram을 점점 확장시키면 Trigram(3개의 단어만 확인), 4-gram(4개의 단어), 5-gram(5개의 단어)으로 확장할 수 있음
- 대부분의 경우 n-gram만으로도 좋은 결과를 얻을 수 있음
- 하지만 이 모델의 단점은 장기문맥 의존성(멀리 떨어진 단어들간의 관계)을 완벽하게 모델링할 수 없음

---

### Bigram 확률계산

#### MLE 유도과정(참고)
- 최대 우도 추정(Maximum likelihood estimation)<br>
  $\displaystyle P(w_i \mid w_{i-1}) = \frac{count(w_{i-1}, w_i)}{count(w_{i-1})}$<br>
<br>
- MLE를 유도하는 과정은 다음과 같음<br>
  $P(D \mid \Theta)$에서 $D = \{w_1, \cdots, w_N\}$는 문장들의 집합<br>
  <br>
  $P(a) = \beta_{a}$: a라는 단어가 등장할 확률<br>
  $P(a \mid b) = \alpha_{ab}$: b라는 단어가 나왔을 때 a가 등장할 확률<br>
  그렇다면 vocabulary 집합 $V$에 있는 임의의 단어 x가 b라는 단어 뒤에 올 확률은 $\alpha_{xb}$<br>
  <br>
  
  만약 vocabulary에 있는 단어들이 b라는 단어 뒤에 올 확률은 1인 것을 직관적으로 알 수 있지만 수식으로 표현하면<br>
  $\displaystyle \sum_{x \in V}\alpha_{xb} = \sum_{x \in V} \frac{P(x, b)}{P(b)} = \frac{1}{P(b)} \sum_{x \in V}P(x, b)$<br>
  이 때, $\displaystyle \sum_{x \in V}P(x, b)$는 $b$에 대한 주변확률분포의 합이므로 그 값은 $P(b)$<br>
  따라서 $\displaystyle \sum_{x \in V}\alpha_{xb} = 1$이 성립
  <br>
- 따라서 관측결과 $\Theta = \{\alpha_{ab}, \beta_{a}\}$라고 할 때  <br>
  $\displaystyle P(D \mid \Theta) = \prod_{i=1}^N P(w_i)$로 표현이 가능하며,<br>
  그 형태는 $\displaystyle P(D \mid \Theta) = P(w_1)P(w_2)\cdots P(w_N)$과 같이 표현<br>
  <br>
  즉, 각각의 문장이 생성될 확률은 위에서 구한 $\alpha$와 $\beta$의 곱으로써 표현이 가능<br>
<br>

- 그렇다면 위의 식의 양 변에 로그를 취해주고 $\alpha_{ab}$에 관한 항만 나타낸 식을 $l_{ab}$라고 한다면<br>
  $\displaystyle l_{ab} = c(b, a)\ln\alpha_{ab} + \lambda(\sum_{x \in V}\alpha_{xb} - 1)$로 표현이 가능<br>
  $c(b, a)$는 $\alpha_{ab}$의 빈도 수,<br>
  $\displaystyle \lambda(\sum_{x \in V}\alpha_{xb} - 1)$는 $c(b, a)\ln\alpha_{ab}$에서 각각의 파라미터 값이 어느 값을 가질 때 최대값을 가지는지 구하기 위해 추가한 규제항(라그랑주 승수법 적용)<br>
<br>

- 위의 식을 미분한 값이 0을 만족시키는 $\alpha_{ab}$(우도를 최대화 시키는 지점)를 구하는 것이 목적<br>
  $\displaystyle \frac{c(b, a)}{\alpha_{ab}} + \lambda = 0 \Leftrightarrow \lambda\sum_{a \in V}\alpha_{ab} = -\sum_{a \in V}c(b, a)$<br>
  $\displaystyle \lambda = - \frac{\sum_{a \in V}c(b, a)}{\sum_{a \in V}\alpha_{ab}} = -\sum_{a \in V}c(b, a)$<br>
<br>
- 결국 $\displaystyle \frac{c(b, a)}{\alpha_{ab}} = \sum_{a \in V}c(b, a)$<br>
  $\displaystyle \therefore \alpha_{ab} = \frac{c(b, a)}{\sum_{a \in V}c(b, a)}$<br>
<br>

- 위에서 도출한 $\alpha_{ab}$에 관한 식은 최초 $\displaystyle P(w_i \mid w_{i-1}) = \frac{count(w_{i-1}, w_i)}{count(w_{i-1})}$ 식과 동일한 의미를 가진다는 것을 알 수 있음<br>
  따라서 **Bigram의 확률은 최대우도 추정법이라는 것을 알 수 있음**<br>
<br>


#### 예시
- 3개의 문장<br>
  `<s>I am Sam</s>`<br>
  `<s>Sam I am</s>`<br>
  `<s>I do not like green eggs and ham</s>`<br>
<br>

- $P(\text{I} \mid \text{<s>}) = 2 / 3 = 0.67$
- $P(\text{<\s>} \mid \text{sam}) = 1 / 2 = 0.5$
- $P(\text{sam} \mid \text{<am>}) = 1 / 2 = 0.5$
- $P(\text{do} \mid \text{I}) = 1 / 3 = 0.33$

---

### 모델 평가
- 외재적 평가(extrinsic evaluation)<br>
  - 언어모델은 일반적으로 그 자체가 목표이기보다는 특정 과제(맞춤법 검사 등)를 위한 부분으로 적용
  - **언어모델이 좋은지 안좋은지는 해당 과제의 평가지표를 사용**하는 경우가 많음
  - 기계번역을 위해 여러가지 모델을 적용할 때 각 모델이 얼마나 정확하게 번역을 하는지 계산하고 성능이 뛰어난 언어모델으 최종적으로 사용
  - 시간이 오래걸린다는 단점이 있음<br>
<br>

- 내재적 평가(intrinsic evaluation)<br>
  - 언어모델이 학습하는 확률자체를 평가할 수 있음(Perplexity)
  - 단순히 확률값을 잘 학습하는지 여부를 확인하는 것이기에 **버그가 있었는지 빠르게 확인하는 용도**로 사용
  - 즉, 해당 모델이 좋은 성능을 내는지는 알 수 없음<br>
<br>

- 좋은 언어모델을 만들기 위해서는 **테스트 데이터를 높은 확률로 예측하는 모델**을 만들어야 함<br>

### 느낀점
언어모델 특히, n-gram에 대해서 자세히 알아보았고, 직접 최대우도를 유도를 해보았는데<br>
단순하게 생각했던 것들, 직관적으로 생각했던 것 배후로 수학적인 근거가 뒷받침되어 있었다는 것을 알 수 있었다.<br>
논문 세미나를 하면서 공부했던 것들이 나와서 강의에서 나온 내용들을 흡수하는데 크게 어려운 점은 없었지만<br>
미처 알지 못했던 부분들은 확실하게 잡고 넘어가야할 것 같다.
