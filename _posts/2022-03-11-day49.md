---
title: 프로그래머스 인공지능 데브코스 49일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 10주차(22.2.14.(월) ~ 22.2.18.(토)) 
  선형회귀
  선형분류
  Weekly Mission
tags:
- 다양한 CNN 모델 성능
- 전이학습
- 기계번역
- 규칙 기반 기계번역(RBMT)
- 통계 기반 기계번역(SMT)
- 신경망 기반 기계번역(NMT)
- seq2seq
- Transformer
- Attention
- Encoder
- Decoder
- Embedding
- 배치 정규화
- 레이어 정규화

use_math: True
---
# 49일차(신경망 기초 - weekly mission)

10주차(22.2.14.(월) ~ 22.2.18.(금)): CNN & RNN
* Deep Learning 기초
* CNN Models
* Deep Learning 최적화
* RNN
* weekly mission
<br>
<br>

출처: 프로그래머스 인공지능 데브코스 3기 강의

### 다양한 CNN 모델 성능 비교
    
    
|모델||성능|
|:------:|------|:------:|
|AlexNet||79.066|
|VGG-16||90.382|
|VGG-19||90.876|
|VGG-16 with batch normalization||91.516|
|VGG-19 with batch normalization||91.842|
|ResNet-50||92.862|
|ResNet-101||93.546|
|ResNet-152||94.046|
|Densenet-121||91.972|
|Densenet-161||93.560|
|Inception v3||93.450|
|GoogleNet||89.530|
|EfficientNet-B3||96.054|
|EfficientNet-B7||96.908|    


### 전이 학습

- 사전 학습된(pre-trained) 모델을 불러와 feature extractor의 가중치를 초기 가중치로 사용하는 방법
- 사전 학습된 모델과 현재 내가 개발하려고하는 모델의 학습 데이터가 유사하다면 학습 속도 개선 및 정확도 향상에 이점
- **단기간 학습 모델을 구현할 시 주로 사용되는 방법**<br>
<br>

- freezing all but last layer: 모델의 가장 마지막 레이어에서 분류되는 클래스의 개수만 조정하는 방식
- finetuning all layer: 앞 쪽에 있는 모델의 추출기를 모두 바꿔주는 방식


### 기계번역의 역사

#### 기계번역
- 자연어 처리의 한 분야로 어떤 문장을 해석(NLU)하고 해석한 문장을 언어로 생성(NLG)하는 모델을 만드는 것을의미<br>
<br>

#### 규칙 기반 기계번역(Rule-Based Machine Translation, RBMT)
- 단어와 단어 사이의 규칙을 이용하여 번역문장을 생성
- 형태소, 구문 분석을 통하여 규칙을 만들어내고 번역문장을 생성
- 형태소, 구문 분석 성능이 좋지 않다면 당연히 번역결과도 좋지 않게됨
- 하나의 맥락에서 결과를 개선하기 위해 규칙을 조정하면 다른 맥락에서 결과가 악화되는 한계점<br>
<br>

#### 통계 기반 기계번역(Statistical Machine Translation, SMT)
- 수학적 확률 기반 번역 접근 방식, 데이터를 계산적으로 분석하여 문자열을 처리, 패턴을 결정하는 규칙성을 기반으로 개발
- 두 언어의 텍스트 말뭉치(코퍼스)에서 통계 정보를 바탕으로 번역을 수행
- 언어 모델: 어떤 단어, 구문이 나왔을 때 다음 단어가 나올 확률
- 번역 모델: 어떤 단어 혹은 구문이 번역하고자 하는 언어에 매칭되는 것을 추출
- 다양한 번역열 중 확률이 가장 높은 최적의 번역열을 골라내는 것
- 새로운 언어 쌍을 빠르게 추가하여 학습이 가능하여 상세한 언어 지식 없이도 시스템 개발 가능
- 요구되는 데이터량이 너무나도 많으며, 데이터량이 일정 수준을 넘어가면 푸밀이 오히려 하락<br>
<br>

#### 신경망 기반 기계번역(Neural Machine Translation, NMT)
- 이전 SMT방식보다 유창성 측면(얼마나 원어민에게 번역이 자연스럽게 보이는가)에 대해 훨씬 좋은 평가를 받고 있음
- Encoder와 Decoder로 구성되어 있고, Encoder에서 입력 문장을 벡터화, Decoder에서는 입력 문장을 복호화
- 신경망 시스템에서 문제가 발생했을 때 디버깅하고 문제를 찾는 것은 굉장히 어려움
- GPU, TPU등의 높은 수준의 리소스가 요구됨

### Transformer
- seq2seq처럼 Encoder, Decoder로 구성
- 특정 단어, 부분에 집중하는 Attention만으로 구성되었음, 여러 레이어에서 Attention 과정을 반복
- 현재 기계번역에서 적용되고 있으며, transformer가 등장한 이후로 RNN은 더이상 사용되지 않음<br>
<br>

#### 입력 값 임베딩(Embedding)
- RNN을 사용하지 않기 때문에 위치 정보를 포함하고 있는 임베딩을 사용
- Positional Encoding을 사용하여 각각의 단어가 어떤 순서를 가지는지 네트워크에게 정보를 전달<br>
<br>

#### 인코더(Encoder)
- 임베딩이 끝난 후 Attention을 진행(Multi-head Attention)
- Attention이 받은 입력은 입력 문장에 대한 정보와 각 단어의 순서에 대한 정보를 포함한 값
- 각각의 단어가 서로에게 얼마나 연관성이 있는지 학습(self-Attention)
- 성능 향상을 위해 skip connection을 두는 잔여 학습(Residual Learning)을 사용
- 잔여 학습을 마친 값은 정규화를 통해 다음 레이어로 값을 전달
- Attention과 정규화 과정을 반복하여 레이어를 구성하고 각 레이어에서는 서로 다른 파라미터를 가짐<br>
<br>

#### 디코더(Decoder)
- 인코더의 가장 마지막 레이어의 출력이 모든 디코더 레이어의 입력으로 들어감(seq2seq와 동일)
- 하나의 디코더에서는 두 개의 Attention을 사용
- 인코더와 마찬가지로 매 시점 디코더에 입력되는 값들 또한 Positional Encoding을 이용하여 위치 정보를 디코더에게 전달
- 또한 self-Attention을 이용하여 각각의 단어가 서로에게 얼마나 연관성이 있는지 가중치를 생성, **출력되는 문장에 대한 표현을 학습**
- 다음 Attention(Encoder-Decoder Attention)에서는 인코더의 출력 값을 입력받아 **각각 출력되고 있는 단어가 입력문장에서의 어떤 단어와 연관성이 있는지 보여줌**
- `<eos>`가 출력될 때까지 디코더를 이용<br>
<br>

#### 어텐션(Attention)
- Encoder와 Decoder는 Multi-Head Attention 레이어를 사용하며 여러 개의 Scaled Dot-Product Attention으로 구성
- 어텐션을 위한 세 가지 입력 요소는 **쿼리(알고자하는 주체), 키(주체와 연관된 대상), 값(연관된 정도)**
- 어떤 단어(쿼리)가 다른 단어들(키)과 비교해서 얼마나 연관성이 있는지(값)를 구하는 것은 키에 대해 Attention score를 구하는 것
- $QK$ > Scaling > Masking > Softmax 결과 값을 $V$와 행렬곱을 하는 구조(Scaled Dot-Product Attention)<br>
<br>

![3.png](attachment:3.png)

### 배치 정규화, 레이어 정규화
- 배치 정규화: 1개의 배치(여러 개의 샘플)에서 각 피처들끼리 정규화
- 레이어 정규화: 1개 샘플에서 모든 피처들의 값을 정규화

![2.png](attachment:2.png)

![1.png](attachment:1.png)

### 느낀점
여러가지 실습을 통해 CNN과 RNN에 대해서 알아보았다. 물론 아직 부족한 부분도 많지만 심화된 내용도 학습할 수 있었고,<br>
잘 알지 못했던 레이어 정규화를 inline question을 통해 알 수 있었다.<br>
물론 표현이 애매해서 당황스러운 부분도 있었지만, 그 덕분에 조금 더 개념을 확실하게 알 수 있었던 것 같았다.<br>
생각을 수학적으로 표현하는 것도 중요한 것 같다. 단순히 어떤 현상이 일어나는 것의 이유를 논리적으로 설명하는 것은 중요한 것 같다<br>
아직 정리하지 못한 노트북들이 있는데 방학기간동안 차근차근 정리해야겠다.
