---
title: 프로그래머스 인공지능 데브코스 58일차
layout: post
post-image: https://user-images.githubusercontent.com/83870423/148743292-e6a1b86d-95ca-4f30-b96a-482104d72319.png
description: 12주차(22.2.28.(월) ~ 22.3.4.(금)) 
  데이터 팀에 관한 커리어 이야기
  빅데이터와 분산처리 시스템
  Hadoop과 Spark에 대한 이해
  Spark를 이용한 데이터 전처리 및 머신러닝 모델 설계
tags:
- Spark MLlib
- Spark.mllib
- Spark.ml
- 모델 설계 기본구조
- 피처 변환
- StringIndexer
- Scaler
- Imputer
- Spark MLlib ML Pipeline
- Transformer
- DataFrame
- Estimator
- Parameter

use_math: True
---
# 58일차(Spark MLlib, ML 모델 빌딩, ML pipeline)

12주차(22.2.28.(월) ~ 22.3.4.(금)): 데이터 팀에 관한 내용, SPARK
* 데이터 팀의 역할
* 커리어 이야기
* SPARK
* SPARK 실습
<br>
<br>

출처: 프로그래머스 인공지능 데브코스 3기 강의

### Spark MLlib
- **Spark가 제공하는 머신러닝 관련 다양한 알고리즘, 유틸리티로 구성된 라이브러리**
- Classification, Regression, Clustering, Collaborative Filtering, Dimensionality Reduction
- 딥러닝 지원은 아직은 좀 미약<br>
<br>


#### spark.mllib vs spark.ml
- RDD 기반과 데이터프레임 기반 두 가지 버전이 존재<br>
- spark.mllib가 RDD 기반(더 이상 업데이트가 안됨), spark.ml은 데이터프레임 기반
- `import pyspark.ml` **항상 spark.ml을 사용할 것**<br>
<br>

#### Spark MLlib의 장점
- 대용량 데이터 처리 가능
- 원스톱 ML 프레임워크<br>
  - 데이터 프레임과 SparkSQL등을 이용해서 데이터 전처리
  - Spark MLlib를 이용해 모델 빌딩
  - ML pipeline을 통해 모델 빌딩 자동화 가능
  - MLflow로 모델 관리하고 서빙<br>
    - 모델 개발과 테스트와 관리와 서빙까지 제공해주는 E2E 프레임 워크
    - python, Java, R, 지원하고 다른 언어의 경우 API 형태로 엑세스 가능
    - 트래킹, 모델, 프로젝트를 지원<br>
<br>

### Spark MLlib 기반 모델 빌딩의 기본 구조
- 다른 라이브러리를 사용한 모델 빌딩과 크게 다르지 않음<br>
  훈련집합 전처리, 모델 빌딩, 모델 검증<br>
<br>
- 사이킷런과 비교했을 때 장점: 데이터의 클 때 처리가 가능<br>
  사이킷런의 경우 하나의 컴퓨터에서 돌아가는 모델 빌딩, Spark MLlib은 여러 서버 위에서 모델 빌딩
- 훈련집합의 크기가 크면 전처리와 모델 빌딩에 있어 Spark가 유리
- ML pipeline을 통해 모델 개발의 반복을 쉽게 해줌

---

### Spark MLlib 피처 변환
- 피처 값들을 모델 훈련에 적합한 형태로 변환 시키는 것이 목적
- Feature Extractor와 Feature Transformer 두 가지가 존재<br>
<br>

#### Featrue Transformer
- 피처 값들은 숫자 필드여야 함(연속적인 값), 카테고리 값들은 숫자 필드로 변환해야 함
- 숫자 필드라고 해도 가능한 값의 범위를 특정 범위로 정규화 진행해주어야 함(Scaling 혹은 Normalization)
- 결측치를 어떻게 보간해줄지에 대해서도 생각해봐야 함<br>
<br>

#### Feature Transformer - StringIndexer: 텍스트 카테고리를 숫자로 변환
- 각각의 카테고리를 숫자로 변환
- 사이킷런에서는 sklearn.preprocessing 모듈 아래 LabelEncoder, OrdinalEncoder, OneHotEncoder, ...
- **Spark MLlib의 경우 pyspark.ml.feature 모듈 아래 StringIndexer, OneHotEncoder**
- Indexer 모델을 만들고 훈련, Indexer모델로 데이터프레임을 변환<br>
<br>

#### Feature Transformer - Scaler: 숫자 필드값의 정규화
- 숫자 필드 값의 범위를 특정 범위로 변환(주로 0에서 1)
- 스케일링(Scaling) 혹은 정규화(Normalization)이라고 부름
- 사이킷런에서는 sklearn.preprocessing 모듈 아래 StandardScaler, MinMaxScaler, ...
- **Spark MLlib의 경우 pyspark.ml.feature StandardScaler, MinMaxScaler**<br>
<br>

#### Feature Transformer - Imputer: 값이 없는 필드 채우기
- 결측 데이터를 보간
- 사이킷런의 경우 skelarn.preprocessing 모듈 아래 Imputer 존재
- **Spark MLlib의 경우 pyspark.ml.feature 모듈 아래 Imputer**<br>
<br>

#### Feature Extractor
- 기존 피처에서 새로운 피처를 추출
- TF-IDF, Worde2Vec 등 텍스트 데이터를 인코딩하는 경우가 해당<br>
<br>


---

### 모델 빌딩과 관련된 흔한 문제들
- 훈련 집합 관리가 안됨
- 모델 훈련 방법이 기록이 안됨(어떤 훈련 셋, 어떤 피처, 하이퍼 파라미터는 무엇을 사용했는지?)
- 모델 훈련에 많은 시간 소요(매번 각 스텝들을 노트북에서 일일이 수행)
- 에러 발생 여지가 많음(특정 스텝을 까먹거나 조금 다른 방식 적용)<br>
<br>

### Spark MLlib ML Pipeline
- 데이터 과학자가 머신러닝 개발과 테스트를 쉽게 해주는 기능(DataFrame 기반)
- 하나 이상의 Transformer와 Estimator가 연결된 모델링 워크플로우(입력: 데이터 프레임, 출력: 머신러닝 모델)
- 머신러닝 알고리즘 관계없이 일관된 형태의 API제공
- 4가지 요소(DataFrame, Transformer, Estimator, Parameter)으로 구성<br>
<br>

- 모델을 훈련 시키고 테스트하는 과정을 자동화
- 자동화를 통해 에러 소지를 줄이고 빠른 반복이 가능하게 함
- 즉, 한번 파이프라인을 마들면 반복 모델빌딩이 쉬워짐
- Load data > Extract features > Train model > Evaluate<br>
<br>

#### ML Pipeline의 구성요소 - 데이터 프레임
- 기본 데이터 포맷으로 CSV, JSON, Parquet, JDBC(RDB)를 지원
- ML Pipeline 관점에서 2가지 새로운 데이터 소스를 지원(이미지(jpeg, png 등), LIBSVM(label, features 두 개의 컬럼으로 구성) 데이터 소스)<br>
<br>

#### ML Pipeline의 구성요소 - Transformer
- 입력 데이터프레임을 다른 데이터프레임으로 변환, **transform**이 메인함수
- 2종류의 transformer 존재(Feature Transformer, Learning Model)
- Feature Tranformer: 입력 데이터프레임으로 부터 새로운 칼럼을 만들어내고, 새로운 데이터프레임을 출력
- Learning Model: 머신러닝 모델이 훈련하여 예측값을 갖는 새로운 칼럼을 만들어내고, 새로운 데이터프레임 출력(prediction, probability)<br>
<br>

#### ML Pipeline의 구성요소 - Estimator
- 머신러닝 알고리즘에 해당하며, **fit**이 메인함수
- 훈련집합 데이터프레임을 입력으로 받아 머신러닝 모델을 만들어냄
- LogisticRegression은 Estimator, LogisticRegression.fit()을 호출하면 머신러닝 모델을 만들어냄
- ML Pipeline도 Estimator로 fit 함수 호출로 시작
- 저장 읽기 함수 제공 **재활용이 가능**<br>
<br>

#### ML Pipeline의 구성요소 - Parameter
- Transformer와 Estimator의 공통 API로 다양한 인자를 적용
- Param, ParamMap 두 가지 파라미터가 존재
- 예를 들면, 훈련 반복수(iteration) 지정을 위해 setMaxIter()를 사용(lr.maxIter -> 10)
- Estimator, Transformer에 인자로 지정이 가능

---

### 실습
- 보스턴 집값 예측을 통한 회귀 문제 실습
- 타이타닉 생존자 예측을 통한 분류 문제 실습


```python
!pip install pyspark==3.0.1 py4j==0.10.9
```

    Requirement already satisfied: pyspark==3.0.1 in /usr/local/lib/python3.7/dist-packages (3.0.1)
    Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (0.10.9)



```python
from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .appName("Boston Housing Linear Regression example")\
    .getOrCreate()
```


```python
spark
```





    <div>
        <p><b>SparkSession - in-memory</b></p>

<div>
    <p><b>SparkContext</b></p>

    <p><a href="http://f993716feab9:4040">Spark UI</a></p>

    <dl>
      <dt>Version</dt>
        <dd><code>v3.0.1</code></dd>
      <dt>Master</dt>
        <dd><code>local[*]</code></dd>
      <dt>AppName</dt>
        <dd><code>Python Spark SQL basic example</code></dd>
    </dl>
</div>

    </div>




---

### 보스턴 주택 가격 예측 모델 만들기

#### 데이터 불러오기


```python
!wget https://s3-geospatial.s3-us-west-2.amazonaws.com/boston_housing.csv
```

    --2022-03-03 11:13:02--  https://s3-geospatial.s3-us-west-2.amazonaws.com/boston_housing.csv
    Resolving s3-geospatial.s3-us-west-2.amazonaws.com (s3-geospatial.s3-us-west-2.amazonaws.com)... 52.218.212.113
    Connecting to s3-geospatial.s3-us-west-2.amazonaws.com (s3-geospatial.s3-us-west-2.amazonaws.com)|52.218.212.113|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 36240 (35K) [text/csv]
    Saving to: ‘boston_housing.csv’
    
    boston_housing.csv  100%[===================>]  35.39K  --.-KB/s    in 0.04s   
    
    2022-03-03 11:13:02 (971 KB/s) - ‘boston_housing.csv’ saved [36240/36240]
    



```python
data = spark.read.csv('./boston_housing.csv', header = True, inferSchema = True)
```


```python
data.printSchema()
```

    root
     |-- crim: double (nullable = true)
     |-- zn: double (nullable = true)
     |-- indus: double (nullable = true)
     |-- chas: integer (nullable = true)
     |-- nox: double (nullable = true)
     |-- rm: double (nullable = true)
     |-- age: double (nullable = true)
     |-- dis: double (nullable = true)
     |-- rad: integer (nullable = true)
     |-- tax: integer (nullable = true)
     |-- ptratio: double (nullable = true)
     |-- b: double (nullable = true)
     |-- lstat: double (nullable = true)
     |-- medv: double (nullable = true)
    



```python
data.show(5)
```

    +-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+
    |   crim|  zn|indus|chas|  nox|   rm| age|   dis|rad|tax|ptratio|     b|lstat|medv|
    +-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+
    |0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3| 396.9| 4.98|24.0|
    |0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8| 396.9| 9.14|21.6|
    |0.02729| 0.0| 7.07|   0|0.469|7.185|61.1|4.9671|  2|242|   17.8|392.83| 4.03|34.7|
    |0.03237| 0.0| 2.18|   0|0.458|6.998|45.8|6.0622|  3|222|   18.7|394.63| 2.94|33.4|
    |0.06905| 0.0| 2.18|   0|0.458|7.147|54.2|6.0622|  3|222|   18.7| 396.9| 5.33|36.2|
    +-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+
    only showing top 5 rows
    


#### 피처 벡터 만들기


```python
from pyspark.ml.feature import VectorAssembler

# medv 제외 모든 칼럼
feature_columns = data.columns[:-1]
assembler = VectorAssembler(inputCols=feature_columns, outputCol = "features")
```


```python
print("[", end="")
for f in feature_columns[:-1]:
    print(f"'{f}'", end=', ') 
print(f"'{feature_columns[-1]}']")
```

    ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']



```python
# transform은 하나의 데이터 프레임을 입력으로 받아서 새로운 칼럼을 추가 시킴
# feature_columns는 모든 칼럼에 있는 값을 하나의 벡터로 변환
data_2 = assembler.transform(data)
data_2.show(5)
```

    +-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+
    |   crim|  zn|indus|chas|  nox|   rm| age|   dis|rad|tax|ptratio|     b|lstat|medv|            features|
    +-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+
    |0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3| 396.9| 4.98|24.0|[0.00632,18.0,2.3...|
    |0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8| 396.9| 9.14|21.6|[0.02731,0.0,7.07...|
    |0.02729| 0.0| 7.07|   0|0.469|7.185|61.1|4.9671|  2|242|   17.8|392.83| 4.03|34.7|[0.02729,0.0,7.07...|
    |0.03237| 0.0| 2.18|   0|0.458|6.998|45.8|6.0622|  3|222|   18.7|394.63| 2.94|33.4|[0.03237,0.0,2.18...|
    |0.06905| 0.0| 2.18|   0|0.458|7.147|54.2|6.0622|  3|222|   18.7| 396.9| 5.33|36.2|[0.06905,0.0,2.18...|
    +-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+
    only showing top 5 rows
    


#### 훈련, 테스트 데이터 분할, 모델 훈련


```python
train, test = data_2.randomSplit([0.7, 0.3])
```


```python
from pyspark.ml.regression import LinearRegression

# 입력 데이터는 반드시 피처 벡터, 예측 값은 medv
algo = LinearRegression(featuresCol="features", labelCol="medv")
model = algo.fit(train)
```

#### 성능 측정


```python
evaluation_summary = model.evaluate(test)
# evaluation_summary 또한 데이터 프레임
evaluation_summary
```




    <pyspark.ml.regression.LinearRegressionSummary at 0x7f4513cd6b10>




```python
# L1 오차
L1_error = evaluation_summary.meanAbsoluteError
# L2 오차
L2_error = evaluation_summary.rootMeanSquaredError
```


```python
print('L1 오차:', L1_error)
print('L2 오차:', L2_error)
```

    L1 오차: 3.5755659745856643
    L2 오차: 5.100927575671511


#### 모델 예측값 확인


```python
# transform을 통해 테스트 데이터에 대한 예측 값을 predictions라는 칼럼에 추가
predictions = model.transform(test)
```


```python
# transform을 통해 prediction 칼럼이 추가 된 것을 확인
predictions.select(predictions.columns[13:]).show()
```

    +----+--------------------+------------------+
    |medv|            features|        prediction|
    +----+--------------------+------------------+
    |32.2|[0.00906,90.0,2.9...|31.865521580096917|
    |32.7|[0.01301,35.0,1.5...|30.517218789484986|
    |29.1|[0.01439,60.0,2.9...|31.568111486990606|
    |24.5|[0.01501,80.0,2.0...|27.671810253572126|
    |32.9|[0.01778,95.0,1.4...| 31.03402322656632|
    |23.1|[0.0187,85.0,4.15...|25.525318019354447|
    |20.1|[0.01965,80.0,1.7...|20.342041758917144|
    |42.3|[0.02177,82.5,2.0...| 37.36103587104139|
    |23.9|[0.02543,55.0,3.7...|27.790576890717826|
    |34.7|[0.02729,0.0,7.07...|30.477153033466173|
    |21.6|[0.02731,0.0,7.07...| 24.85708462044287|
    |26.6|[0.02899,40.0,1.2...|22.474873649067394|
    |17.5|[0.03113,0.0,4.39...|16.456095471998726|
    |33.4|[0.03237,0.0,2.18...|28.386030249182575|
    |34.9|[0.03359,75.0,2.9...| 34.19519553960593|
    |22.9|[0.03551,25.0,4.8...| 24.78051578901551|
    |45.4|[0.03578,20.0,3.3...| 38.60866190791643|
    |23.5|[0.03584,80.0,3.3...| 30.33460471155081|
    |34.6|[0.03768,80.0,1.5...|35.012527245700035|
    |22.0|[0.03932,0.0,3.41...| 27.15577122753536|
    +----+--------------------+------------------+
    only showing top 20 rows
    


---

### 타이타닉 생존 예측 모델 만들기


```python
spark = SparkSession \
    .builder \
    .appName("Titanic Binary Classification example") \
    .getOrCreate()
spark
```





    <div>
        <p><b>SparkSession - in-memory</b></p>

<div>
    <p><b>SparkContext</b></p>

    <p><a href="http://f993716feab9:4040">Spark UI</a></p>

    <dl>
      <dt>Version</dt>
        <dd><code>v3.0.1</code></dd>
      <dt>Master</dt>
        <dd><code>local[*]</code></dd>
      <dt>AppName</dt>
        <dd><code>Python Spark SQL basic example</code></dd>
    </dl>
</div>

    </div>




#### 데이터터 불러오기


```python
!wget https://s3-geospatial.s3-us-west-2.amazonaws.com/titanic.csv
```

    --2022-03-03 11:44:23--  https://s3-geospatial.s3-us-west-2.amazonaws.com/titanic.csv
    Resolving s3-geospatial.s3-us-west-2.amazonaws.com (s3-geospatial.s3-us-west-2.amazonaws.com)... 52.218.251.33
    Connecting to s3-geospatial.s3-us-west-2.amazonaws.com (s3-geospatial.s3-us-west-2.amazonaws.com)|52.218.251.33|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 61197 (60K) [text/csv]
    Saving to: ‘titanic.csv’
    
    titanic.csv         100%[===================>]  59.76K  --.-KB/s    in 0.04s   
    
    2022-03-03 11:44:23 (1.44 MB/s) - ‘titanic.csv’ saved [61197/61197]
    



```python
# Survived를 예측하는 것이 목표
data = spark.read.csv('./titanic.csv', header=True, inferSchema=True)
data.printSchema()
```

    root
     |-- PassengerId: integer (nullable = true)
     |-- Survived: integer (nullable = true)
     |-- Pclass: integer (nullable = true)
     |-- Name: string (nullable = true)
     |-- Gender: string (nullable = true)
     |-- Age: double (nullable = true)
     |-- SibSp: integer (nullable = true)
     |-- Parch: integer (nullable = true)
     |-- Ticket: string (nullable = true)
     |-- Fare: double (nullable = true)
     |-- Cabin: string (nullable = true)
     |-- Embarked: string (nullable = true)
    



```python
data.show(5)
```

    +-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+
    |PassengerId|Survived|Pclass|                Name|Gender| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|
    +-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+
    |          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|
    |          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|
    |          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|
    |          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|
    |          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|
    +-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+
    only showing top 5 rows
    



```python
# 데이터 통계 확인
# 결측치, 유의미한 칼럼 확인 가능
data.select(['*']).describe().show()
```

    +-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+
    |summary|      PassengerId|           Survived|            Pclass|                Name|Gender|               Age|             SibSp|              Parch|            Ticket|             Fare|Cabin|Embarked|
    +-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+
    |  count|              891|                891|               891|                 891|   891|               714|               891|                891|               891|              891|  204|     889|
    |   mean|            446.0| 0.3838383838383838| 2.308641975308642|                null|  null| 29.69911764705882|0.5230078563411896|0.38159371492704824|260318.54916792738| 32.2042079685746| null|    null|
    | stddev|257.3538420152301|0.48659245426485753|0.8360712409770491|                null|  null|14.526497332334035|1.1027434322934315| 0.8060572211299488|471609.26868834975|49.69342859718089| null|    null|
    |    min|                1|                  0|                 1|"Andersson, Mr. A...|female|              0.42|                 0|                  0|            110152|              0.0|  A10|       C|
    |    max|              891|                  1|                 3|van Melkebeke, Mr...|  male|              80.0|                 8|                  6|         WE/P 5735|         512.3292|    T|       S|
    +-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+
    


#### 데이터 전처리
- PassengerID, Name, Ticket, Cabin, Embarked 삭제
- Age: 결측치들을을 디폴트 값으로 채울 예정정
- Gender: 숫자로 인코딩 필요


```python
final_data = data.select(['Survived', 'Pclass', 'Gender', 'Age', 'SibSp', 'Parch', 'Fare'])
final_data.show(5)
```

    +--------+------+------+----+-----+-----+-------+
    |Survived|Pclass|Gender| Age|SibSp|Parch|   Fare|
    +--------+------+------+----+-----+-----+-------+
    |       0|     3|  male|22.0|    1|    0|   7.25|
    |       1|     1|female|38.0|    1|    0|71.2833|
    |       1|     3|female|26.0|    0|    0|  7.925|
    |       1|     1|female|35.0|    1|    0|   53.1|
    |       0|     3|  male|35.0|    0|    0|   8.05|
    +--------+------+------+----+-----+-----+-------+
    only showing top 5 rows
    



```python
# Age는 평균값으로 보간
# 1. 모듈 import
# 2. 인스턴스 생성
# 3. 어떤 데이터를 보간할건지 명시하고 fit
# 4. transform을 통해 새로운 칼럼을 추가
from pyspark.ml.feature import Imputer

imputer = Imputer(strategy='mean', inputCols=['Age'], outputCols=['AgeImputed'])
imputer_model = imputer.fit(final_data)
final_data = imputer_model.transform(final_data)

final_data.select("Age", "AgeImputed").show(10)
```

    +----+-----------------+
    | Age|       AgeImputed|
    +----+-----------------+
    |22.0|             22.0|
    |38.0|             38.0|
    |26.0|             26.0|
    |35.0|             35.0|
    |35.0|             35.0|
    |null|29.69911764705882|
    |54.0|             54.0|
    | 2.0|              2.0|
    |27.0|             27.0|
    |14.0|             14.0|
    +----+-----------------+
    only showing top 10 rows
    



```python
# 성별 정보 인코딩
# 남성: 0 / 여성: 1
from pyspark.ml.feature import StringIndexer

gender_indexer = StringIndexer(inputCol='Gender', outputCol='GenderIndexed')
gender_indexer_model = gender_indexer.fit(final_data)
final_data = gender_indexer_model.transform(final_data)

final_data.select("Gender", "GenderIndexed").show(10)
```

    +------+-------------+
    |Gender|GenderIndexed|
    +------+-------------+
    |  male|          0.0|
    |female|          1.0|
    |female|          1.0|
    |female|          1.0|
    |  male|          0.0|
    |  male|          0.0|
    |  male|          0.0|
    |  male|          0.0|
    |female|          1.0|
    |female|          1.0|
    +------+-------------+
    only showing top 10 rows
    


#### 피처 벡터 만들기


```python
from pyspark.ml.feature import VectorAssembler

# Age, Gender는 transform된 형태의 칼럼을 입력
assembler = VectorAssembler(inputCols = ['Pclass', 'SibSp', 'Parch', 'Fare', 'AgeImputed', 'GenderIndexed'],
                            outputCol= 'features')
data_vec = assembler.transform(final_data)

data_vec.show(10)
```

    +--------+------+------+----+-----+-----+-------+-----------------+-------------+--------------------+
    |Survived|Pclass|Gender| Age|SibSp|Parch|   Fare|       AgeImputed|GenderIndexed|            features|
    +--------+------+------+----+-----+-----+-------+-----------------+-------------+--------------------+
    |       0|     3|  male|22.0|    1|    0|   7.25|             22.0|          0.0|[3.0,1.0,0.0,7.25...|
    |       1|     1|female|38.0|    1|    0|71.2833|             38.0|          1.0|[1.0,1.0,0.0,71.2...|
    |       1|     3|female|26.0|    0|    0|  7.925|             26.0|          1.0|[3.0,0.0,0.0,7.92...|
    |       1|     1|female|35.0|    1|    0|   53.1|             35.0|          1.0|[1.0,1.0,0.0,53.1...|
    |       0|     3|  male|35.0|    0|    0|   8.05|             35.0|          0.0|[3.0,0.0,0.0,8.05...|
    |       0|     3|  male|null|    0|    0| 8.4583|29.69911764705882|          0.0|[3.0,0.0,0.0,8.45...|
    |       0|     1|  male|54.0|    0|    0|51.8625|             54.0|          0.0|[1.0,0.0,0.0,51.8...|
    |       0|     3|  male| 2.0|    3|    1| 21.075|              2.0|          0.0|[3.0,3.0,1.0,21.0...|
    |       1|     3|female|27.0|    0|    2|11.1333|             27.0|          1.0|[3.0,0.0,2.0,11.1...|
    |       1|     2|female|14.0|    1|    0|30.0708|             14.0|          1.0|[2.0,1.0,0.0,30.0...|
    +--------+------+------+----+-----+-----+-------+-----------------+-------------+--------------------+
    only showing top 10 rows
    


#### 훈련, 테스트 데이터 분할, 모델 훈련


```python
train, test = data_vec.randomSplit([0.7, 0.3])
```


```python
from pyspark.ml.classification import LogisticRegression

algo = LogisticRegression(featuresCol="features", labelCol="Survived")
model = algo.fit(train)
```

#### 성능 측정


```python
predictions = model.transform(test)

# 로지스틱 회귀의 경우 probability값도 확인 가능
# threshold를 설정하여 0, 1을 구분하는 확률값 조정 가능
predictions.select(['Survived', 'prediction', 'probability']).show(10)
```

    +--------+----------+--------------------+
    |Survived|prediction|         probability|
    +--------+----------+--------------------+
    |       0|       0.0|[0.58357519016095...|
    |       0|       0.0|[0.56617573161017...|
    |       0|       1.0|[0.42978542519145...|
    |       0|       1.0|[0.49275879740467...|
    |       0|       0.0|[0.58581186361648...|
    |       0|       0.0|[0.67800077003738...|
    |       0|       0.0|[0.65836261364182...|
    |       0|       0.0|[0.70992975878422...|
    |       0|       0.0|[0.69499883038836...|
    |       0|       0.0|[0.70105687627266...|
    +--------+----------+--------------------+
    only showing top 10 rows
    



```python
# 84.7% 성능
from pyspark.ml.evaluation import BinaryClassificationEvaluator

evaluator = BinaryClassificationEvaluator(labelCol='Survived', metricName = 'areaUnderROC')
evaluator.evaluate(predictions)
```




    0.8473173847404477



---

### 느낀점
Spark MLlib에 관해 알아보았다. 전반적으로 알고 있었고, 적용했었던 내용들이라 어려움은 없었다.<br>
보스턴 집값 예측, 타이타닉 예제는 많이 실습 해봤지만 이렇게 간단하게 모델을 만들고 에측한 것은 처음이었다.<br>
내일 과제 간에 적용해보면서 트러블 슈팅을 통해 잘 알지 못했던 개념들을 해결해야할 것 같고,<br>
이번 주에 배운 많은 것들을 직접 실습해봐야겠다.<br>
